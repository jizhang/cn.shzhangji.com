<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张吉的博客</title>
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/cnblogs/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/"/>
  <updated>2017-06-19T01:06:32.000Z</updated>
  <id>http://shzhangji.com/</id>
  
  <author>
    <name>张吉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 Crossfilter 和 dc.js 构建交互式报表</title>
    <link href="http://shzhangji.com/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/"/>
    <id>http://shzhangji.com/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/</id>
    <published>2017-06-18T10:59:53.000Z</published>
    <updated>2017-06-19T01:06:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>在对多维数据集进行图表分析时，我们希望在图表之间建立联系，选择图表中的一部分数据后，其他图表也会相应变动。这项工作可以通过开发完成，即在服务端对数据进行过滤，并更新所有图表。此外，我们还可以借助 Crossfilter 和 dc.js 这两个工具，直接在浏览器中对数据进行操作。</p>
<h2 id="航班延误统计"><a href="#航班延误统计" class="headerlink" title="航班延误统计"></a>航班延误统计</h2><p>这是 Crossfilter 官方网站提供的示例，基于 <a href="http://stat-computing.org/dataexpo/2009/" target="_blank" rel="external">ASA Data Expo</a> 数据集的航班延误统计。下面我们将介绍如何用 dc.js 来实现这份交互式报表。项目源码可以在 <a href="https://jsfiddle.net/zjerryj/gjao9sws/" target="_blank" rel="external">JSFiddle</a> 中浏览，演示的数据量减少到 1000 条。</p>
<p><img src="/cnblogs/images/airline-ontime-performance.png" alt=""></p>
<a id="more"></a>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter</a> 是一个 JavaScript 类库，能够在浏览器端对大量数据进行多维分析。它的特点是可以在不同的 Group By 查询之间实现“交叉过滤”，自动连接和更新查询结果。结合 <a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js</a> 图表类库，我们就可以构建出高性能、交互式的分析报表了。</p>
<h2 id="数据集、维度、度量"><a href="#数据集、维度、度量" class="headerlink" title="数据集、维度、度量"></a>数据集、维度、度量</h2><p>Crossfilter 中有维度、度量等概念。如果你对数据仓库或统计分析有所了解，这些术语和 OLAP 立方体中的定义是相似的。</p>
<ul>
<li>数据集：即一张二维表，包含行和列，在 JavaScript 中通常是由对象组成的数组；</li>
<li>维度：用于进行 Group By 操作的字段，它通常是可枚举的值，如日期、性别，也可以是数值范围，如年龄范围等；</li>
<li>度量：可以进行合计、计算标准差等操作，通常是数值型的，如收入、子女人数；记录数也是一种度量；</li>
</ul>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> flights = d3.csv.parse(flightsCsv)</div><div class="line"><span class="keyword">let</span> flight = crossfilter(flights)</div><div class="line"><span class="keyword">let</span> hour = flight.dimension(<span class="function">(<span class="params">d</span>) =&gt;</span> d.date.getHours() + d.date.getMinutes() / <span class="number">60</span>)</div><div class="line"><span class="keyword">let</span> hours = hour.group(<span class="built_in">Math</span>.floor)</div></pre></td></tr></table></figure>
<p>这段代码首先创建了一个 crossfilter 对象，数据来源是 CSV 格式的文本。之后，我们定义了一个“小时”的维度，它是从 <code>date</code> 字段计算得到的，并近似到了整数，用作 Group By 条件。定义完这个查询后，我们就可以获取航班延误次数最多的三个小时的数据了：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hours.top(<span class="number">3</span>)</div><div class="line"><span class="comment">// 输出结果</span></div><div class="line">[</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">13</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">20</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>:  <span class="number">8</span>, <span class="attr">value</span>: <span class="number">71</span> &#125;,</div><div class="line">]</div></pre></td></tr></table></figure>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>我们将 24 个小时的延误次数用柱状图展现出来：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> hourChart = dc.barChart(<span class="string">'#hour-chart'</span>)</div><div class="line">hourChart</div><div class="line">  .width(<span class="number">350</span>)</div><div class="line">  .height(<span class="number">150</span>)</div><div class="line">  .dimension(hour)</div><div class="line">  .group(hours)</div><div class="line">  .x(d3.scale.linear()</div><div class="line">    .domain([<span class="number">0</span>, <span class="number">24</span>])</div><div class="line">    .rangeRound([<span class="number">0</span>, <span class="number">10</span> * <span class="number">24</span>]))</div><div class="line">  .controlsUseVisibility(<span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>对应的 HTML 代码是：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"hour-chart"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Time of Day</div><div class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"reset"</span> <span class="attr">href</span>=<span class="string">"javascript:;"</span> <span class="attr">style</span>=<span class="string">"visibility: hidden;"</span>&gt;</span>reset<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure>
<p>我们可以看到，dc.js 和 crossfilter 结合的非常好，只需将维度信息传入 dc.js ，并对坐标轴进行适当配置，就可以完成可视化。在这个例子中，X 轴是 0 到 24 个小时（维度），Y 轴则是航班延误次数（度量）。</p>
<p>注意这里 <code>class=&quot;reset&quot;</code> 的用法，它和 <code>controlUseVisibility</code> 一起使用，会在指定位置产生一个 <code>reset</code> 按钮。你可以试着在图表上进行拖拽，对数据进行筛选后就能看到 <code>reset</code> 按钮了。</p>
<h2 id="交叉过滤"><a href="#交叉过滤" class="headerlink" title="交叉过滤"></a>交叉过滤</h2><p>接下来我们可以创建更多图表，如航班延误时间的直方图，源代码可以在 JSFiddle 中找到。当你对其中一个图表进行过滤时，其他图表就会自动过滤和更新，这样我们就可以查看某些限定条件下数据的分布情况了。</p>
<p>dc.js 还提供了许多可视化类型，如饼图、表格、自定义HTML等。当然，要完全掌握这些，还需要学习 d3.js ，市面上很多绘图类库都是构建在它之上的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter - Fast Multidimensional Filtering for Coordinated Views</a></li>
<li><a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js - Dimensional Charting Javascript Library</a></li>
<li><a href="http://blog.rusty.io/2012/09/17/crossfilter-tutorial/" target="_blank" rel="external">Crossfiler Tutorial</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在对多维数据集进行图表分析时，我们希望在图表之间建立联系，选择图表中的一部分数据后，其他图表也会相应变动。这项工作可以通过开发完成，即在服务端对数据进行过滤，并更新所有图表。此外，我们还可以借助 Crossfilter 和 dc.js 这两个工具，直接在浏览器中对数据进行操作。&lt;/p&gt;
&lt;h2 id=&quot;航班延误统计&quot;&gt;&lt;a href=&quot;#航班延误统计&quot; class=&quot;headerlink&quot; title=&quot;航班延误统计&quot;&gt;&lt;/a&gt;航班延误统计&lt;/h2&gt;&lt;p&gt;这是 Crossfilter 官方网站提供的示例，基于 &lt;a href=&quot;http://stat-computing.org/dataexpo/2009/&quot;&gt;ASA Data Expo&lt;/a&gt; 数据集的航班延误统计。下面我们将介绍如何用 dc.js 来实现这份交互式报表。项目源码可以在 &lt;a href=&quot;https://jsfiddle.net/zjerryj/gjao9sws/&quot;&gt;JSFiddle&lt;/a&gt; 中浏览，演示的数据量减少到 1000 条。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/airline-ontime-performance.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="crossfilter" scheme="http://shzhangji.com/tags/crossfilter/"/>
    
      <category term="dc.js" scheme="http://shzhangji.com/tags/dc-js/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
  </entry>
  
  <entry>
    <title>Hive+Druid 实现快速查询；回归分析是机器学习吗；StructuredStreaming 可用于生产环境</title>
    <link href="http://shzhangji.com/2017/06/18/druid-machine-learning-spark/"/>
    <id>http://shzhangji.com/2017/06/18/druid-machine-learning-spark/</id>
    <published>2017-06-18T02:41:59.000Z</published>
    <updated>2017-06-20T06:31:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结合-Apache-Hive-和-Druid-实现高速-OLAP-查询"><a href="#结合-Apache-Hive-和-Druid-实现高速-OLAP-查询" class="headerlink" title="结合 Apache Hive 和 Druid 实现高速 OLAP 查询"></a>结合 Apache Hive 和 Druid 实现高速 OLAP 查询</h2><p><img src="https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Part1Image2.png" alt="使用 HiveQL 预汇总数据并保存至 Druid"></p>
<p>Hadoop 生态中，我们使用 Hive 将 SQL 语句编译为 MapReduce 任务，对海量数据进行操作；Druid 则是一款独立的分布式列式存储系统，通常用于执行面向最终用户的即席查询和实时分析。</p>
<p>Druid 的高速查询主要得益于列式存储和倒排索引，其中倒排索引是和 Hive 的主要区别。数据表中的维度字段越多，查询速度也会越快。不过 Druid 也有其不适用的场景，如无法支持大数据量的 Join 操作，对标准 SQL 的实现也十分有限。</p>
<p>Druid 和 Hive 的结合方式是这样的：首先使用 Hive 对数据进行预处理，生成 OLAP Cube 存入 Druid；当发生查询时，使用 Calcite 优化器进行分析，使用合适的引擎（Hive 或 Druid）执行操作。如，Druid 擅长执行维度汇总、TopN、时间序列查询，而 Hive 则能胜任 Join、子查询、UDF 等操作。</p>
<p>原文：<a href="https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru" target="_blank" rel="external">https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru</a></p>
<a id="more"></a>
<h2 id="回归分析是机器学习吗？"><a href="#回归分析是机器学习吗？" class="headerlink" title="回归分析是机器学习吗？"></a>回归分析是机器学习吗？</h2><p><img src="http://www.kdnuggets.com/wp-content/uploads/is-regression-machine-learning.jpg" alt=""></p>
<p>作者认为纠结这个问题的人们其实是“将重点放在了森林而忽视了树木”。我们应该将统计分析和机器学习都用作是实现“理解数据”这一目标的工具。用哪种工具并不重要，如何用好工具、建立恰当的模型、增强对数据的认知，才是最重要的。</p>
<p>过去人们也曾争论过数据挖掘和机器学习的区别。作者认为数据挖掘是一种 <em>过程</em> ，机器学习则是过程中使用的 <em>工具</em> 。因此，数据挖掘可以总结为 <em>对海量数据进行高效的统计分析</em> 。</p>
<p>机器学习由三个元素组成：数据，模型，成本函数。假设我们有 10 个数据点，使用其中的 9 个来训练模型，最后一个用来测试，这个过程人工就可以解决。那当数据越来越多，或特征值增加，或使用计算机而非人工，是否就由统计学演变成机器学习了呢？所以说，传统统计学和机器学习之间是有一个过渡的，这个过渡过程我们无法衡量，也无需衡量。</p>
<p>原文：<a href="http://www.kdnuggets.com/2017/06/regression-analysis-really-machine-learning.html" target="_blank" rel="external">http://www.kdnuggets.com/2017/06/regression-analysis-really-machine-learning.html</a></p>
<h2 id="StructuredStreaming-可用于生产环境"><a href="#StructuredStreaming-可用于生产环境" class="headerlink" title="StructuredStreaming 可用于生产环境"></a>StructuredStreaming 可用于生产环境</h2><p><img src="https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png" alt="将数据流看做一张无边界的表"></p>
<p>2016 年中，Spark 推出了 StructredStreaming，随着时间的推移，这一创新的实时计算框架也日臻成熟。</p>
<p>实时计算的复杂性在于三个方面：数据来源多样，且存在脏数据、乱序等情况；执行逻辑复杂，需要处理事件时间，结合机器学习等；运行平台也多种多样，需要应对各种系统失效。</p>
<p>StructuredStreaming 构建在 Spark SQL 之上，提供了统一的 API，同时做到高效、可扩、容错。它的理念是 <em>实时计算无需考虑数据是否实时</em> ，将实时数据流当做一张没有边界的数据表来看待。</p>
<p>传统 ETL 的流程是先将数据导出成文件，再将文件导入到表中使用，这个过程通常以小时计。而实时 ETL 则可以直接从数据源进入到表，过程以秒计。对于更为复杂的实时计算，StructuredStreaming 也提供了相应方案，包括事件时间计算、有状态的聚合算子、水位线等。</p>
<p>原文：<a href="https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/" target="_blank" rel="external">https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;结合-Apache-Hive-和-Druid-实现高速-OLAP-查询&quot;&gt;&lt;a href=&quot;#结合-Apache-Hive-和-Druid-实现高速-OLAP-查询&quot; class=&quot;headerlink&quot; title=&quot;结合 Apache Hive 和 Druid 实现高速 OLAP 查询&quot;&gt;&lt;/a&gt;结合 Apache Hive 和 Druid 实现高速 OLAP 查询&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Part1Image2.png&quot; alt=&quot;使用 HiveQL 预汇总数据并保存至 Druid&quot;&gt;&lt;/p&gt;
&lt;p&gt;Hadoop 生态中，我们使用 Hive 将 SQL 语句编译为 MapReduce 任务，对海量数据进行操作；Druid 则是一款独立的分布式列式存储系统，通常用于执行面向最终用户的即席查询和实时分析。&lt;/p&gt;
&lt;p&gt;Druid 的高速查询主要得益于列式存储和倒排索引，其中倒排索引是和 Hive 的主要区别。数据表中的维度字段越多，查询速度也会越快。不过 Druid 也有其不适用的场景，如无法支持大数据量的 Join 操作，对标准 SQL 的实现也十分有限。&lt;/p&gt;
&lt;p&gt;Druid 和 Hive 的结合方式是这样的：首先使用 Hive 对数据进行预处理，生成 OLAP Cube 存入 Druid；当发生查询时，使用 Calcite 优化器进行分析，使用合适的引擎（Hive 或 Druid）执行操作。如，Druid 擅长执行维度汇总、TopN、时间序列查询，而 Hive 则能胜任 Join、子查询、UDF 等操作。&lt;/p&gt;
&lt;p&gt;原文：&lt;a href=&quot;https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru&quot;&gt;https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Digest" scheme="http://shzhangji.com/categories/Digest/"/>
    
    
      <category term="hive" scheme="http://shzhangji.com/tags/hive/"/>
    
      <category term="druid" scheme="http://shzhangji.com/tags/druid/"/>
    
      <category term="machine learning" scheme="http://shzhangji.com/tags/machine-learning/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>开发人员必知的5种开源框架</title>
    <link href="http://shzhangji.com/2016/03/13/top-5-frameworks/"/>
    <id>http://shzhangji.com/2016/03/13/top-5-frameworks/</id>
    <published>2016-03-13T08:25:00.000Z</published>
    <updated>2017-03-09T04:26:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://opensource.com/business/15/12/top-5-frameworks" target="_blank" rel="external">John Esposito</a></p>
<p>软件侵吞着世界<a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460" target="_blank" rel="external">已经四年多了</a>，但开发人员看待软件的方式稍有不同。我们一直在致力于<a href="http://www.dougengelbart.org/pubs/augment-3906.html" target="_blank" rel="external">解决实际问题</a>，而<a href="http://worrydream.com/refs/Brooks-NoSilverBullet.pdf" target="_blank" rel="external">很少思考软件开发的基石</a>。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html" target="_blank" rel="external">产生泄漏</a>的抽象工具就显得越来越重要。</p>
<p>简单地来说，在那些追求生产效率的开发者眼中，<em>框架</em>正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？</p>
<p>开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。</p>
<h2 id="1-展现层：Bootstrap"><a href="#1-展现层：Bootstrap" class="headerlink" title="1. 展现层：Bootstrap"></a>1. 展现层：Bootstrap</h2><p>我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是<a href="http://getbootstrap.com/" target="_blank" rel="external">Bootstrap</a>。Bootstrap的<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln" target="_blank" rel="external">流行度</a>非常之惊人，<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">远远甩开</a>了它的老对手<a href="http://foundation.zurb.com/" target="_blank" rel="external">Foundation</a>，以及新星<a href="http://www.getmdl.io/" target="_blank" rel="external">Material Design Lite</a>。在<a href="http://trends.builtwith.com/docinfo/Twitter-Bootstrap" target="_blank" rel="external">BuiltWith</a>上，Bootstrap占据主导地位；而在GitHub上则长期保持<a href="https://github.com/search?q=stars:%3E1&amp;s=stars&amp;type=Repositories" target="_blank" rel="external">Star数</a>和<a href="https://github.com/search?o=desc&amp;q=stars:%3E1&amp;s=forks&amp;type=Repositories" target="_blank" rel="external">Fork数</a>最多的记录。</p>
<p>如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了<a href="http://v4-alpha.getbootstrap.com/" target="_blank" rel="external">v4</a><a href="http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/" target="_blank" rel="external">内测版</a>，庆祝它的四岁生日。这个版本是对现有功能的<a href="http://v4-alpha.getbootstrap.com/migration/" target="_blank" rel="external">简化和扩充</a>，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了<a href="http://themes.getbootstrap.com/" target="_blank" rel="external">官方主题市场</a>，进一步扩充现有的<a href="https://www.google.com/search?q=bootstrap+theme+sites" target="_blank" rel="external">主题生态</a>。</p>
<h2 id="2-网页MVC：AngularJS"><a href="#2-网页MVC：AngularJS" class="headerlink" title="2. 网页MVC：AngularJS"></a>2. 网页MVC：AngularJS</h2><p>随着网页平台技术越来越<a href="https://www.w3.org/blog/news/" target="_blank" rel="external">成熟</a>，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">最</a><a href="https://www.pluralsight.com/browse#tab-courses-popular" target="_blank" rel="external">流行</a>的SPA框架当属<a href="https://angularjs.org/" target="_blank" rel="external">AngularJS</a>。</p>
<p>AngularJS有什么特别之处呢？一个词：指令（<a href="https://docs.angularjs.org/guide/directive" target="_blank" rel="external">directive</a>）。一个简单的<code>ng-</code>就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。</p>
<p>有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在<a href="https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2" target="_blank" rel="external">Angular2</a>中移除，已经<a href="http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html" target="_blank" rel="external">临近公测</a>。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。</p>
<a id="more"></a>
<h2 id="3-企业级Java：Spring-Boot"><a href="#3-企业级Java：Spring-Boot" class="headerlink" title="3. 企业级Java：Spring Boot"></a>3. 企业级Java：Spring Boot</h2><p>Java的优点是什么？运行速度快，成熟，完善的类库，庞大的生态环境，一处编译处处执行，活跃的社区等等——除了痛苦的项目起始阶段。即便是最忠实的Java开发者也会转而使用Ruby或Python来快速编写一些只会用到一次的小型脚本（别不承认）。然而，鉴于以上种种原因，Java仍然是企业级应用的首选语言。</p>
<p>这时，Spring Boot出现了，它是模板代码的终结者，有了它，你就能在一条推文中写出一个Java应用程序来：</p>
<p><a href="https://twitter.com/rob_winch/status/364871658483351552" target="_blank" rel="external">https://twitter.com/rob_winch/status/364871658483351552</a></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spring run app.groovy</span></div><div class="line"><span class="meta">@Controller</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThisWillActuallyRun</span> &#123;</span></div><div class="line">  <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</div><div class="line">  <span class="meta">@ResponseBody</span></div><div class="line">  String home() &#123;</div><div class="line">    <span class="string">"Hello World!"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>没有让人不快的XML配置，无需生成别扭的代码。这怎么可能？很简单，Spring Boot在背后做了很多工作，读上面的代码就能看出，框架会自动生成一个内嵌的servlet容器，监听8080端口，处理接收到的请求。这些都无需用户配置，而是遵从Spring Boot的约定。</p>
<p>Spring Boot有多流行？它是目前为止<a href="https://github.com/spring-projects" target="_blank" rel="external">fork数</a>和下载量最高的Spring应用（主框架除外）。2015年，<a href="https://www.google.com/trends/explore#q=spring%20boot%2C%20spring%20framework&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">在谷歌上搜索Spring Boot的人数首次超过搜索Spring框架的人</a>。</p>
<h2 id="4-数据处理：Apache-Spark"><a href="#4-数据处理：Apache-Spark" class="headerlink" title="4. 数据处理：Apache Spark"></a>4. 数据处理：Apache Spark</h2><p>很久很久以前（2004年），谷歌研发出一种编程模型（<a href="http://ayende.com/blog/4435/map-reduce-a-visual-explanation" target="_blank" rel="external">MapReduce</a>），将分布式批处理任务通用化了，并撰写了一篇<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank" rel="external">著名的论文</a>。之后，Yahoo的工程师用Java编写了一个框架（<a href="https://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a>）实现了MapReduce，以及一个<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="external">分布式文件系统</a>，使得MapReduce任务能够更简便地读写数据。</p>
<p>将近十年的时间，Hadoop主宰了大数据处理框架的生态系统，即使批处理只能解决有限的问题——也许大多数企业和科学工作者已经习惯了大数据量的批处理分析吧。然而，并不是所有大型数据集都适合使用批处理。特别地，流式数据（如传感器数据）和迭代式数据分析（机器学习算法中最为常见）都不适合使用批处理。因此，大数据领域诞生了很多<a href="https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan" target="_blank" rel="external">新的编程模型、应用架构</a>、以及各类<a href="http://www.journalofbigdata.com/content/2/1/18" target="_blank" rel="external">数据存储</a>也逐渐流行开来（甚至还从MapReduce中分离出了一种<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">新的集群管理系统</a>）。</p>
<p>在这些新兴的系统之中，<a href="http://spark.apache.org/research.html" target="_blank" rel="external">伯克利AMPLab</a>研发的<a href="http://spark.apache.org/" target="_blank" rel="external">Apache Spark</a>在2015年脱颖而出。各类调研和报告（<a href="https://dzone.com/guides/big-data-business-intelligence-and-analytics-2015" target="_blank" rel="external">DZone</a>、<a href="http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf" target="_blank" rel="external">Databricks</a>、<a href="https://info.typesafe.com/COLL-20XX-Spark-Survey-Report_LP.html?lst=PR&amp;lsd=COLL-20XX-Spark-Survey-Trends-Adoption-Report" target="_blank" rel="external">Typesafe</a>）都显示Spark的成长速度非常之快。GitHub提交数从2013年开始就呈<a href="https://github.com/apache/spark/graphs/contributors" target="_blank" rel="external">线性增长</a>，而谷歌趋势则在2015年呈现出<a href="https://www.google.com/trends/explore#q=%2Fm%2F0ndhxqz" target="_blank" rel="external">指数级的增长</a>。</p>
<p>Spark如此流行，它究竟是做什么的呢？答案很简单，非常快速的批处理，不过这点是构建在Spark的一个杀手级特性之上的，能够应用到比Hadoop多得多的编程模型中。Spark将数据表达为弹性分布式数据集（RDD），处理结果保存在多个节点的内存中，不进行复制，只是记录数据的计算过程（这点可以和<a href="http://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">CQRS</a>、<a href="http://plato.stanford.edu/entries/peirce/" target="_blank" rel="external">实用主义</a>、<a href="http://people.cs.uchicago.edu/~fortnow/papers/kaikoura.pdf" target="_blank" rel="external">Kolmogorov复杂度</a>相较）。这一特点可以让迭代算法无需从底层（较慢的）分布式存储层读取数据。同时也意味着批处理流程无需再背负Nathan Marz在<a href="http://lambda-architecture.net/" target="_blank" rel="external">Lambda架构</a>中所描述的“数据卡顿”之恶名。RDD还能让Spark模拟实时流数据处理，通过将数据切分成小块，降低延迟时间，达到大部分应用对“准实时”的要求。</p>
<h2 id="5-软件交付：Docker"><a href="#5-软件交付：Docker" class="headerlink" title="5. 软件交付：Docker"></a>5. 软件交付：Docker</h2><p>严格意义上说，<a href="https://www.docker.com/" target="_blank" rel="external">Docker</a>并不是符合“框架”的定义：代码库，通用性好，使用一系列特殊约定来解决大型重复性问题。但是，如果框架指的是能够让程序员在一种舒适的抽象层之上进行编码，那Docker将是框架中的佼佼者（我们可以称它为“外壳型框架”，多制造一些命名上的混乱吧）。而且，如果将本文的标题改为“开发人员必知的5样东西”，又不把Docker包含进来，就显得太奇怪了。</p>
<p>为什么Docker很出色？首先我们应该问什么容器很受欢迎（FreeBSD Jail, Solaries Zones, OpenVZ, LXC）？很简单：无需使用一个完整的操作系统来实现隔离；或者说，在获得安全性和便利性的提升时，无需承担额外的开销。但是，隔离也有很多种形式（比如最先想到的<code>chroot</code>，以及各种虚拟内存技术），而且可以非常方便地用<code>systemd-nspawn</code>来启动程序，而不使用Docker。然而，仅仅隔离进程还不够，那Docker有什么<a href="http://techapostle.blogspot.com/2015/04/the-3-reasons-why-docker-got-it-right.html" target="_blank" rel="external">过人之处</a>呢？</p>
<p>两个原因：Dockefile（新型的tar包）增加了便携性；它的格式成为了现行的标准。第一个原因使得应用程序交付变得容易（之前人们是通过创建轻型虚拟机来实现的）。第二个原因则让容器更容易分享（不单是<a href="https://hub.docker.com/" target="_blank" rel="external">DockerHub</a>）。我可以很容易地尝试你编写的应用程序，而不是先要做些不相关的事（想想<code>apt-get</code>给你的体验）。</p>
<h2 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h2><p><img src="https://opensource.com/sites/default/files/styles/profile_pictures/public/pictures/john-esposito.jpg?itok=xPVFPzr2" alt="John Esposito"></p>
<p>John Esposito是DZone的主编，最近刚刚完成古典学博士学位的学习，养了两只猫。之前他是VBA和Force.com的开发者、DBA、网络管理员。（但说真的，Lisp是最棒的！）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://opensource.com/business/15/12/top-5-frameworks&quot;&gt;John Esposito&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;软件侵吞着世界&lt;a href=&quot;http://www.wsj.com/articles/SB10001424053111903480904576512250915629460&quot;&gt;已经四年多了&lt;/a&gt;，但开发人员看待软件的方式稍有不同。我们一直在致力于&lt;a href=&quot;http://www.dougengelbart.org/pubs/augment-3906.html&quot;&gt;解决实际问题&lt;/a&gt;，而&lt;a href=&quot;http://worrydream.com/refs/Brooks-NoSilverBullet.pdf&quot;&gt;很少思考软件开发的基石&lt;/a&gt;。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么&lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;产生泄漏&lt;/a&gt;的抽象工具就显得越来越重要。&lt;/p&gt;
&lt;p&gt;简单地来说，在那些追求生产效率的开发者眼中，&lt;em&gt;框架&lt;/em&gt;正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？&lt;/p&gt;
&lt;p&gt;开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。&lt;/p&gt;
&lt;h2 id=&quot;1-展现层：Bootstrap&quot;&gt;&lt;a href=&quot;#1-展现层：Bootstrap&quot; class=&quot;headerlink&quot; title=&quot;1. 展现层：Bootstrap&quot;&gt;&lt;/a&gt;1. 展现层：Bootstrap&lt;/h2&gt;&lt;p&gt;我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是&lt;a href=&quot;http://getbootstrap.com/&quot;&gt;Bootstrap&lt;/a&gt;。Bootstrap的&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln&quot;&gt;流行度&lt;/a&gt;非常之惊人，&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot;&gt;远远甩开&lt;/a&gt;了它的老对手&lt;a href=&quot;http://foundation.zurb.com/&quot;&gt;Foundation&lt;/a&gt;，以及新星&lt;a href=&quot;http://www.getmdl.io/&quot;&gt;Material Design Lite&lt;/a&gt;。在&lt;a href=&quot;http://trends.builtwith.com/docinfo/Twitter-Bootstrap&quot;&gt;BuiltWith&lt;/a&gt;上，Bootstrap占据主导地位；而在GitHub上则长期保持&lt;a href=&quot;https://github.com/search?q=stars:%3E1&amp;amp;s=stars&amp;amp;type=Repositories&quot;&gt;Star数&lt;/a&gt;和&lt;a href=&quot;https://github.com/search?o=desc&amp;amp;q=stars:%3E1&amp;amp;s=forks&amp;amp;type=Repositories&quot;&gt;Fork数&lt;/a&gt;最多的记录。&lt;/p&gt;
&lt;p&gt;如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了&lt;a href=&quot;http://v4-alpha.getbootstrap.com/&quot;&gt;v4&lt;/a&gt;&lt;a href=&quot;http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/&quot;&gt;内测版&lt;/a&gt;，庆祝它的四岁生日。这个版本是对现有功能的&lt;a href=&quot;http://v4-alpha.getbootstrap.com/migration/&quot;&gt;简化和扩充&lt;/a&gt;，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了&lt;a href=&quot;http://themes.getbootstrap.com/&quot;&gt;官方主题市场&lt;/a&gt;，进一步扩充现有的&lt;a href=&quot;https://www.google.com/search?q=bootstrap+theme+sites&quot;&gt;主题生态&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;2-网页MVC：AngularJS&quot;&gt;&lt;a href=&quot;#2-网页MVC：AngularJS&quot; class=&quot;headerlink&quot; title=&quot;2. 网页MVC：AngularJS&quot;&gt;&lt;/a&gt;2. 网页MVC：AngularJS&lt;/h2&gt;&lt;p&gt;随着网页平台技术越来越&lt;a href=&quot;https://www.w3.org/blog/news/&quot;&gt;成熟&lt;/a&gt;，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot;&gt;最&lt;/a&gt;&lt;a href=&quot;https://www.pluralsight.com/browse#tab-courses-popular&quot;&gt;流行&lt;/a&gt;的SPA框架当属&lt;a href=&quot;https://angularjs.org/&quot;&gt;AngularJS&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;AngularJS有什么特别之处呢？一个词：指令（&lt;a href=&quot;https://docs.angularjs.org/guide/directive&quot;&gt;directive&lt;/a&gt;）。一个简单的&lt;code&gt;ng-&lt;/code&gt;就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。&lt;/p&gt;
&lt;p&gt;有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在&lt;a href=&quot;https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2&quot;&gt;Angular2&lt;/a&gt;中移除，已经&lt;a href=&quot;http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html&quot;&gt;临近公测&lt;/a&gt;。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="translation" scheme="http://shzhangji.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>使用Spring AOP向领域模型注入依赖</title>
    <link href="http://shzhangji.com/2015/09/12/model-dependency-injection-with-spring-aop/"/>
    <id>http://shzhangji.com/2015/09/12/model-dependency-injection-with-spring-aop/</id>
    <published>2015-09-12T14:03:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://shzhangji.com/blog/2015/09/05/anemic-domain-model/">贫血领域模型</a>这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用<code>new</code>关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。</p>
<h2 id="面向切面编程"><a href="#面向切面编程" class="headerlink" title="面向切面编程"></a>面向切面编程</h2><p>面向切面编程，或<a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming" target="_blank" rel="external">AOP</a>，是一种编程范式，和面向对象编程（<a href="https://en.wikipedia.org/wiki/Object-oriented_programming" target="_blank" rel="external">OOP</a>）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。</p>
<p>对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，<a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop" target="_blank" rel="external">Spring AOP</a>已经提供了<code>@Configurable</code>注解来帮助我们实现这一需求。</p>
<a id="more"></a>
<h2 id="Configurable注解"><a href="#Configurable注解" class="headerlink" title="Configurable注解"></a>Configurable注解</h2><p>Spring应用程序会定义一个上下文容器，在该容器内创建的对象会由Spring负责注入依赖。对于容器外创建的对象，我们可以使用<code>@Configurable</code>来修饰类，告知Spring对这些类的实例也进行依赖注入。</p>
<p>假设有一个<code>Report</code>类（领域模型），其中一个方法需要解析JSON，我们可以使用<code>@Configurable</code>将容器内的<code>ObjectMapper</code>对象注入到类的实例中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Entity</span></div><div class="line"><span class="meta">@Configurable</span>(autowire = Autowire.BY_TYPE)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Report</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Id</span> <span class="meta">@GeneratedValue</span></div><div class="line">    <span class="keyword">private</span> Integer id;</div><div class="line">    </div><div class="line">    <span class="meta">@Autowired</span> <span class="meta">@Transient</span></div><div class="line">    <span class="keyword">private</span> ObjectMapper mapper;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">render</span><span class="params">()</span> </span>&#123;</div><div class="line">        mapper.readValue(...);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>autowire</code>参数默认是<code>NO</code>，因此需要显式打开，否则只能使用XML定义依赖。<code>@Autowired</code>是目前比较推荐的注入方式。</li>
<li><code>@Transient</code>用于告知JPA该属性不需要进行持久化。你也可以使用<code>transient</code>关键字来声明，效果相同。</li>
<li>项目依赖中需要包含<code>spring-aspects</code>。如果已经使用了<code>spring-boot-starter-data-jpa</code>，则无需配置。</li>
<li>应用程序配置中需要加入<code>@EnableSpringConfigured</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SpringBootApplication</span></div><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div><div class="line"><span class="meta">@EnableSpringConfigured</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        SpringApplication.run(Application.class, args);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>在<code>src/main/resources</code>目录下，新建<code>META-INF/aop.xml</code>文件，用来限定哪些包会用到AOP。否则，AOP的织入操作会作用于所有的类（包括第三方类库），产生不必要的的报错信息。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&lt;!DOCTYPE aspectj PUBLIC "-//AspectJ//DTD//EN" "http://www.eclipse.org/aspectj/dtd/aspectj.dtd"&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">aspectj</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">weaver</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">include</span> <span class="attr">within</span>=<span class="string">"com.foobar..*"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">weaver</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">aspectj</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="运行时织入（Load-Time-Weaving-LTW）"><a href="#运行时织入（Load-Time-Weaving-LTW）" class="headerlink" title="运行时织入（Load-Time Weaving, LTW）"></a>运行时织入（Load-Time Weaving, LTW）</h2><p>除了项目依赖和应用程序配置，我们还需要选择一种织入方式来使AOP生效。Spring AOP推荐的方式是运行时织入，并提供了一个专用的Jar包。运行时织入的原理是：当类加载器在读取类文件时，动态修改类的字节码。这一机制是从<a href="http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/instrument/package-summary.html" target="_blank" rel="external">JDK1.5</a>开始提供的，需要使用<code>-javaagent</code>参数开启，如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/spring-instrument.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>在测试时发现，Spring AOP提供的这一Jar包对普通的类是有效果的，但对于使用<code>@Entity</code>修饰的类就没有作用了。因此，我们改用AspectJ提供的Jar包（可到<a href="http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.aspectj%22%20AND%20a%3A%22aspectjweaver%22" target="_blank" rel="external">Maven中央仓库</a>下载）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/aspectjweaver.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>对于<a href="http://projects.spring.io/spring-boot/" target="_blank" rel="external">Spring Boot</a>应用程序，可以在Maven命令中加入以下参数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mvn spring-boot:run -Drun.agent=/path/to/aspectjweaver.jar</div></pre></td></tr></table></figure>
<p>此外，在使用AspectJ作为LTW的提供方后，会影响到Spring的事务管理，因此需要在应用程序配置中加入：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div></pre></td></tr></table></figure>
<h2 id="AnnotationBeanConfigurerAspect"><a href="#AnnotationBeanConfigurerAspect" class="headerlink" title="AnnotationBeanConfigurerAspect"></a>AnnotationBeanConfigurerAspect</h2><p>到这里我们已经通过简单配置完成了领域模型的依赖注入，这背后都是Spring中的<code>AnnotationBeanConfigurerAspect</code>在做工作。我们不妨浏览一下精简后的源码：</p>
<p><a href="https://github.com/spring-projects/spring-framework/blob/master/spring-aspects/src/main/java/org/springframework/beans/factory/aspectj/AnnotationBeanConfigurerAspect.aj" target="_blank" rel="external">AnnotationBeanConfigurerAspect.aj</a></p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">aspect</span> <span class="title">AnnotationBeanConfigurerAspect</span> <span class="keyword">implements</span> <span class="title">BeanFactoryAware</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="keyword">private</span> BeanConfigurerSupport beanConfigurerSupport = <span class="keyword">new</span> BeanConfigurerSupport();</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setBeanFactory</span><span class="params">(BeanFactory beanFactory)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.setBeanFactory(beanFactory);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">configureBean</span><span class="params">(Object bean)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.configureBean(bean);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">inConfigurableBean</span>() : @<span class="keyword">this</span>(Configurable);</div><div class="line"></div><div class="line">	<span class="keyword">declare</span> <span class="keyword">parents</span>: @Configurable * <span class="keyword">implements</span> ConfigurableObject;</div><div class="line">	</div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">beanConstruction</span>(Object bean) :</div><div class="line">			<span class="keyword">initialization</span>(ConfigurableObject+.<span class="keyword">new</span>(..)) &amp;&amp; <span class="keyword">this</span>(bean);</div><div class="line"></div><div class="line">	<span class="keyword">after</span>(Object bean) <span class="keyword">returning</span> :</div><div class="line">		<span class="title">beanConstruction</span>(bean) &amp;&amp; inConfigurableBean() &#123;</div><div class="line">		configureBean(bean);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>.aj</code>文件是AspectJ定义的语言，增加了pointcut、after等关键字，用来定义切点、通知等；</li>
<li><code>inConfigurationBean</code>切点用于匹配使用<code>Configurable</code>修饰的类型；</li>
<li><code>declare parents</code>将这些类型声明为<code>ConfigurableObject</code>接口，从而匹配<code>beanConstruction</code>切点；</li>
<li><code>ConfigurableObject+.new(..)</code>表示匹配该类型所有的构造函数；</li>
<li><code>after</code>定义一个通知，表示对象创建完成后执行<code>configureBean</code>方法；</li>
<li>该方法会调用<code>BeanConfigurerSupport</code>来对新实例进行依赖注入。</li>
</ul>
<h2 id="其它方案"><a href="#其它方案" class="headerlink" title="其它方案"></a>其它方案</h2><ol>
<li>将依赖作为参数传入。比如上文中的<code>render</code>方法可以定义为<code>render(ObjectMapper mapper)</code>。</li>
<li>将<code>ApplicationContext</code>作为某个类的静态成员，领域模型通过这个引用来获取依赖。</li>
<li>编写一个工厂方法，所有新建对象都要通过这个方法生成，进行依赖注入。</li>
<li>如果领域模型大多从数据库获得，并且JPA的提供方是Hibernate，则可以使用它的拦截器功能进行依赖注入。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable" target="_blank" rel="external">http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable</a></li>
<li><a href="http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html" target="_blank" rel="external">http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html</a></li>
<li><a href="http://jblewitt.com/blog/?p=129" target="_blank" rel="external">http://jblewitt.com/blog/?p=129</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://shzhangji.com/blog/2015/09/05/anemic-domain-model/&quot;&gt;贫血领域模型&lt;/a&gt;这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用&lt;code&gt;new&lt;/code&gt;关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。&lt;/p&gt;
&lt;h2 id=&quot;面向切面编程&quot;&gt;&lt;a href=&quot;#面向切面编程&quot; class=&quot;headerlink&quot; title=&quot;面向切面编程&quot;&gt;&lt;/a&gt;面向切面编程&lt;/h2&gt;&lt;p&gt;面向切面编程，或&lt;a href=&quot;https://en.wikipedia.org/wiki/Aspect-oriented_programming&quot;&gt;AOP&lt;/a&gt;，是一种编程范式，和面向对象编程（&lt;a href=&quot;https://en.wikipedia.org/wiki/Object-oriented_programming&quot;&gt;OOP&lt;/a&gt;）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。&lt;/p&gt;
&lt;p&gt;对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，&lt;a href=&quot;http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop&quot;&gt;Spring AOP&lt;/a&gt;已经提供了&lt;code&gt;@Configurable&lt;/code&gt;注解来帮助我们实现这一需求。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>贫血领域模型</title>
    <link href="http://shzhangji.com/2015/09/05/anemic-domain-model/"/>
    <id>http://shzhangji.com/2015/09/05/anemic-domain-model/</id>
    <published>2015-09-05T11:02:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.martinfowler.com/bliki/AnemicDomainModel.html" target="_blank" rel="external">http://www.martinfowler.com/bliki/AnemicDomainModel.html</a></p>
<p>贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为<a href="http://martinfowler.com/eaaCatalog/domainModel.html" target="_blank" rel="external">领域模型</a>的推广者，我们觉得这不是一件好事。</p>
<p>贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。</p>
<p>这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。</p>
<a id="more"></a>
<p>如今，面向对象的概念已经传播得很广泛了，而要反对这种贫血领域模型的做法，我还需要更多论据。贫血领域模型的根本问题在于，它引入了领域模型设计的所有成本，却没有带来任何好处。最主要的成本是将对象映射到数据库中，从而产生了一个对象关系映射层。只有当你充分使用了面向对象设计来组织复杂的业务逻辑后，这一成本才能够被抵消。如果将所有行为都写入到Service对象，那最终你会得到一组<a href="http://martinfowler.com/eaaCatalog/transactionScript.html" target="_blank" rel="external">事务处理脚本</a>，从而错过了领域模型带来的好处。正如我在<a href="http://martinfowler.com/books/eaa.html" target="_blank" rel="external">企业应用架构模式</a>一书中说到的，领域模型并不一定是最好的工具。</p>
<p>还需要强调的是，将行为放入领域模型，这点和分层设计（领域层、持久化层、展现层等）并不冲突。因为领域模型中放入的是和领域相关的逻辑——验证、计算、商业规则等。如果你要讨论能否将数据访问和展现逻辑放入到领域模型中，这就不在本文论述范围之内了。</p>
<p>一些面向对象专家的观点有时会让人产生疑惑，他们认为的确应该有一个面向过程的<a href="http://martinfowler.com/eaaCatalog/serviceLayer.html" target="_blank" rel="external">服务层</a>。但是，这并不意味着领域模型就不应该包含行为。事实上，服务层需要和一组富含行为的领域模型结合起来使用。</p>
<p>Eric Evans的<a href="http://domaindrivendesign.org/books/" target="_blank" rel="external">领域驱动设计</a>一书中有关于分层的论述：</p>
<blockquote>
<p>应用层（也就是上文中的服务层）：用来描述应用程序所要做的工作，并调度丰富的领域模型来完成它。这个层次的任务是描述业务逻辑，或和其它项目的应用层做交互。这个层次很薄，它不包含任何业务规则或知识，仅用于调度和派发任务给下一层的领域模型。这个层次没有业务状态，但可以为用户或程序提供任务状态。</p>
<p>领域层（或者叫模型层）：用于表示业务逻辑、业务场景和规则。这个层次会控制和使用业务状态，即使这些状态最终会交由持久化层来存储。总之，这个层次是软件的核心。</p>
</blockquote>
<p>关键点在于服务层是很薄的——所有重要的业务逻辑都写在领域层。他在服务模式中复述了这一观点：</p>
<blockquote>
<p>如今人们常犯的错误是不愿花时间将业务逻辑放置到合适的领域模型中，从而逐渐形成面向过程的程序设计。</p>
</blockquote>
<p>我不清楚为什么这种反模式会那么常见。我怀疑是因为大多数人并没有使用过一个设计良好的领域模型，特别是那些以数据为中心的开发人员。此外，有些技术也会推动这种反模式，比如J2EE的Entity Bean，这会让我更倾向于使用<a href="http://www.martinfowler.com/bliki/POJO.html" target="_blank" rel="external">POJO</a>领域模型。</p>
<p>总之，如果你将大部分行为都放置在服务层，那么你就会失去领域模型带来的好处。如果你将所有行为都放在服务层，那就无可救药了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.martinfowler.com/bliki/AnemicDomainModel.html&quot;&gt;http://www.martinfowler.com/bliki/AnemicDomainModel.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为&lt;a href=&quot;http://martinfowler.com/eaaCatalog/domainModel.html&quot;&gt;领域模型&lt;/a&gt;的推广者，我们觉得这不是一件好事。&lt;/p&gt;
&lt;p&gt;贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。&lt;/p&gt;
&lt;p&gt;这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>HotSpot JVM中的对象指针压缩</title>
    <link href="http://shzhangji.com/2015/06/25/compressed-oops-in-the-hotspot-jvm/"/>
    <id>http://shzhangji.com/2015/06/25/compressed-oops-in-the-hotspot-jvm/</id>
    <published>2015-06-25T09:41:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://wiki.openjdk.java.net/display/HotSpot/CompressedOops" target="_blank" rel="external">https://wiki.openjdk.java.net/display/HotSpot/CompressedOops</a></p>
<h2 id="什么是一般对象指针？"><a href="#什么是一般对象指针？" class="headerlink" title="什么是一般对象指针？"></a>什么是一般对象指针？</h2><p>一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。</p>
<p>在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：</p>
<ul>
<li><a href="https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh" target="_blank" rel="external">Self</a>：一门基于原型的语言，是Smalltalk的近亲</li>
<li><a href="http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects" target="_blank" rel="external">Strongtalk</a>：Smalltalk的一种实现</li>
<li><a href="http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp" target="_blank" rel="external">Hotspot</a></li>
<li><a href="http://code.google.com/p/v8/source/browse/trunk/src/objects.h" target="_blank" rel="external">V8</a></li>
</ul>
<p>部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。</p>
<h2 id="为什么需要压缩？"><a href="#为什么需要压缩？" class="headerlink" title="为什么需要压缩？"></a>为什么需要压缩？</h2><p>在<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">LP64</a>系统中，指针需要使用64位来表示；<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">ILP32</a>系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。</p>
<p>在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。</p>
<p>压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。</p>
<p>我们使用<em>解码</em>来表示从32位对象指针转换成64位地址的过程，其反过程则称为<em>编码</em>。</p>
<a id="more"></a>
<h2 id="什么情况下会进行压缩？"><a href="#什么情况下会进行压缩？" class="headerlink" title="什么情况下会进行压缩？"></a>什么情况下会进行压缩？</h2><p>运行在ILP32模式下的Java虚拟机，或在运行时将<code>UseCompressedOops</code>标志位关闭，则所有的对象指针都不会被压缩。</p>
<p>如果<code>UseCompressedOops</code>是打开的，则以下对象的指针会被压缩：</p>
<ul>
<li>所有对象的<a href="http://stackoverflow.com/questions/16721021/what-is-klass-klassklass" target="_blank" rel="external">klass</a>属性</li>
<li>所有<a href="http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/sun/jvm/hotspot/oops/Oop.java#Oop" target="_blank" rel="external">对象指针实例</a>的属性</li>
<li>所有对象指针数组的元素（objArray）</li>
</ul>
<p>HotSpot VM中，用于表示Java类的数据结构是不会压缩的，这部分数据都存放在永久代（PermGen）中。</p>
<p>在解释器中，一般对象指针也是不压缩的，包括JVM本地变量和栈内元素、调用参数、返回值等。解释器会在读取堆内对象时解码对象指针，并在存入时进行编码。</p>
<p>同样，方法调用序列（method calling sequence），无论是解释执行还是编译执行，都不会使用对象指针压缩。</p>
<p>在编译后的代码中，对象指针是否压缩取决于不同的优化结果。优化后的代码可能会将压缩后的对象指针直接从一处搬往另一处，而不进行编解码操作。如果芯片（如x86）支持解码，那在使用对象指针时就不需要自行解码了。</p>
<p>所以，以下数据结构在编译后的代码中既可以是压缩后的对象指针，也可能是本地地址：</p>
<ul>
<li>寄存器或溢出槽（spill slot）中的数据</li>
<li>对象指针映射表（GC映射表）</li>
<li>调试信息</li>
<li>嵌套在机器码中的对象指针（在非RISC芯片中支持，如x86）</li>
<li><a href="http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html#nmethod" target="_blank" rel="external">nmethod</a>常量区（包括那些影响到机器码的重定位操作）</li>
</ul>
<p>在HotSpot JVM的C++代码部分，对象指针压缩与否反映在C++的静态类型系统中。通常情况下，对象指针是不压缩的。具体来说，C++的成员函数在操作本地代码传递过来的指针时（如<em>this</em>），其执行过程不会有什么不同。JVM中的部分方法则提供了重载，能够处理压缩和不压缩的对象指针。</p>
<p>重要的C++数据不会被压缩：</p>
<ul>
<li>C++对象指针（<em>this</em>）</li>
<li>受托管指针的句柄（Handle类型等）</li>
<li>JNI句柄（jobject类型）</li>
</ul>
<p>C++在使用对象指针压缩时（加载和存储等），会以<code>narrowOop</code>作为标记。</p>
<h2 id="使用压缩寻址"><a href="#使用压缩寻址" class="headerlink" title="使用压缩寻址"></a>使用压缩寻址</h2><p>以下是使用对象指针压缩的x86指令示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! int R8; oop[] R9;  // R9是64位</div><div class="line">! oop R10 = R9[R8];  // R10是32位</div><div class="line">! 从原始基址指针加载压缩对象指针：</div><div class="line">movl R10, [R9 + R8&lt;&lt;3 + 16]</div><div class="line">! klassOop R11 = R10._klass;  // R11是32位</div><div class="line">! void* const R12 = GetHeapBase();</div><div class="line">! 从压缩基址指针加载klass指针：</div><div class="line">movl R11, [R12 + R10&lt;&lt;3 + 8]</div></pre></td></tr></table></figure>
<p>以下sparc指令用于解压对象指针（可为空）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! java.lang.Thread::getThreadGroup@1 (line 1072)</div><div class="line">! L1 = L7.group</div><div class="line">ld  [ %l7 + 0x44 ], %l1</div><div class="line">! L3 = decode(L1)</div><div class="line">cmp  %l1, 0</div><div class="line">sllx  %l1, 3, %l3</div><div class="line">brnz,a   %l3, .+8</div><div class="line">add  %l3, %g6, %l3  ! %g6是常量堆基址</div></pre></td></tr></table></figure>
<p><em>输出中的注解来自<a href="https://wiki.openjdk.java.net/display/HotSpot/PrintAssembly" target="_blank" rel="external">PrintAssembly插件</a>。</em></p>
<h2 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h2><p>32位零值会被解压为64位空值，这就需要在解码逻辑中加入一段特殊的逻辑。或者说可以默认某些压缩对象指针肯定不会空（如klass的属性），这样就能使用简单一些的编解码逻辑了。</p>
<p>隐式空值检测对JVM的性能至关重要，包括解释执行和编译执行的字节码。对于一个偏移量较小的对象指针，如果基址指针为空，那很有可能造成系统崩溃，因为虚拟地址空间的前几页通常是没有映射的。</p>
<p>对于压缩对象指针，我们可以用一种类似的技巧来欺骗它：将堆内存前几页的映射去除，如果解压出的指针为空（相对于基址指针），仍可以用它来做加载和存储的操作，隐式空值检测也能照常运行。</p>
<h2 id="对象头信息"><a href="#对象头信息" class="headerlink" title="对象头信息"></a>对象头信息</h2><p>对象头信息通常包含几个部分：固定长度的标志位；klass信息；如果对象是数组，则包含一个32位的信息，并可能追加一个32位的空隙进行对齐；零个或多个实例属性，数组元素，元信息等。（有趣的是，Klass的对象头信息包含了一个C++的<a href="https://en.wikipedia.org/wiki/Virtual_method_table" target="_blank" rel="external">虚拟方法表</a>）</p>
<p>上述追加的32位空隙通常也可用于存储属性信息。</p>
<p>如果<code>UseCompressedOops</code>关闭，标志位和klass都是正常长度。对于数组，32位空隙在LP64系统中总是存在；而ILP32系统中，只有当数组元素是64位数据时才存在这个空隙。</p>
<p>如果<code>UseCompressedOops</code>打开，则klass是32位的。非数组对象在klass后会追加一个空隙，而数组对象则直接开始存储元素信息。</p>
<h2 id="零基压缩技术"><a href="#零基压缩技术" class="headerlink" title="零基压缩技术"></a>零基压缩技术</h2><p>压缩对象指针（narrow-oop）是基于某个地址的偏移量，这个基础地址（narrow-oop-base）是由Java堆内存基址减去一个内存页的大小得来的，从而支持隐式空值检测。所以一个属性字段的地址可以这样得到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3) + &lt;field-offset&gt;.</div></pre></td></tr></table></figure>
<p>如果基础地址可以是0（Java堆内存不一定要从0偏移量开始），那么公式就可以简化为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(&lt;narrow-oop &lt;&lt; 3) + &lt;field-offset&gt;</div></pre></td></tr></table></figure>
<p>理论上说，这一步可以省去一次寄存器上的加和操作。而且使用零基压缩技术后，空值检测也就不需要了。</p>
<p>之前的解压代码是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if (&lt;narrow-oop&gt; == NULL)</div><div class="line">    &lt;wide_oop&gt; = NULL</div><div class="line">else</div><div class="line">    &lt;wide_oop&gt; = &lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3)</div></pre></td></tr></table></figure>
<p>使用零基压缩后，只需使用移位操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;wide_oop&gt; = &lt;narrow-oop&gt; &lt;&lt; 3</div></pre></td></tr></table></figure>
<p>零基压缩技术会根据堆内存的大小以及平台特性来选择不同的策略：</p>
<ol>
<li>堆内存小于4Gb，直接使用压缩对象指针进行寻址，无需压缩和解压；</li>
<li>堆内存大于4Gb，则尝试分配小于32Gb的堆内存，并使用零基压缩技术；</li>
<li>如果仍然失败，则使用普通的对象指针压缩技术，即<code>narrow-oop-base</code>。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&quot;&gt;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;什么是一般对象指针？&quot;&gt;&lt;a href=&quot;#什么是一般对象指针？&quot; class=&quot;headerlink&quot; title=&quot;什么是一般对象指针？&quot;&gt;&lt;/a&gt;什么是一般对象指针？&lt;/h2&gt;&lt;p&gt;一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。&lt;/p&gt;
&lt;p&gt;在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh&quot;&gt;Self&lt;/a&gt;：一门基于原型的语言，是Smalltalk的近亲&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects&quot;&gt;Strongtalk&lt;/a&gt;：Smalltalk的一种实现&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp&quot;&gt;Hotspot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/v8/source/browse/trunk/src/objects.h&quot;&gt;V8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。&lt;/p&gt;
&lt;h2 id=&quot;为什么需要压缩？&quot;&gt;&lt;a href=&quot;#为什么需要压缩？&quot; class=&quot;headerlink&quot; title=&quot;为什么需要压缩？&quot;&gt;&lt;/a&gt;为什么需要压缩？&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot;&gt;LP64&lt;/a&gt;系统中，指针需要使用64位来表示；&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot;&gt;ILP32&lt;/a&gt;系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。&lt;/p&gt;
&lt;p&gt;在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。&lt;/p&gt;
&lt;p&gt;压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。&lt;/p&gt;
&lt;p&gt;我们使用&lt;em&gt;解码&lt;/em&gt;来表示从32位对象指针转换成64位地址的过程，其反过程则称为&lt;em&gt;编码&lt;/em&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>Apache HBase的适用场景</title>
    <link href="http://shzhangji.com/2015/03/08/hbase-dos-and-donts/"/>
    <id>http://shzhangji.com/2015/03/08/hbase-dos-and-donts/</id>
    <published>2015-03-08T00:03:00.000Z</published>
    <updated>2017-03-09T04:27:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/" target="_blank" rel="external">http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</a></p>
<p>最近我在<a href="http://www.meetup.com/LA-HUG/" target="_blank" rel="external">洛杉矶Hadoop用户组</a>做了一次关于<a href="http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/" target="_blank" rel="external">HBase适用场景</a>的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：</p>
<ul>
<li>HBase很棒，但不是关系型数据库或HDFS的替代者；</li>
<li>配置得当才能运行良好；</li>
<li>监控，监控，监控，重要的事情要说三遍。</li>
</ul>
<p>Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多<a href="#use-cases">成功案例</a>，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。</p>
<a id="more"></a>
<h2 id="何时使用HBase"><a href="#何时使用HBase" class="headerlink" title="何时使用HBase"></a>何时使用HBase</h2><p>虽然HBase是一种绝佳的工具，但我们一定要记住，它并非银弹。HBase并不擅长传统的事务处理程序或关联分析，它也不能完全替代MapReduce过程中使用到的HDFS。从文末的<a href="#use-cases">成功案例</a>中你可以大致了解HBase适用于怎样的应用场景。如果你还有疑问，可以到<a href="http://www.cloudera.com/community/" target="_blank" rel="external">社区</a>中提问，我说过这是一个非常棒的社区。</p>
<p>除去上述限制之外，你为何要选择HBase呢？如果你的应用程序中，数据表每一行的结构是有差别的，那就可以考虑使用HBase，比如在标准化建模的过程中使用它；如果你需要经常追加字段，且大部分字段是NULL值的，那可以考虑HBase；如果你的数据（包括元数据、消息、二进制数据等）都有着同一个主键，那就可以使用HBase；如果你需要通过键来访问和修改数据，使用HBase吧。</p>
<h2 id="后台服务"><a href="#后台服务" class="headerlink" title="后台服务"></a>后台服务</h2><p>如果你已决定尝试一下HBase，那以下是一些部署过程中的提示。HBase会用到一些后台服务，这些服务非常关键。如果你之前没有了解过ZooKeeper，那现在是个好时候。HBase使用ZooKeeper作为它的分布式协调服务，用于选举Master等。随着HBase的发展，ZooKeeper发挥的作用越来越重要。另外，你需要搭建合适的网络基础设施，如NTP和DNS。HBase要求集群内的所有服务器时间一致，并且能正确地访问其它服务器。正确配置NTP和DNS可以杜绝一些奇怪的问题，如服务器A认为当前是明天，B认为当前是昨天；再如Master要求服务器C开启新的Region，而C不知道自己的机器名，从而无法响应。NTP和DNS服务器可以让你减少很多麻烦。</p>
<p>我前面提到过，在考虑是否使用HBase时，需要针对你自己的应用场景来进行判别。而在真正使用HBase时，监控则成了第一要务。和大多数分布式服务一样，HBase服务器宕机会有多米诺骨牌效应。如果一台服务器因内存不足开始swap数据，它会失去和Master的联系，这时Master会命令其他服务器接过这部分请求，可能会导致第二台服务器也发生宕机。所以，你需要密切监控服务器的CPU、I/O以及网络延迟，确保每台HBase服务器都在良好地工作。监控对于维护HBase集群的健康至关重要。</p>
<h2 id="HBase架构最佳实践"><a href="#HBase架构最佳实践" class="headerlink" title="HBase架构最佳实践"></a>HBase架构最佳实践</h2><p>当你找到了适用场景，并搭建起一个健康的HBase集群后，我们来看一些使用过程中的最佳实践。键的前缀要有良好的分布性。如果你使用时间戳或其他类似的递增量作为前缀，那就会让单个Region承载所有请求，而不是分布到各个Region上。此外，你需要根据Memstore和内存的大小来控制Region的数量。RegionServer的JVM堆内存应该控制在12G以内，从而避免过长的GC停顿。举个例子，在一台内存为36G的服务器上部署RegionServer，同时还运行着DataNode，那大约可以提供100个48M大小的Region。这样的配置对HDFS、HBase、以及Linux本身的文件缓存都是有利的。</p>
<p>其他一些设置包括禁用自动合并机制（默认的合并操作会在HBase启动后每隔24小时进行），改为手动的方式在低峰期间执行。你还应该配置数据文件压缩（如LZO），并将正确的配置文件加入HBase的CLASSPATH中。</p>
<h2 id="非适用场景"><a href="#非适用场景" class="headerlink" title="非适用场景"></a>非适用场景</h2><p>上文讲述了HBase的适用场景和最佳实践，以下则是一些需要规避的问题。比如，不要期许HBase可以完全替代关系型数据库——虽然它在许多方面都表现优秀。它不支持SQL，也没有优化器，更不能支持跨越多条记录的事务或关联查询。如果你用不到这些特性，那HBase将是你的不二选择。</p>
<p>在复用HBase的服务器时有一些注意事项。如果你需要保证HBase的服务器质量，同时又想在HBase上运行批处理脚本（如使用Pig从HBase中获取数据进行处理），建议还是另搭一套集群。HBase在处理大量顺序I/O操作时（如MapReduce），其CPU和内存资源将会十分紧张。将这两类应用放置在同一集群上会造成不可预估的服务延迟。此外，共享集群时还需要调低任务槽（task slot）的数量，至少要留一半的CPU核数给HBase。密切关注内存，因为一旦发生swap，HBase很可能会停止心跳，从而被集群判为无效，最终产生一系列宕机。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后要提的一点是，在加载数据到HBase时，应该使用MapReduce+HFileOutputFormat来实现。如果仅使用客户端API，不仅速度慢，也没有充分利用HBase的分布式特性。</p>
<p>用一句话概述，HBase可以让你用键来存储和搜索数据，且无需定义表结构。</p>
<h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a><a id="use-cases"></a>使用案例</h2><ul>
<li>Apache HBase: <a href="http://wiki.apache.org/hadoop/Hbase/PoweredBy" target="_blank" rel="external">Powered By HBase Wiki</a></li>
<li>Mozilla: <a href="http://blog.mozilla.com/webdev/2010/07/26/moving-socorro-to-hbase/" target="_blank" rel="external">Moving Socorro to HBase</a></li>
<li>Facebook: <a href="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html" target="_blank" rel="external">Facebook’s New Real-Time Messaging System: HBase</a></li>
<li>StumbleUpon: <a href="http://www.stumbleupon.com/devblog/hbase_at_stumbleupon/" target="_blank" rel="external">HBase at StumbleUpon</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&quot;&gt;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近我在&lt;a href=&quot;http://www.meetup.com/LA-HUG/&quot;&gt;洛杉矶Hadoop用户组&lt;/a&gt;做了一次关于&lt;a href=&quot;http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/&quot;&gt;HBase适用场景&lt;/a&gt;的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase很棒，但不是关系型数据库或HDFS的替代者；&lt;/li&gt;
&lt;li&gt;配置得当才能运行良好；&lt;/li&gt;
&lt;li&gt;监控，监控，监控，重要的事情要说三遍。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多&lt;a href=&quot;#use-cases&quot;&gt;成功案例&lt;/a&gt;，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="translation" scheme="http://shzhangji.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Reduce-side Join</title>
    <link href="http://shzhangji.com/2015/01/13/understand-reduce-side-join/"/>
    <id>http://shzhangji.com/2015/01/13/understand-reduce-side-join/</id>
    <published>2015-01-13T06:20:00.000Z</published>
    <updated>2017-03-09T07:25:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在《<a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176" target="_blank" rel="external">MapReduce Design Patterns</a>》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：</p>
<p><img src="/cnblogs/images/reduce-side-join/reduce-side-join.png" alt=""></p>
<ol>
<li>使用<a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html" target="_blank" rel="external">MultipleInputs</a>指定不同的来源表和相应的Mapper类；</li>
<li>Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；</li>
<li>Reducer在接收到同一个Key的记录后，执行以下两步：<ol>
<li>遍历Values，根据标签将来源表的记录分别放到两个List中；</li>
<li>遍历两个List，输出Join结果。</li>
</ol>
</li>
</ol>
<p>具体实现可以参考<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java" target="_blank" rel="external">这段代码</a>。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：</p>
<ol>
<li>Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；</li>
<li>如果前面几张表内存中放不下，就写入磁盘。</li>
</ol>
<a id="more"></a>
<p>按照我们的实现，Mapper输出的Key是<code>product_id</code>，Values是打了标签的产品表（Product）和订单表（Order）的记录。从数据量来看，应该缓存产品表，扫描订单表。这就要求两表记录到达Reducer时是有序的，产品表在前，边扫描边放入内存；订单表在后，边扫描边结合产品表的记录进行输出。要让Hadoop在Shuffle&amp;Sort阶段先按<code>product_id</code>排序、再按表的标签排序，就需要用到二次排序。</p>
<p>二次排序的概念很简单，将Mapper输出的Key由单一的<code>product_id</code>修改为<code>product_id+tag</code>的复合Key就可以了，但需通过以下几步实现：</p>
<h3 id="自定义Key类型"><a href="#自定义Key类型" class="headerlink" title="自定义Key类型"></a>自定义Key类型</h3><p>原来<code>product_id</code>是Text类型，我们的复合Key则要包含<code>product_id</code>和<code>tag</code>两个数据，并实现<code>WritableComparable</code>接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedKey</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TaggedKey</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Text joinKey = <span class="keyword">new</span> Text();</div><div class="line">    <span class="keyword">private</span> IntWritable tag = <span class="keyword">new</span> IntWritable();</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TaggedKey taggedKey)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> compareValue = joinKey.compareTo(taggedKey.getJoinKey());</div><div class="line">        <span class="keyword">if</span> (compareValue == <span class="number">0</span>) &#123;</div><div class="line">            compareValue = tag.compareTo(taggedKey.getTag());</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> compareValue;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// 此处省略部分代码</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到，在比较两个TaggedKey时，会先比较joinKey（即<code>product_id</code>），再比较<code>tag</code>。</p>
<h3 id="自定义分区方法"><a href="#自定义分区方法" class="headerlink" title="自定义分区方法"></a>自定义分区方法</h3><p>默认情况下，Hadoop会对Key进行哈希，以保证相同的Key会分配到同一个Reducer中。由于我们改变了Key的结构，因此需要重新编 写分区函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">TaggedKey</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(TaggedKey taggedKey, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> taggedKey.getJoinKey().hashCode() % numPartitions;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="自定义分组方法"><a href="#自定义分组方法" class="headerlink" title="自定义分组方法"></a>自定义分组方法</h3><p>同理，调用reduce函数需要传入同一个Key的所有记录，这就需要重新定义分组函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TaggedJoiningGroupingComparator</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>(TaggedKey.class, <span class="keyword">true</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</div><div class="line">        TaggedKey taggedKey1 = (TaggedKey) a;</div><div class="line">        TaggedKey taggedKey2 = (TaggedKey) b;</div><div class="line">        <span class="keyword">return</span> taggedKey1.getJoinKey().compareTo(taggedKey2.getJoinKey());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="配置Job"><a href="#配置Job" class="headerlink" title="配置Job"></a>配置Job</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">job.setMapOutputKeyClass(TaggedKey.class);</div><div class="line">job.setMapOutputValueClass(Text.class);</div><div class="line"></div><div class="line">job.setPartitionerClass(TaggedJoiningPartitioner.class);</div><div class="line">job.setGroupingComparatorClass(TaggedJoiningGroupingComparator.class);</div></pre></td></tr></table></figure>
<h3 id="MapReduce过程"><a href="#MapReduce过程" class="headerlink" title="MapReduce过程"></a>MapReduce过程</h3><p>最后，我们在Mapper阶段使用TaggedKey，在Reducer阶段按照tag进行不同的操作就可以了：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(TaggedKey key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line">        <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">    List&lt;String&gt; products = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (Text value : values) &#123;</div><div class="line">        <span class="keyword">switch</span> (key.getTag().get()) &#123;</div><div class="line">        <span class="keyword">case</span> <span class="number">1</span>: <span class="comment">// Product</span></div><div class="line">            products.add(value.toString());</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="number">2</span>: <span class="comment">// Order</span></div><div class="line">            String[] order = value.toString().split(<span class="string">","</span>);</div><div class="line">            <span class="keyword">for</span> (String productString : products) &#123;</div><div class="line">                String[] product = productString.split(<span class="string">","</span>);</div><div class="line">                List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">                output.add(order[<span class="number">0</span>]);</div><div class="line">                <span class="comment">// ...</span></div><div class="line">                context.write(NullWritable.get(), <span class="keyword">new</span> Text(StringUtils.join(output, <span class="string">","</span>)));</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">default</span>:</div><div class="line">            <span class="keyword">assert</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>遍历values时，开始都是tag=1的记录，之后都是tag=2的记录。以上代码可以<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/ReduceSideJoinJob.java" target="_blank" rel="external">在这里</a>查看。</p>
<p>对于第二个问题，超过缓存大小的记录（默认25000条）就会存入临时文件，由Hive的RowContainer类实现，具体可以看<a href="http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hive/hive-exec/0.10.0-cdh4.5.0/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#RowContainer.add%28java.util.List%29" target="_blank" rel="external">这个链接</a>。</p>
<p>需要注意的是，Hive默认是按SQL中表的书写顺序来决定排序的，因此应该将大表放在最后。如果要人工改变顺序，可以使用STREAMTABLE配置：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(a) */</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key = b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key1)</div></pre></td></tr></table></figure>
<p>但不要将这点和Map-side Join混淆，在配置了<code>hive.auto.convert.join=true</code>后，是不需要注意表的顺序的，Hive会自动将小表缓存在Mapper的内存中。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://codingjunkie.net/mapreduce-reduce-joins/" target="_blank" rel="external">http://codingjunkie.net/mapreduce-reduce-joins/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在《&lt;a href=&quot;http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176&quot;&gt;MapReduce Design Patterns&lt;/a&gt;》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/reduce-side-join/reduce-side-join.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用&lt;a href=&quot;https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html&quot;&gt;MultipleInputs&lt;/a&gt;指定不同的来源表和相应的Mapper类；&lt;/li&gt;
&lt;li&gt;Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；&lt;/li&gt;
&lt;li&gt;Reducer在接收到同一个Key的记录后，执行以下两步：&lt;ol&gt;
&lt;li&gt;遍历Values，根据标签将来源表的记录分别放到两个List中；&lt;/li&gt;
&lt;li&gt;遍历两个List，输出Join结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体实现可以参考&lt;a href=&quot;https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java&quot;&gt;这段代码&lt;/a&gt;。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；&lt;/li&gt;
&lt;li&gt;如果前面几张表内存中放不下，就写入磁盘。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="http://shzhangji.com/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://shzhangji.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>使用git rebase让历史变得清晰</title>
    <link href="http://shzhangji.com/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <id>http://shzhangji.com/2014/12/23/use-git-rebase-to-clarify-history/</id>
    <published>2014-12-23T08:10:00.000Z</published>
    <updated>2017-03-09T07:25:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>
<p><img src="/cnblogs/images/git-rebase/rebase-result.png" alt=""></p>
<h2 id="“Merge-branch”提交的产生"><a href="#“Merge-branch”提交的产生" class="headerlink" title="“Merge branch”提交的产生"></a>“Merge branch”提交的产生</h2><p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>
<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>
<h2 id="使用git-pull-–rebase"><a href="#使用git-pull-–rebase" class="headerlink" title="使用git pull –rebase"></a>使用git pull –rebase</h2><p>修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>
<a id="more"></a>
<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">nothing to commit, working directory clean</div></pre></td></tr></table></figure>
<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">Untracked files:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</div><div class="line">    test.txt</div><div class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</div></pre></td></tr></table></figure>
<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>
<h2 id="修改git-pull的默认行为"><a href="#修改git-pull的默认行为" class="headerlink" title="修改git pull的默认行为"></a>修改git pull的默认行为</h2><p>每次都加–rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config branch.master.rebase true</div></pre></td></tr></table></figure>
<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config --global branch.autosetuprebase always</div></pre></td></tr></table></figure>
<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>
<h2 id="扩展阅读-1-：git-rebase工作原理"><a href="#扩展阅读-1-：git-rebase工作原理" class="headerlink" title="扩展阅读[1]：git rebase工作原理"></a>扩展阅读[1]：git rebase工作原理</h2><p>先看看git merge的示意图：</p>
<p><img src="/cnblogs/images/git-rebase/merge.png" alt=""></p>
<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches" target="_blank" rel="external">图片来源</a></p>
<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>
<p>再来看git rebase的示意图：</p>
<p><img src="/cnblogs/images/git-rebase/rebase-1.png" alt=""></p>
<p><img src="/cnblogs/images/git-rebase/rebase-2.png" alt=""></p>
<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>
<h2 id="扩展阅读-2-：git-merge-–no-ff"><a href="#扩展阅读-2-：git-merge-–no-ff" class="headerlink" title="扩展阅读[2]：git merge –no-ff"></a>扩展阅读[2]：git merge –no-ff</h2><p>在做项目开发时会用到分支，合并时采用以下步骤：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git checkout feature-branch</div><div class="line">$ git rebase master</div><div class="line">$ git checkout master</div><div class="line">$ git merge --no-ff feature-branch</div><div class="line">$ git push origin master</div></pre></td></tr></table></figure>
<p>历史就成了这样：</p>
<p><img src="/cnblogs/images/git-rebase/no-ff.png" alt=""></p>
<p>可以看到，Merge branch ‘feature-branch’那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/git-rebase/rebase-result.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;“Merge-branch”提交的产生&quot;&gt;&lt;a href=&quot;#“Merge-branch”提交的产生&quot; class=&quot;headerlink&quot; title=&quot;“Merge branch”提交的产生&quot;&gt;&lt;/a&gt;“Merge branch”提交的产生&lt;/h2&gt;&lt;p&gt;我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。&lt;/p&gt;
&lt;p&gt;要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。&lt;/p&gt;
&lt;h2 id=&quot;使用git-pull-–rebase&quot;&gt;&lt;a href=&quot;#使用git-pull-–rebase&quot; class=&quot;headerlink&quot; title=&quot;使用git pull –rebase&quot;&gt;&lt;/a&gt;使用git pull –rebase&lt;/h2&gt;&lt;p&gt;修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark快速入门</title>
    <link href="http://shzhangji.com/2014/12/16/spark-quick-start/"/>
    <id>http://shzhangji.com/2014/12/16/spark-quick-start/</id>
    <published>2014-12-16T07:59:00.000Z</published>
    <updated>2017-06-06T03:12:23.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt=""></p>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>
<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>
<p>Spark是用<a href="http://www.scala-lang.org/" target="_blank" rel="external">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>
<h2 id="安装Spark和SBT"><a href="#安装Spark和SBT" class="headerlink" title="安装Spark和SBT"></a>安装Spark和SBT</h2><ul>
<li>从<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_HOME=/path/to/spark</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</div></pre></td></tr></table></figure>
<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/" target="_blank" rel="external">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar" target="_blank" rel="external">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SBT_OPTS=<span class="string">"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"</span></div><div class="line">java <span class="variable">$SBT_OPTS</span> -jar `dirname <span class="variable">$0</span>`/sbt-launch.jar <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="日志分析示例"><a href="#日志分析示例" class="headerlink" title="日志分析示例"></a>日志分析示例</h2><p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2014-12-11 18:33:52	INFO	Java	some message</div><div class="line">2014-12-11 18:34:33	INFO	MySQL	some message</div><div class="line">2014-12-11 18:34:54	WARN	Java	some message</div><div class="line">2014-12-11 18:35:25	WARN	Nginx	some message</div><div class="line">2014-12-11 18:36:09	INFO	Java	some message</div></pre></td></tr></table></figure>
<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>
<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ spark-shell</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0</div><div class="line">      /_/</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<h3 id="加载并预览数据"><a href="#加载并预览数据" class="headerlink" title="加载并预览数据"></a>加载并预览数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> lines = sc.textFile(<span class="string">"/tmp/logs.txt"</span>)</div><div class="line">lines: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /tmp/logs.txt <span class="type">MappedRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">12</span></div><div class="line"></div><div class="line">scala&gt; lines.first()</div><div class="line">res0: <span class="type">String</span> = <span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>	<span class="type">INFO</span>	<span class="type">Java</span>	some message</div></pre></td></tr></table></figure>
<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>
<h3 id="解析日志"><a href="#解析日志" class="headerlink" title="解析日志"></a>解析日志</h3><p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> logs = lines.map(line =&gt; line.split(<span class="string">"\t"</span>))</div><div class="line">logs: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">MappedRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">14</span></div><div class="line"></div><div class="line">scala&gt; logs.first()</div><div class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>, <span class="type">INFO</span>, <span class="type">Java</span>, some message)</div></pre></td></tr></table></figure>
<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line =&gt; line.split(“\t”)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>
<h3 id="过滤并计数"><a href="#过滤并计数" class="headerlink" title="过滤并计数"></a>过滤并计数</h3><p>我们想要统计错误日志的数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> errors = logs.filter(log =&gt; log(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">errors: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">FilteredRDD</span>[<span class="number">3</span>] at filter at &lt;console&gt;:<span class="number">16</span></div><div class="line"></div><div class="line">scala&gt; errors.first()</div><div class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message)</div><div class="line"></div><div class="line">scala&gt; errors.count()</div><div class="line">res3: <span class="type">Long</span> = <span class="number">158</span></div></pre></td></tr></table></figure>
<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == “ERROR”表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; errors.cache()</div></pre></td></tr></table></figure>
<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>
<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>
<h3 id="显示前10条记录"><a href="#显示前10条记录" class="headerlink" title="显示前10条记录"></a>显示前10条记录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> firstTenErrors = errors.take(<span class="number">10</span>)</div><div class="line">firstTenErrors: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message), <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>, <span class="type">ERROR</span>, <span class="type">Nginx</span>, some message), ...)</div><div class="line"></div><div class="line">scala&gt; firstTenErrors.map(log =&gt; log.mkString(<span class="string">"\t"</span>)).foreach(line =&gt; println(line))</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>	<span class="type">ERROR</span>	<span class="type">Java</span>	some message</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>	<span class="type">ERROR</span>	<span class="type">Nginx</span>	some message</div><div class="line">...</div></pre></td></tr></table></figure>
<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>
<h3 id="按应用进行统计"><a href="#按应用进行统计" class="headerlink" title="按应用进行统计"></a>按应用进行统计</h3><p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> apps = errors.map(log =&gt; (log(<span class="number">2</span>), <span class="number">1</span>))</div><div class="line">apps: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MappedRDD</span>[<span class="number">15</span>] at map at &lt;console&gt;:<span class="number">18</span></div><div class="line"></div><div class="line">scala&gt; apps.first()</div><div class="line">res20: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">Java</span>,<span class="number">1</span>)</div><div class="line"></div><div class="line">scala&gt; <span class="keyword">val</span> counts = apps.reduceByKey((a, b) =&gt; a + b)</div><div class="line">counts: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">17</span>] at reduceByKey at &lt;console&gt;:<span class="number">20</span></div><div class="line"></div><div class="line">scala&gt; counts.foreach(t =&gt; println(t))</div><div class="line">(<span class="type">Java</span>,<span class="number">58</span>)</div><div class="line">(<span class="type">Nginx</span>,<span class="number">53</span>)</div><div class="line">(<span class="type">MySQL</span>,<span class="number">47</span>)</div></pre></td></tr></table></figure>
<p>errors.map(log =&gt; (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>(&quot;Java&quot;, 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>
<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>).reduce((a, b) =&gt; a + b)</div><div class="line">res23: <span class="type">Int</span> = <span class="number">10</span></div></pre></td></tr></table></figure>
<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>
<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>
<h2 id="打包应用程序"><a href="#打包应用程序" class="headerlink" title="打包应用程序"></a>打包应用程序</h2><p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">spark-sandbox</div><div class="line">├── build.sbt</div><div class="line">├── project</div><div class="line">│   ├── build.properties</div><div class="line">│   └── plugins.sbt</div><div class="line">└── src</div><div class="line">    └── main</div><div class="line">        └── scala</div><div class="line">            └── LogMining.scala</div></pre></td></tr></table></figure>
<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox" target="_blank" rel="external">这个项目</a>作为模板。下面说明一些关键部分：</p>
<h3 id="配置依赖"><a href="#配置依赖" class="headerlink" title="配置依赖"></a>配置依赖</h3><p><code>build.sbt</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"1.1.1"</span></div></pre></td></tr></table></figure>
<h3 id="程序内容"><a href="#程序内容" class="headerlink" title="程序内容"></a>程序内容</h3><p><code>src/main/scala/LogMining.scala</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogMining</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogMining"</span>)</div><div class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">  <span class="keyword">val</span> inputFile = args(<span class="number">0</span>)</div><div class="line">  <span class="keyword">val</span> lines = sc.textFile(inputFile)</div><div class="line">  <span class="comment">// 解析日志</span></div><div class="line">  <span class="keyword">val</span> logs = lines.map(_.split(<span class="string">"\t"</span>))</div><div class="line">  <span class="keyword">val</span> errors = logs.filter(_(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">  <span class="comment">// 缓存错误日志</span></div><div class="line">  errors.cache()</div><div class="line">  <span class="comment">// 统计错误日志记录数</span></div><div class="line">  println(errors.count())</div><div class="line">  <span class="comment">// 获取前10条MySQL的错误日志</span></div><div class="line">  <span class="keyword">val</span> mysqlErrors = errors.filter(_(<span class="number">2</span>) == <span class="string">"MySQL"</span>)</div><div class="line">  mysqlErrors.take(<span class="number">10</span>).map(_ mkString <span class="string">"\t"</span>).foreach(println)</div><div class="line">  <span class="comment">// 统计每个应用的错误日志数</span></div><div class="line">  <span class="keyword">val</span> errorApps = errors.map(_(<span class="number">2</span>) -&gt; <span class="number">1</span>)</div><div class="line">  errorApps.countByKey().foreach(println)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="打包运行"><a href="#打包运行" class="headerlink" title="打包运行"></a>打包运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> spark-sandbox</div><div class="line">$ sbt package</div><div class="line">$ spark-submit --class LogMining --master <span class="built_in">local</span> target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final" target="_blank" rel="external">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals" target="_blank" rel="external">Spark Runtime Internals</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://spark.apache.org/images/spark-logo.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通用计算引擎&lt;/strong&gt; 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于内存&lt;/strong&gt; 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与Hadoop集成&lt;/strong&gt; 能够直接读写HDFS中的数据，并能运行在YARN之上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark是用&lt;a href=&quot;http://www.scala-lang.org/&quot;&gt;Scala语言&lt;/a&gt;编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。&lt;/p&gt;
&lt;h2 id=&quot;安装Spark和SBT&quot;&gt;&lt;a href=&quot;#安装Spark和SBT&quot; class=&quot;headerlink&quot; title=&quot;安装Spark和SBT&quot;&gt;&lt;/a&gt;安装Spark和SBT&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;从&lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;官网&lt;/a&gt;上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。&lt;/li&gt;
&lt;li&gt;为了方便起见，可以将spark/bin添加到$PATH环境变量中：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; SPARK_HOME=/path/to/spark&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;variable&quot;&gt;$SPARK_HOME&lt;/span&gt;/bin&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;在练习例子时，我们还会用到&lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;SBT&lt;/a&gt;这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：&lt;ul&gt;
&lt;li&gt;下载&lt;a href=&quot;https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar&quot;&gt;sbt-launch.jar&lt;/a&gt;到$HOME/bin目录；&lt;/li&gt;
&lt;li&gt;新建$HOME/bin/sbt文件，权限设置为755，内容如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;SBT_OPTS=&lt;span class=&quot;string&quot;&gt;&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;java &lt;span class=&quot;variable&quot;&gt;$SBT_OPTS&lt;/span&gt; -jar `dirname &lt;span class=&quot;variable&quot;&gt;$0&lt;/span&gt;`/sbt-launch.jar &lt;span class=&quot;string&quot;&gt;&quot;&lt;span class=&quot;variable&quot;&gt;$@&lt;/span&gt;&quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
      <category term="scala" scheme="http://shzhangji.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>离线环境下构建sbt项目</title>
    <link href="http://shzhangji.com/2014/11/07/sbt-offline/"/>
    <id>http://shzhangji.com/2014/11/07/sbt-offline/</id>
    <published>2014-11-07T07:02:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/" target="_blank" rel="external">sbt</a>、<a href="http://maven.apache.org/" target="_blank" rel="external">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/" target="_blank" rel="external">Nexus</a>镜像服务，原因有以下几个：</p>
<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>
<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/" target="_blank" rel="external">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/" target="_blank" rel="external">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>
<a id="more"></a>
<h2 id="配置Nexus镜像"><a href="#配置Nexus镜像" class="headerlink" title="配置Nexus镜像"></a>配置Nexus镜像</h2><p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html" target="_blank" rel="external">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>
<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt=""></p>
<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>
<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/" target="_blank" rel="external">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/" target="_blank" rel="external">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/" target="_blank" rel="external">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>
<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>
<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/" target="_blank" rel="external">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/" target="_blank" rel="external">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>
<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>
<h2 id="配置sbt"><a href="#配置sbt" class="headerlink" title="配置sbt"></a>配置sbt</h2><p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[repositories]</div><div class="line">  local</div><div class="line">  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</div><div class="line">  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/</div></pre></td></tr></table></figure>
<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sbt -Dsbt.override.build.repos=<span class="literal">true</span></div></pre></td></tr></table></figure>
<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>
<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在公司网络中使用&lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;sbt&lt;/a&gt;、&lt;a href=&quot;http://maven.apache.org/&quot;&gt;Maven&lt;/a&gt;等项目构建工具时，我们通常会搭建一个公用的&lt;a href=&quot;http://www.sonatype.org/nexus/&quot;&gt;Nexus&lt;/a&gt;镜像服务，原因有以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免重复下载依赖，节省公司带宽；&lt;/li&gt;
&lt;li&gt;国内网络环境不理想，下载速度慢；&lt;/li&gt;
&lt;li&gt;IDC服务器没有外网访问权限；&lt;/li&gt;
&lt;li&gt;用于发布内部模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;sbt的依赖管理基于&lt;a href=&quot;http://ant.apache.org/ivy/&quot;&gt;Ivy&lt;/a&gt;，虽然它能直接使用&lt;a href=&quot;http://search.maven.org/&quot;&gt;Maven中央仓库&lt;/a&gt;中的Jar包，在配置时还是有一些注意事项的。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL异常UTF-8字符的处理</title>
    <link href="http://shzhangji.com/2014/10/14/mysql-incorrent-utf8-value/"/>
    <id>http://shzhangji.com/2014/10/14/mysql-incorrent-utf8-value/</id>
    <published>2014-10-14T05:16:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Incorrect string value: &apos;\xF0\x9D\x8C\x86&apos; for column ...</div></pre></td></tr></table></figure>
<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>
<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> REGEXP_REPLACE(<span class="keyword">content</span>, <span class="string">'[^\\u0000-\\uFFFF]'</span>, <span class="string">''</span>) <span class="keyword">FROM</span> ...</div></pre></td></tr></table></figure>
<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>
<p>以下是一些详细说明和参考资料。</p>
<a id="more"></a>
<h2 id="Unicode字符集和UTF编码"><a href="#Unicode字符集和UTF编码" class="headerlink" title="Unicode字符集和UTF编码"></a>Unicode字符集和UTF编码</h2><p><a href="http://en.wikipedia.org/wiki/Unicode" target="_blank" rel="external">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_\(Unicode\" target="_blank" rel="external">基本多语平面（BMP）</a>#Basic_Multilingual_Plane)和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>
<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set" target="_blank" rel="external">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>
<p>比如，磁盘上记录了以下二进制数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">1101000 1100101 1101100 1101100 1101111</div></pre></td></tr></table></figure>
<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">104 101 108 108 111</div></pre></td></tr></table></figure>
<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>
<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>
<h2 id="UTF-8编码"><a href="#UTF-8编码" class="headerlink" title="UTF-8编码"></a>UTF-8编码</h2><p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="external">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>
<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>
<h2 id="MySQL的UTF-8编码"><a href="#MySQL的UTF-8编码" class="headerlink" title="MySQL的UTF-8编码"></a>MySQL的UTF-8编码</h2><p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html" target="_blank" rel="external">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>
<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8" target="_blank" rel="external">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html" target="_blank" rel="external">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode" target="_blank" rel="external">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Incorrect string value: &amp;apos;\xF0\x9D\x8C\x86&amp;apos; for column ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。&lt;/p&gt;
&lt;p&gt;根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; REGEXP_REPLACE(&lt;span class=&quot;keyword&quot;&gt;content&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;[^\\u0000-\\uFFFF]&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。&lt;/p&gt;
&lt;p&gt;以下是一些详细说明和参考资料。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>在CDH 4.5上安装Shark 0.9</title>
    <link href="http://shzhangji.com/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <id>http://shzhangji.com/2014/07/05/deploy-shark-0.9-with-cdh-4.5/</id>
    <published>2014-07-05T09:16:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org" target="_blank" rel="external">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org" target="_blank" rel="external">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org" target="_blank" rel="external">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/" target="_blank" rel="external">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/" target="_blank" rel="external">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/" target="_blank" rel="external">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">STANDALONE_SPARK_MASTER_HOST=one-843</div><div class="line">DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh</div><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh</div></pre></td></tr></table></figure>
<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(<span class="string">"hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo"</span>).count</div></pre></td></tr></table></figure>
<h2 id="安装Shark"><a href="#安装Shark" class="headerlink" title="安装Shark"></a>安装Shark</h2><ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz" target="_blank" rel="external">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45</div><div class="line"><span class="built_in">export</span> SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</div><div class="line"></div><div class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/etc/hive/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> MASTER=spark://one-843:7077</div><div class="line"></div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[program:sharkserver2]</div><div class="line">command = /opt/shark/bin/shark --service sharkserver2</div><div class="line">autostart = true</div><div class="line">autorestart = true</div><div class="line">stdout_logfile = /var/log/sharkserver2.log</div><div class="line">redirect_stderr = true</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ supervisorctl start sharkserver2</div></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root</div></pre></td></tr></table></figure>
<h2 id="版本问题"><a href="#版本问题" class="headerlink" title="版本问题"></a>版本问题</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h4><p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html" target="_blank" rel="external">CDH4.5</a>的部分对应关系如下：</p>
<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>
<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html" target="_blank" rel="external">Metastore Server</a>使用的也是0.10.0版本的API。</p>
<h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h4><p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>
<h4 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h4><p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>
<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive" target="_blank" rel="external">AMPLab patched Hive</a>：</p>
<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>
<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>
<h4 id="Shark与Hive的并存"><a href="#Shark与Hive的并存" class="headerlink" title="Shark与Hive的并存"></a>Shark与Hive的并存</h4><p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>
<h3 id="目前发现的不兼容SQL"><a href="#目前发现的不兼容SQL" class="headerlink" title="目前发现的不兼容SQL"></a>目前发现的不兼容SQL</h3><ul>
<li>DROP TABLE …</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: &apos;drop_table_with_environment_context&apos;</div></pre></td></tr></table></figure>
<ul>
<li>INSERT OVERWRITE TABLE … PARTITION (…) SELECT …</li>
<li>LOAD DATA INPATH ‘…’ OVERWRITE INTO TABLE … PARTITION (…)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Failed with exception org.apache.thrift.TApplicationException: Invalid method name: &apos;partition_name_has_valid_characters&apos;</div></pre></td></tr></table></figure>
<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="对存在问题的SQL使用Hive命令去调"><a href="#对存在问题的SQL使用Hive命令去调" class="headerlink" title="对存在问题的SQL使用Hive命令去调"></a>对存在问题的SQL使用Hive命令去调</h4><p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>
<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>
<h4 id="升级到CDH5"><a href="#升级到CDH5" class="headerlink" title="升级到CDH5"></a>升级到CDH5</h4><p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>
<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc" target="_blank" rel="external">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>
<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot;&gt;Spark&lt;/a&gt;是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的&lt;a href=&quot;http://hadoop.apache.org&quot;&gt;Hadoop&lt;/a&gt;生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于&lt;a href=&quot;http://hive.apache.org&quot;&gt;Hive&lt;/a&gt;，Spark也有相应的替代项目——&lt;a href=&quot;http://shark.cs.berkeley.edu/&quot;&gt;Shark&lt;/a&gt;，能做到 &lt;strong&gt;drop-in replacement&lt;/strong&gt; ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。&lt;/p&gt;
&lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动&lt;/li&gt;
&lt;li&gt;软件版本&lt;ul&gt;
&lt;li&gt;Cloudera Manager 4.8.2&lt;/li&gt;
&lt;li&gt;CDH 4.5&lt;/li&gt;
&lt;li&gt;Spark 0.9.0 Parcel&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cloudera.rst.im/shark/&quot;&gt;Shark 0.9.1 Binary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;服务器基础配置&lt;ul&gt;
&lt;li&gt;可用的软件源（如&lt;a href=&quot;http://mirrors.ustc.edu.cn/&quot;&gt;中科大的源&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;配置主节点至子节点的root账户SSH无密码登录。&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;/etc/hosts&lt;/code&gt;中写死IP和主机名，或者DNS做好正反解析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive小文件问题的处理</title>
    <link href="http://shzhangji.com/2014/04/07/hive-small-files/"/>
    <id>http://shzhangji.com/2014/04/07/hive-small-files/</id>
    <published>2014-04-07T09:09:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。</p>
<h2 id="小文件带来的问题"><a href="#小文件带来的问题" class="headerlink" title="小文件带来的问题"></a>小文件带来的问题</h2><p>关于这个问题的阐述可以读一读Cloudera的<a href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/" target="_blank" rel="external">这篇文章</a>。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>
<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。</p>
<a id="more"></a>
<h2 id="Hive小文件产生的原因"><a href="#Hive小文件产生的原因" class="headerlink" title="Hive小文件产生的原因"></a>Hive小文件产生的原因</h2><p>前面已经提到，汇总后的数据量通常比源数据要少得多。而为了提升运算速度，我们会增加Reducer的数量，Hive本身也会做类似优化——Reducer数量等于源数据的量除以hive.exec.reducers.bytes.per.reducer所配置的量（默认1G）。Reducer数量的增加也即意味着结果文件的增加，从而产生小文件的问题。</p>
<h2 id="配置Hive结果合并"><a href="#配置Hive结果合并" class="headerlink" title="配置Hive结果合并"></a>配置Hive结果合并</h2><p>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</p>
<ul>
<li><code>hive.merge.mapfiles</code> 在map-only job后合并文件，默认<code>true</code></li>
<li><code>hive.merge.mapredfiles</code> 在map-reduce job后合并文件，默认<code>false</code></li>
<li><code>hive.merge.size.per.task</code> 合并后每个文件的大小，默认<code>256000000</code></li>
<li><code>hive.merge.smallfiles.avgsize</code> 平均文件大小，是决定是否执行合并操作的阈值，默认<code>16000000</code></li>
</ul>
<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>
<ol>
<li>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</li>
<li>结果文件的平均大小需要大于avgsize参数的值。</li>
</ol>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">-- map-red job，5个reducer，产生5个60K的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，一个mapper，产生一个300K的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- map-only job，45个mapper，产生45个25M左右的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，4个mapper，产生4个250M左右的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h3 id="压缩文件的处理"><a href="#压缩文件的处理" class="headerlink" title="压缩文件的处理"></a>压缩文件的处理</h3><p>如果结果表使用了压缩格式，则必须配合SequenceFile来存储，否则无法进行合并，以下是示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span> mapred.output.compression.type=<span class="keyword">BLOCK</span>;</div><div class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</div><div class="line"><span class="keyword">set</span> mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec;</div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"></div><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> dw_stage.zj_small;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small</div><div class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE</div><div class="line"><span class="keyword">as</span> <span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h2 id="使用HAR归档文件"><a href="#使用HAR归档文件" class="headerlink" title="使用HAR归档文件"></a>使用HAR归档文件</h2><p>Hadoop的<a href="http://hadoop.apache.org/docs/stable1/hadoop_archives.html" target="_blank" rel="external">归档文件</a>格式也是解决小文件问题的方式之一。而且Hive提供了<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving" target="_blank" rel="external">原生支持</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">set hive.archive.enabled=true;</div><div class="line">set hive.archive.har.parentdir.settable=true;</div><div class="line">set har.partfile.size=1099511627776;</div><div class="line"></div><div class="line">ALTER TABLE srcpart ARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div><div class="line"></div><div class="line">ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div></pre></td></tr></table></figure>
<p>如果使用的不是分区表，则可创建成外部表，并使用<code>har://</code>协议来指定路径。</p>
<h2 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h2><p>Hadoop V2引入了HDFS Federation的概念：</p>
<p><img src="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/images/federation.gif" alt=""></p>
<p>实则是将NameNode做了拆分，从而增强了它的扩展性，小文件的问题也能够得到缓解。</p>
<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><p>对于通常的应用，使用Hive结果合并就能达到很好的效果。如果不想因此增加运行时间，可以自行编写一些脚本，在系统空闲时对分区内的文件进行合并，也能达到目的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。&lt;/p&gt;
&lt;h2 id=&quot;小文件带来的问题&quot;&gt;&lt;a href=&quot;#小文件带来的问题&quot; class=&quot;headerlink&quot; title=&quot;小文件带来的问题&quot;&gt;&lt;/a&gt;小文件带来的问题&lt;/h2&gt;&lt;p&gt;关于这个问题的阐述可以读一读Cloudera的&lt;a href=&quot;http://blog.cloudera.com/blog/2009/02/the-small-files-problem/&quot;&gt;这篇文章&lt;/a&gt;。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。&lt;/p&gt;
&lt;p&gt;此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Java反射机制</title>
    <link href="http://shzhangji.com/2014/01/25/java-reflection-tutorial/"/>
    <id>http://shzhangji.com/2014/01/25/java-reflection-tutorial/</id>
    <published>2014-01-25T01:42:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.programcreek.com/2013/09/java-reflection-tutorial/" target="_blank" rel="external">http://www.programcreek.com/2013/09/java-reflection-tutorial/</a></p>
<p>什么是反射？它有何用处？</p>
<h2 id="1-什么是反射？"><a href="#1-什么是反射？" class="headerlink" title="1. 什么是反射？"></a>1. 什么是反射？</h2><p>“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：</p>
<ol>
<li>内省用于在运行时检测某个对象的类型和其包含的属性；</li>
<li>反射用于在运行时检测和修改某个对象的结构及其行为。</li>
</ol>
<p>从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。</p>
<p><img src="http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png" alt="反射和内省"></p>
<a id="more"></a>
<p>内省示例：<code>instanceof</code>运算符用于检测某个对象是否属于特定的类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Dog) &#123;</div><div class="line">    Dog d = (Dog) obj;</div><div class="line">    d.bark();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>反射示例：<code>Class.forName()</code>方法可以通过类或接口的名称（一个字符串或完全限定名）来获取对应的<code>Class</code>对象。<code>forName</code>方法会触发类的初始化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 使用反射</span></div><div class="line">Class&lt;?&gt; c = Class.forName(<span class="string">"classpath.and.classname"</span>);</div><div class="line">Object dog = c.newInstance();</div><div class="line">Method m = c.getDeclaredMethod(<span class="string">"bark"</span>, <span class="keyword">new</span> Class&lt;?&gt;[<span class="number">0</span>]);</div><div class="line">m.invoke(dog);</div></pre></td></tr></table></figure>
<p>在Java中，反射更接近于内省，因为你无法改变一个对象的结构。虽然一些API可以用来修改方法和属性的可见性，但并不能修改结构。</p>
<h2 id="2-我们为何需要反射？"><a href="#2-我们为何需要反射？" class="headerlink" title="2. 我们为何需要反射？"></a>2. 我们为何需要反射？</h2><p>反射能够让我们：</p>
<ul>
<li>在运行时检测对象的类型；</li>
<li>动态构造某个类的对象；</li>
<li>检测类的属性和方法；</li>
<li>任意调用对象的方法；</li>
<li>修改构造函数、方法、属性的可见性；</li>
<li>以及其他</li>
</ul>
<p>反射是框架中常用的方法。</p>
<p>例如，<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/" target="_blank" rel="external">JUnit</a>通过反射来遍历包含 <em>@Test</em> 注解的方法，并在运行单元测试时调用它们。（<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/" target="_blank" rel="external">这个连接</a>中包含了一些JUnit的使用案例）</p>
<p>对于Web框架，开发人员在配置文件中定义他们对各种接口和类的实现。通过反射机制，框架能够快速地动态初始化所需要的类。</p>
<p>例如，Spring框架使用如下的配置文件：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"someID"</span> <span class="attr">class</span>=<span class="string">"com.programcreek.Foo"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"someField"</span> <span class="attr">value</span>=<span class="string">"someValue"</span> /&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></div></pre></td></tr></table></figure>
<p>当Spring容器处理&lt;bean&gt;元素时，会使用<code>Class.forName(&quot;com.programcreek.Foo&quot;)</code>来初始化这个类，并再次使用反射获取&lt;property&gt;元素对应的<code>setter</code>方法，为对象的属性赋值。</p>
<p>Servlet也会使用相同的机制：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">servlet</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">servlet-name</span>&gt;</span>someServlet<span class="tag">&lt;/<span class="name">servlet-name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">servlet-class</span>&gt;</span>com.programcreek.WhyReflectionServlet<span class="tag">&lt;/<span class="name">servlet-class</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">servlet</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="3-如何使用反射？"><a href="#3-如何使用反射？" class="headerlink" title="3. 如何使用反射？"></a>3. 如何使用反射？</h2><p>让我们通过几个典型的案例来学习如何使用反射。</p>
<p>示例1：获取对象的类型名称。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"><span class="keyword">import</span> java.lang.reflect.Method;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        Foo f = <span class="keyword">new</span> Foo();</div><div class="line">        System.out.println(f.getClass().getName());			</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">myreflection.Foo</div></pre></td></tr></table></figure>
<p>示例2：调用未知对象的方法。</p>
<p>在下列代码中，设想对象的类型是未知的。通过反射，我们可以判断它是否包含<code>print</code>方法，并调用它。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"><span class="keyword">import</span> java.lang.reflect.Method;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        Foo f = <span class="keyword">new</span> Foo();</div><div class="line"> </div><div class="line">        Method method;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            method = f.getClass().getMethod(<span class="string">"print"</span>, <span class="keyword">new</span> Class&lt;?&gt;[<span class="number">0</span>]);</div><div class="line">            method.invoke(f);</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;           </div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">abc</div></pre></td></tr></table></figure>
<p>示例3：创建对象</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        <span class="comment">// 创建Class实例</span></div><div class="line">        Class&lt;?&gt; c = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span>&#123;</div><div class="line">            c=Class.forName(<span class="string">"myreflection.Foo"</span>);</div><div class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">// 创建Foo实例</span></div><div class="line">        Foo f = <span class="keyword">null</span>;</div><div class="line"> </div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            f = (Foo) c.newInstance();</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;   </div><div class="line"> </div><div class="line">        f.print();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>示例4：获取构造函数，并创建对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">import</span> java.lang.reflect.Constructor;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        <span class="comment">// 创建Class实例</span></div><div class="line">        Class&lt;?&gt; c = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span>&#123;</div><div class="line">            c=Class.forName(<span class="string">"myreflection.Foo"</span>);</div><div class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">// 创建Foo实例</span></div><div class="line">        Foo f1 = <span class="keyword">null</span>;</div><div class="line">        Foo f2 = <span class="keyword">null</span>;</div><div class="line"> </div><div class="line">        <span class="comment">// 获取所有的构造函数</span></div><div class="line">        Constructor&lt;?&gt; cons[] = c.getConstructors();</div><div class="line"> </div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            f1 = (Foo) cons[<span class="number">0</span>].newInstance();</div><div class="line">            f2 = (Foo) cons[<span class="number">1</span>].newInstance(<span class="string">"abc"</span>);</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;   </div><div class="line"> </div><div class="line">        f1.print();</div><div class="line">        f2.print();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    String s; </div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">()</span></span>&#123;&#125;</div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(String s)</span></span>&#123;</div><div class="line">        <span class="keyword">this</span>.s=s;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(s);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">null</div><div class="line">abc</div></pre></td></tr></table></figure>
<p>此外，你可以通过<code>Class</code>实例来获取该类实现的接口、父类、声明的属性等。</p>
<p>示例5：通过反射来修改数组的大小。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">import</span> java.lang.reflect.Array;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span>[] intArray = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</div><div class="line">        <span class="keyword">int</span>[] newIntArray = (<span class="keyword">int</span>[]) changeArraySize(intArray, <span class="number">10</span>);</div><div class="line">        print(newIntArray);</div><div class="line"> </div><div class="line">        String[] atr = &#123; <span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span> &#125;;</div><div class="line">        String[] str1 = (String[]) changeArraySize(atr, <span class="number">10</span>);</div><div class="line">        print(str1);</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="comment">// 修改数组的大小</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Object <span class="title">changeArraySize</span><span class="params">(Object obj, <span class="keyword">int</span> len)</span> </span>&#123;</div><div class="line">        Class&lt;?&gt; arr = obj.getClass().getComponentType();</div><div class="line">        Object newArray = Array.newInstance(arr, len);</div><div class="line"> </div><div class="line">        <span class="comment">// 复制数组</span></div><div class="line">        <span class="keyword">int</span> co = Array.getLength(obj);</div><div class="line">        System.arraycopy(obj, <span class="number">0</span>, newArray, <span class="number">0</span>, co);</div><div class="line">        <span class="keyword">return</span> newArray;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="comment">// 打印</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">(Object obj)</span> </span>&#123;</div><div class="line">        Class&lt;?&gt; c = obj.getClass();</div><div class="line">        <span class="keyword">if</span> (!c.isArray()) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        System.out.println(<span class="string">"\nArray length: "</span> + Array.getLength(obj));</div><div class="line"> </div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Array.getLength(obj); i++) &#123;</div><div class="line">            System.out.print(Array.get(obj, i) + <span class="string">" "</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Array length: 10</div><div class="line">1 2 3 4 5 0 0 0 0 0 </div><div class="line">Array length: 10</div><div class="line">a b c d e null null null null null</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上述示例代码仅仅展现了Java反射机制很小一部分的功能。如果你觉得意犹未尽，可以前去阅读<a href="http://docs.oracle.com/javase/tutorial/reflect/" target="_blank" rel="external">官方文档</a>。</p>
<p>参考资料：</p>
<ol>
<li><a href="http://en.wikipedia.org/wiki/Reflection_(computer_programming" target="_blank" rel="external">http://en.wikipedia.org/wiki/Reflection_(computer_programming</a>)</li>
<li><a href="http://docs.oracle.com/javase/tutorial/reflect/" target="_blank" rel="external">http://docs.oracle.com/javase/tutorial/reflect/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.programcreek.com/2013/09/java-reflection-tutorial/&quot;&gt;http://www.programcreek.com/2013/09/java-reflection-tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;什么是反射？它有何用处？&lt;/p&gt;
&lt;h2 id=&quot;1-什么是反射？&quot;&gt;&lt;a href=&quot;#1-什么是反射？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是反射？&quot;&gt;&lt;/a&gt;1. 什么是反射？&lt;/h2&gt;&lt;p&gt;“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内省用于在运行时检测某个对象的类型和其包含的属性；&lt;/li&gt;
&lt;li&gt;反射用于在运行时检测和修改某个对象的结构及其行为。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png&quot; alt=&quot;反射和内省&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
      <category term="Tutorial" scheme="http://shzhangji.com/categories/Translation/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>抽象泄漏定律</title>
    <link href="http://shzhangji.com/2013/12/17/the-law-of-leaky-abstractions/"/>
    <id>http://shzhangji.com/2013/12/17/the-law-of-leaky-abstractions/</id>
    <published>2013-12-17T05:05:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html" target="_blank" rel="external">http://www.joelonsoftware.com/articles/LeakyAbstractions.html</a></p>
<p>TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。</p>
<p>TCP是一种 <em>可靠的</em> 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。</p>
<p>我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！</p>
<p>和TCP协议相比，IP协议也是一种传输协议，但它是 <em>不可靠的</em> 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。</p>
<p>这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 <em>使用一个不可靠的工具来可靠地传输数据</em> 。</p>
<a id="more"></a>
<p>为了更好地说明这有多么神奇，让我们设想下面的场景。虽然有些荒诞，但本质上是相同的。</p>
<p>假设我们用一辆辆汽车将百老汇的演员们运送到好莱坞，这是一条横跨美国的漫长线路。其中一些汽车出了交通事故，车上的演员在事故中死亡。有些演员则在车上酗酒嗑药，兴奋之余将自己的头发剃了，或是纹上了丑陋的纹身，这样一来就失去了他们原先的样貌，无法在好莱坞演出。更普遍的情况是，演员们没有按照出发的顺序到达目的地，因为他们走的都是不同的线路。现在再让我们设想有一个名为“好莱坞快线”的运输服务，在运送这些演员时能够保证三点：他们都能够到达；到达顺序和出发顺序一致；并且都完好无损。神奇的是，好莱坞快线除了用汽车来运输这些演员之外，没有任何其他的方法。所以它能做的就是检查每一个到达目的地的演员，看他们是否和原先的相貌一致。如果有所差别，它就立刻通知百老汇的办公室，派出该演员的双胞胎兄妹，重新发送过来。如果演员到达的顺序不同，好莱坞快线会负责重新排序。如果有一架UFO在飞往51区的途中不慎坠毁在内华达州，造成高速公路阻塞，这时所有打算从这条路经过的演员会绕道亚利桑那州。好莱坞快线不会告诉加利福尼亚州的导演路上发生了什么，只是这些演员到的比较迟而已。</p>
<p>这大致上就是TCP协议的神奇之处，计算机科学家们通常会将其称作为“抽象”：将复杂的问题用简单的方式表现出来。事实上，很多计算机编程工作都是在进行抽象。字符串库做了什么？它能让我们觉得计算机可以像处理数字那样处理文字。文件系统是什么？它让硬盘不再是一组高速旋转的磁性盘块，而是一个有着目录层级结构、能够按字节存储字符信息的设备。</p>
<p>我们继续说TCP。刚才我打了一个比方，有些人可能觉得那很疯狂。但是，当我说TCP协议可以保证消息一定能够到达，事实上并非如此。如果你的宠物蛇把网线给咬坏了，那即便是TCP协议也无法传输数据；如果你和网络管理员闹了矛盾，他将你的网口接到了一台负载很高的交换机上，那即便你的数据包可以传输，速度也会奇慢无比。</p>
<p>这就是我所说的“抽象泄漏”。TCP协议试图提供一个完整的抽象，将底层不可靠的数据传输包装起来，但是，底层的传输有时也会发生问题，即便是TCP协议也无法解决，这时你会发现，它也不是万能的。TCP协议就是“抽象泄漏定律”的示例之一，其实，几乎所有的抽象都是泄漏的。这种泄漏有时很小，有时会很严重。下面再举一些例子：</p>
<ul>
<li><p>对于一个简单的操作，如循环遍历一个二维数组，当遍历的方式不同（横向或纵向），也会对性能造成很大影响，这主要取决于数组中数据的分布——按某个方向遍历时可能会产生更多的页缺失（page fault），而页缺失往往是非常消耗性能的。即使是汇编程序员，他们在编写代码时也会假设程序的内存空间是连续的，这是系统底层的虚拟内存机制提供的抽象，而这一机制在遇到页缺失时就会消耗更多时间。</p>
</li>
<li><p>SQL语言意图将过程式的数据库访问操作封装起来，你只需要告诉操作系统你想要的数据，系统会自动生成各个步骤并加以执行。但在有些情况下，某些SQL查询会比其逻辑等同的查询语句要慢得多。一个著名的示例是，对大多数SQL服务器，指定“WHERE a = b AND b = c AND a = c”要比单纯指定“WHERE a = b AND b = c”快的多，即便它们的结果集是一致的。在使用SQL时，我们不需要思考过程，只需关注定义。但有时，这种抽象会造成性能上的大幅下降，你需要去了解SQL语法分析器的工作原理，找出问题的原因，并想出应对措施，让自己的查询运行得更快。</p>
</li>
<li><p>即便有NFS、SMB这样的协议可以让你像在处理本地文件一样处理远程文件，如果网络传输很慢，或是完全中断了，程序员就需要手动处理这种情况。所以，这种“远程文件即本地文件”的抽象机制是存在<a href="http://www.joelonsoftware.com/articles/fog0000000041.html" target="_blank" rel="external">泄漏</a>的。这里举一个现实的例子：如果你将用户的home目录加载到NFS上（一次抽象），你的用户创建了.forward文件，用来转发他所有的电子邮件（二次抽象），当NFS服务器宕机，.forward文件会找不到，这样就无法转发邮件了，造成丢失。</p>
</li>
<li><p>C++的字符串处理类库相当于增加了一种基础数据类型：字符串，将<a href="http://www.joelonsoftware.com/articles/fog0000000319.html" target="_blank" rel="external">各种操作细节</a>封装起来，让程序员可以方便地使用它。几乎所有的C++字符串类都会重载+操作符，这样你就能用 <em>s + “bar”</em> 来拼接字符串了。但是，无论哪种类库都无法实现 <em>“foo” + “bar”</em> 这种语句，因为在C++中，字符串字面量（string literal）都是char *类型的。这就是一种泄漏。（有趣的是，C++语言的发展历程很大一部分是在争论字符串是否应该在语言层面支持。我个人并不太能理解这为何需要争论。）</p>
</li>
<li><p>当你在雨天开车，虽然你坐在车里，前窗有雨刷，车内有空调，这些措施将“天气”给抽象走了。但是，你还是要小心雨天的轮胎打滑，有时这雨下得太大，可见度很糟，所以你还是得慢行。也就是说，“天气”因素并没有被完全抽象走，它也是存在泄漏的。</p>
</li>
</ul>
<p>抽象泄漏引发的麻烦之一是，它并没有完全简化我们的工作。当我指导别人学习C++时，我当然希望可以跳过char *和指针运算，直接讲解STL字符串类库的使用。但是，当某一天他写出了 <em>“foo” + “bar”</em> 这样的代码，并询问我为什么编译错误时，我还是需要告诉它char *的存在。或者说，当他需要调用一个Windows API，需要指定OUT LPTSTR参数，这时他就必须学习char *、指针、Unicode、wchar_t、TCHAR头文件等一系列知识，这些都是抽象泄漏。</p>
<p>在指导COM编程时，我希望可以直接让大家如何使用Visual Studio的代码生成向导。但将来如果出现问题，学员面对这些生成的代码会不知所从，这时还是要回过头来学习IUnknown、CLSID、ProgIDS等等。天呐！</p>
<p>在指导ASP.NET编程时，我希望可以直接告诉大家双击页面上的控件，在弹出的代码框中输入点击响应事件。的确，ASP.NET将处理点击的HTML代码抽象掉了，但问题在于，ASP.NET的设计者需要动用JS来模拟表单的提交，因为HTML中的&lt;a/&gt;标签是没有这一功能的。这样一来，如果终端用户将JS禁止了，这个程序将无法运行。初学者会不知所措，直至他了解ASP.NET的运作方式，了解它究竟将什么样的工作封装起来了，才能进一步排查。</p>
<p>由于抽象定律的存在，每当有人说自己发现了一款新的代码生成工具，能够大大提高我们的编程效率时，你会听很多人说“先学习手工编写，再去用工具生成”。代码生成工具是一种抽象，同样也会泄漏，唯一的解决方法是学习它的实现原理，即它抽象了什么。所以说抽象只是用于提高我们的工作效率的，而不会节省我们的学习时间。</p>
<p>这就形成了一个悖论：当我们拥有越来越高级的开发工具，越来越好的“抽象”，要成为一个高水平的程序员反而越来越困难了。</p>
<p>我在微软实习的第一年，是为Macintosh编写字符串处理类库。很普通的一个任务：编写 <em>strcat</em> 函数，返回一个指针，指向新字符串的尾部。几行C语言代码就能实现了，这些都是从K&amp;R这本C语言编程书上学习到的。</p>
<p>如今，我在CityDesk供职，需要使用Visual Basic、COM、ATL、C++、InnoSetup、Internet Explorer原理、正则表达式、DOM、HTML、CSS、XML等等，这些相对于古老的K&amp;R来说都是非常高级的工具，但是我仍然需要用到K&amp;R的相关知识，否则会困难重重。</p>
<p>十年前，我们会想象未来能够出现各种新式的编程范型，简化我们的工作。的确，这些年我们创造的各类抽象使得开发复杂的大型软件变得比十五年前要简单得多，就像GUI和网络编程。现代的面向对象编程语言让我们的工作变得高效快速。但突然有一天，这种抽象泄漏出一个问题，解决它需要耗费两星期。如果你需要招录一个VB程序员，那不是一个好主意，因为当他碰到VB语言泄漏的问题时，他会变得寸步难行。</p>
<p>抽象泄漏定律正在阻碍我们前进。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。&lt;/p&gt;
&lt;p&gt;TCP是一种 &lt;em&gt;可靠的&lt;/em&gt; 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。&lt;/p&gt;
&lt;p&gt;我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！&lt;/p&gt;
&lt;p&gt;和TCP协议相比，IP协议也是一种传输协议，但它是 &lt;em&gt;不可靠的&lt;/em&gt; 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。&lt;/p&gt;
&lt;p&gt;这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 &lt;em&gt;使用一个不可靠的工具来可靠地传输数据&lt;/em&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive并发情况下报DELETEME表不存在的异常</title>
    <link href="http://shzhangji.com/2013/09/06/hive-deleteme-error/"/>
    <id>http://shzhangji.com/2013/09/06/hive-deleteme-error/</id>
    <published>2013-09-06T03:20:00.000Z</published>
    <updated>2017-03-09T07:25:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在每天运行的Hive脚本中，偶尔会抛出以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">2013-09-03 01:39:00,973 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:getMetaData(1128)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table dw_xxx_xxx</div><div class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:896)</div><div class="line">        ...</div><div class="line">Caused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore</div><div class="line">NestedThrowables:</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &apos;hive.DELETEME1378143540925&apos; doesn&apos;t exist</div><div class="line">        at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:313)</div><div class="line">        ...</div></pre></td></tr></table></figure>
<p>查阅了网上的资料，是DataNucleus的问题。</p>
<p>背景1：我们知道MySQL中的库表信息是存放在information_schema库中的，Hive也有类似的机制，它会将库表信息存放在一个第三方的RDBMS中，目前我们线上配置的是本机MySQL，即：</p>
<p>$ mysql -uhive -ppassword hive</p>
<p><img src="/cnblogs/images/hive-deleteme-error/1.png" alt="1.png"></p>
<a id="more"></a>
<p>背景2：Hive使用的是DataNuclues ORM库来操作数据库的，而基本上所有的ORM框架（对象关系映射）都会提供自动建表的功能，即开发者只需编写Java对象，ORM会自动生成DDL。DataNuclues也有这一功能，而且它在初始化时会通过生成临时表的方式来获取数据库的Catalog和Schema，也就是 DELETEME表：</p>
<p><img src="/cnblogs/images/hive-deleteme-error/2.png" alt="2.png"></p>
<p>这样就有一个问题：在并发量大的情况下，DELETEME表名中的毫秒数可能相同，那在pt.drop(conn)的时候就会产生找不到表的报错。</p>
<p>解决办法已经可以在代码中看到了：将datanucleus.fixedDataStore选项置为true，即告知DataNuclues该数据库的表结构是既定的，不允许执行DDL操作。</p>
<p>这样配置会有什么问题？让我们回忆一下Hive的安装步骤：</p>
<ol>
<li>解压hive-xyz.tar.gz；</li>
<li>在conf/hive-site.xml中配置Hadoop以及用于存放库表信息的第三方数据库；</li>
<li>执行bin/hive -e “…”即可使用。DataNucleus会按需创建上述的DBS等表。</li>
</ol>
<p>这对新手来说很有用，因为不需要手动去执行建表语句，但对生产环境来说，普通帐号是没有DDL权限的，我们公司建表也都是提DB-RT给DBA操作。同理，线上Hive数据库也应该采用手工创建的方式，导入scripts/metastore/upgrade/mysql/hive-schema-0.9.0.mysql.sql文件即可。这样一来，就可以放心地配置datanucleus.fixedDataStore以及 datanecleus.autoCreateSchema两个选项了。</p>
<p>这里我们也明确了一个问题：设置datanucleus.fixedDataStore=true不会影响Hive建库建表，因为Hive中的库表只是DBS、TBLS表中的一条记录而已。</p>
<p>建议的操作：</p>
<ol>
<li>在线上导入hive-schema-0.9.0.mysql.sql，将尚未创建的表创建好（比如我们没有用过Hive的权限管理，所以DataNucleus没有自动创建DB_PRIVS表）；</li>
<li>在hive-site.xml中配置 datanucleus.fixedDataStore=true；datanecleus.autoCreateSchema=false。</li>
</ol>
<p>这样就可以彻底解决这个异常了。</p>
<p>为什么HWI没有遇到类似问题？因为它是常驻内存的，DELETEME表只会在启动的时候创建，后续的查询不会创建。而我们这里每次调用hive命令行都会去创建，所以才有这样的问题。</p>
<p>参考链接：</p>
<ul>
<li><a href="http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html" target="_blank" rel="external">http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html</a></li>
<li><a href="https://github.com/dianping/cosmos-hive/issues/10" target="_blank" rel="external">https://github.com/dianping/cosmos-hive/issues/10</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-1841" target="_blank" rel="external">https://issues.apache.org/jira/browse/HIVE-1841</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在每天运行的Hive脚本中，偶尔会抛出以下错误：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;2013-09-03 01:39:00,973 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:getMetaData(1128)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table dw_xxx_xxx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:896)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Caused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NestedThrowables:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &amp;apos;hive.DELETEME1378143540925&amp;apos; doesn&amp;apos;t exist&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:313)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;查阅了网上的资料，是DataNucleus的问题。&lt;/p&gt;
&lt;p&gt;背景1：我们知道MySQL中的库表信息是存放在information_schema库中的，Hive也有类似的机制，它会将库表信息存放在一个第三方的RDBMS中，目前我们线上配置的是本机MySQL，即：&lt;/p&gt;
&lt;p&gt;$ mysql -uhive -ppassword hive&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/hive-deleteme-error/1.png&quot; alt=&quot;1.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
      <category term="hive" scheme="http://shzhangji.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Ansible FAQ</title>
    <link href="http://shzhangji.com/2013/06/11/ansible-faq/"/>
    <id>http://shzhangji.com/2013/06/11/ansible-faq/</id>
    <published>2013-06-11T13:18:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是从原Ansible官网的FAQ页面翻译而来，网站改版后该页面已无法访问，但可以从<a href="https://github.com/ansible/ansible.github.com/blob/4a2bf7f60a020f0d0a7b042056fc3dd8716588f2/faq.html" target="_blank" rel="external">Github历史提交</a>中获得。翻译这篇原始FAQ文档是因为它陈述了Ansible这款工具诞生的原因，设计思路和特性，以及与Puppet、Fabric等同类软件的比较，可以让我们对Ansible有一个整体的了解，所以值得使用者一读。</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>为什么命名为“Ansible”？</li>
<li>Ansible受到了谁的启发？</li>
<li>与同类软件比较<ul>
<li>Func？</li>
<li>Puppet？</li>
<li>Chef？</li>
<li>Capistrano/Fabric？</li>
</ul>
</li>
<li>其它问题<ul>
<li>Ansible的安全性如何？</li>
<li>Ansible如何扩展？</li>
<li>是否支持SSH以外的协议？</li>
<li>Ansible的适用场景有哪些？</li>
</ul>
</li>
</ul>
<h2 id="为什么命名为“Ansible”？"><a href="#为什么命名为“Ansible”？" class="headerlink" title="为什么命名为“Ansible”？"></a>为什么命名为“Ansible”？</h2><p>我最喜爱的书籍之一是奥森·斯科特·卡特的《安德的游戏》。在这本书中，“Ansible”是一种能够跨越时空的即时通讯工具。强烈推荐这本书！</p>
<a id="more"></a>
<h2 id="Ansible受到了谁的启发？"><a href="#Ansible受到了谁的启发？" class="headerlink" title="Ansible受到了谁的启发？"></a>Ansible受到了谁的启发？</h2><p>我在Red Hat任职期间主要开发Cobbler，很快我和几个同事就发现在部署工具（Cobbler）和配置管理工具（cfengine、Puppet等）之间有一个空缺，即如何更高效地执行临时性的任务。虽然当时有一些并行调用SSH脚本的方案，但并没有形成统一的API。所以我们（Adrian Likins、Seth Vidal、我）就开发了一个SSH分布式脚本框架——Func。</p>
<p>我一直想在Func的基础上开发一个配置管理工具，但因为忙于Cobbler和其他项目的开发，一直没有动手。在此期间，John Eckersberg开发了名为Taboot的自动化部署工具，它基于Func，采用YAML描述，和目前Ansible中的Playbooks很像。</p>
<p>近期我在一家新公司尝试引入Func，但遇到一些SSL和DNS方面的问题，所以想要开发一个更为简单的工具，吸收Func中优秀的理念，并与我在Puppet Labs的工作经验相结合。我希望这一工具能够易于学习，且不需要进行任何安装步骤。使用它不需要引入一整套新的理论，像Puppet和Chef那样，从而降低被某些运维团队排挤的可能。</p>
<p>我也曾参与过一些大型网站的应用部署，发觉现有的配置管理工具都太过复杂了，超过了这些公司的需求。程序发布的过程很繁复，需要一个简单的工具来帮助开发和运维人员。我不想教授他们Puppet或Chef，而且他们也不愿学习这些工具。</p>
<p>于是我便思考，应用程序的部署就应该那么复杂吗？答案是否定的。</p>
<p>我是否能开发一款工具，让运维人员能够在15分钟内学会使用，并用自己熟悉的语言来扩展它？这就是Ansible的由来。运维人员对自己的服务器设施最清楚，Ansible深知这一点，并将同类工具中最核心的功能提取出来，供我们使用。</p>
<p>Ansible不仅易于学习和扩展，它更是集配置管理、应用部署、临时任务等功能于一身。它非常强大，甚至前所未有。</p>
<p>我很想知道你对Ansible的看法，到邮件列表里发表一下意见吧。</p>
<h2 id="与同类软件比较"><a href="#与同类软件比较" class="headerlink" title="与同类软件比较"></a>与同类软件比较</h2><h3 id="Func？"><a href="#Func？" class="headerlink" title="Func？"></a>Func？</h3><p>Ansible默认使用SSH，而非SSL和守护进程，无需在远程服务器上安装任何软件。你可以使用任何语言编写插件，只要它能够返回JSON格式即可。Ansible的API深受Func的影响，但它和Func相较提供了配置管理和多节点统一化部署（Playbooks）等功能。</p>
<h3 id="Puppet？"><a href="#Puppet？" class="headerlink" title="Puppet？"></a>Puppet？</h3><p>首先我要强调的是，如果没有Puppet，就不会有Ansible。Puppet从cfengine中吸收了配置管理的概念，并更合理地加以实现。但是，我依旧认为它可以再简单一些。</p>
<p>Ansible的playbook是一套完整的配置管理系统。和Puppet不同，playbook在编写时就隐含了执行顺序（和Chef类似），但同时也提供了事件机制（和Puppet类似），可以说是结合了两者的优点。</p>
<p>Ansible没有中心节点的概念，从而避免了惊群效应。它一开始就是为多节点部署设计的，这点Puppet很难做到，因为它是一种“拉取”的架构。Ansible以“推送”为基础，从而能够定义执行顺序，同时只操作一部分服务器，无需关注它们的依赖关系。又因为Ansible可以用任何语言进行扩展，因此并不是只有专业的程序员才能为其开发插件。</p>
<p>Ansible中资源的概念深受Puppet的启发，甚至“state”这一关键字直接来自Puppet的“ensure”一词。和Puppet不同的是，Ansbile可以用任何语言进行扩展，甚至是Bash，只需返回JSON格式的输出即可。你不需要懂得Ruby。</p>
<p>和Puppet不同，Ansible若在配置某台服务器时发生错误，它会立即终止这台服务器的配置过程。它提倡的是“提前崩溃”，修正错误，而非最大化应用。这一点在我们需要配置包含依赖关系的服务器架构时尤为重要。</p>
<p>Ansible的学习曲线非常平滑，你不需要掌握编程技能，更不需要学习新的语言。Ansible内置的功能应该能够满足超过80%的用户需求，而且它不会遇到扩展性方面的瓶颈（因为没有中心节点）。</p>
<p>如果系统中安装了factor，Ansible同样支持从中获取系统信息。Ansible使用jinja2作为模板语言，类似于Puppet使用erb文件作为模板。Ansible可以使用自己的信息收集工具，因此factor并不是必需的。</p>
<h3 id="Chef？"><a href="#Chef？" class="headerlink" title="Chef？"></a>Chef？</h3><p>Ansible与Chef的区别和Puppet类似。Chef的配置非常困难，而且需要你掌握Ruby语言。也因为如此，Chef在Rails使用者中很流行。</p>
<p>Ansible是按照编写顺序来执行任务的，而不是显示地定义依赖关系，这点和Chef相似。但Ansible更进一步，它支持事件触发，比如修改了Apache的配置文件，Apache就会被重启。</p>
<p>和Chef不同的是，Ansible的playbook不是一门编程语言，而是一种可以存储的数据结构。这就意味着你的运维工作不是一项开发型的任务，测试起来也相对简单。</p>
<p>无论你有怎样的语言背景，都可以使用Ansible。Chef和Puppet有超过六万行的代码，而Ansible则是一段小巧简单的程序。我相信这一点会使得Ansible更加健壮和可靠，并汇聚一批活跃的社区贡献者——因为任何人都可以提交补丁或是模块。</p>
<p>Ansible同样支持从ohai中获取系统信息，当然这同样不是必需的。</p>
<h3 id="Capistrano-Fabric？"><a href="#Capistrano-Fabric？" class="headerlink" title="Capistrano/Fabric？"></a>Capistrano/Fabric？</h3><p>这些工具并不适合用作服务器配置工具，它们主要用于应用程序的部署。</p>
<p>而Ansible则提供了完整的配置管理，以及在扩展性方面提供了一些高级特性。</p>
<p>Ansible playbook的语法简介只占一个HTML页面，有着非常平缓的学习曲线。由于Ansible使用了“推送”的设计，因此对系统管理员（不仅仅是开发者）同样适用，并能用它处理各种临时性的任务。</p>
<h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><h3 id="Ansible的安全性如何？"><a href="#Ansible的安全性如何？" class="headerlink" title="Ansible的安全性如何？"></a>Ansible的安全性如何？</h3><p>Ansible没有守护进程，主要使用OpenSSH进行通信，这是一款已被反复检验并广泛使用的软件。其它工具都会在远程服务器上以root用户运行守护进程，因此相较于这些工具，Ansible会更为安全，且无需担心网络方面的问题。</p>
<p>如果你的中心节点遭到入侵（或是被恶意员工登录），只要你是使用SSH-agent、或是经过加密的密码，那你的密钥仍然是被锁定的，别人无法操控你的节点。而对于Chef、Puppet等工具来说，一旦配置文件遭到篡改，那危及的将是整个网络。</p>
<p>此外，由于Ansible没有守护进程，可以节省下一部分内存和计算资源，这对需要最大化性能的用户来说也是一个优点。</p>
<h3 id="Ansible如何扩展？"><a href="#Ansible如何扩展？" class="headerlink" title="Ansible如何扩展？"></a>Ansible如何扩展？</h3><p>无论是在单次执行模式还是playbook模式下，Ansible都可以并行执行任务，这要感谢Python提供的多进程处理模块。</p>
<p>你可以自行决定要一次性配置5台还是50台服务器，这取决于服务器的计算能力，以及你想要多快完成任务。</p>
<p>由于没有守护进程，所以平时不会占用任何资源，而且你不用担心一次性有太多节点一起从控制节点上获取信息。</p>
<p>对于SSH，Ansible默认使用paramiko库，当然也能使用原始的openssh。Ansible可以利用SSH的ControlMaster特性来重用网络连接。</p>
<p>当要维护上万个节点时，单个Ansible playbook可能不太合理，这时你就能使用Ansible的“拉取”模式。这种模式下需要配合git和cron，可以扩展到任意多台服务器。“拉取”模式可以使用本地连接，或是SSH。关于这个模式的详细说明可以在帮助文档的“Advanced Playbooks”一节查阅。即使在“拉取”模式下，你同样能够享受到Ansible的种种便利。</p>
<p>如果你想进一步探讨扩展性，可以加入到邮件列表中。</p>
<h3 id="是否支持SSH以外的协议？"><a href="#是否支持SSH以外的协议？" class="headerlink" title="是否支持SSH以外的协议？"></a>是否支持SSH以外的协议？</h3><p>目前Ansible支持SSH和本地连接，但它的接口实际上是非常易于扩展的，因此你可以编写补丁来使Ansible运行于消息系统或XMPP协议之上。</p>
<p>如果你有任何建议，可以加入到邮件列表中一起探讨。Ansible中对于连接的管理都已单独抽象出来，有很强的可扩性。</p>
<h3 id="Ansible的适用场景有哪些？"><a href="#Ansible的适用场景有哪些？" class="headerlink" title="Ansible的适用场景有哪些？"></a>Ansible的适用场景有哪些？</h3><p>最适场景？使用playbook进行多节点云主机部署；从一个初始的操作系统开始部署应用，或是配置一个现有的系统。</p>
<p>Ansible同样适用于执行临时性的任务，能够用于各类 Unix-like 系统，因为它使用的就是系统本身自带的工具，无需安装额外软件。</p>
<p>你还可以用Ansible来编写各类脚本，用于收集信息、执行各种任务，对QA、运维等团队均适用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是从原Ansible官网的FAQ页面翻译而来，网站改版后该页面已无法访问，但可以从&lt;a href=&quot;https://github.com/ansible/ansible.github.com/blob/4a2bf7f60a020f0d0a7b042056fc3dd8716588f2/faq.html&quot;&gt;Github历史提交&lt;/a&gt;中获得。翻译这篇原始FAQ文档是因为它陈述了Ansible这款工具诞生的原因，设计思路和特性，以及与Puppet、Fabric等同类软件的比较，可以让我们对Ansible有一个整体的了解，所以值得使用者一读。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;为什么命名为“Ansible”？&lt;/li&gt;
&lt;li&gt;Ansible受到了谁的启发？&lt;/li&gt;
&lt;li&gt;与同类软件比较&lt;ul&gt;
&lt;li&gt;Func？&lt;/li&gt;
&lt;li&gt;Puppet？&lt;/li&gt;
&lt;li&gt;Chef？&lt;/li&gt;
&lt;li&gt;Capistrano/Fabric？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其它问题&lt;ul&gt;
&lt;li&gt;Ansible的安全性如何？&lt;/li&gt;
&lt;li&gt;Ansible如何扩展？&lt;/li&gt;
&lt;li&gt;是否支持SSH以外的协议？&lt;/li&gt;
&lt;li&gt;Ansible的适用场景有哪些？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;为什么命名为“Ansible”？&quot;&gt;&lt;a href=&quot;#为什么命名为“Ansible”？&quot; class=&quot;headerlink&quot; title=&quot;为什么命名为“Ansible”？&quot;&gt;&lt;/a&gt;为什么命名为“Ansible”？&lt;/h2&gt;&lt;p&gt;我最喜爱的书籍之一是奥森·斯科特·卡特的《安德的游戏》。在这本书中，“Ansible”是一种能够跨越时空的即时通讯工具。强烈推荐这本书！&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
      <category term="ops" scheme="http://shzhangji.com/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop YARN - 项目背景与简介</title>
    <link href="http://shzhangji.com/2013/05/25/apache-hadoop-yarn-background-and-an-overview/"/>
    <id>http://shzhangji.com/2013/05/25/apache-hadoop-yarn-background-and-an-overview/</id>
    <published>2013-05-25T02:57:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/" target="_blank" rel="external">http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/</a></p>
<p>日前，Apache Hadoop YARN已被提升为Apache软件基金会的子项目，这是一个值得庆祝的里程碑。这里我们也第一时间为各位献上Apache Hadoop YARN项目的系列介绍文章。YARN是一个普适的、分布式的应用管理框架，运行于Hadoop集群之上，用以替代传统的Apache Hadoop MapReduce框架。</p>
<h2 id="MapReduce-模式"><a href="#MapReduce-模式" class="headerlink" title="MapReduce 模式"></a>MapReduce 模式</h2><p>本质上来说，MapReduce模型包含两个部分：一是Map过程，将数据拆分成若干份，分别处理，彼此之间没有依赖关系；二是Reduce过程，将中间结果汇总计算成最终结果。这是一种简单而又条件苛刻的模型，但也促使它成为高效和极易扩展的并行计算方式。</p>
<p>Apache Hadoop MapReduce是当下最流行的开源MapReduce模型。</p>
<p>特别地，当MapReduce配合分布式文件系统，类似Apache Hadoop HDFS，就能在大集群上提供高吞吐量的计算，这一经济效应是Hadoop得以流行的重要原因。</p>
<p>这一模式成功的原因之一是，它使用的是“移动计算能力至数据节点”而非通过网络“移动数据至计算节点”的方式。具体来说，一个MapReduce任务会被调度到输入数据所在的HDFS节点执行，这会极大地减少I/O支出，因为大部分I/O会发生在本地磁盘或是同一机架中——这是核心优势。</p>
<a id="more"></a>
<h3 id="回顾2011年的Apache-Hadoop-MapReduce"><a href="#回顾2011年的Apache-Hadoop-MapReduce" class="headerlink" title="回顾2011年的Apache Hadoop MapReduce"></a>回顾2011年的Apache Hadoop MapReduce</h3><p>Apache Hadoop MapReduce是<a href="http://www.apache.org/" target="_blank" rel="external">Apache基金会</a>下的开源项目，实现了如上所述的MapReduce编程模式。作为一个在该项目中全职开发了六年的工作者，我通常会将它细分为以下几个部分：</p>
<ul>
<li>提供给最终用户使用的 <strong>MapReduce API</strong> ，用来编写MapReduce应用程序。</li>
<li><strong>MapReduce框架</strong> ，用来实现运行时的各个阶段，即map、sort/shuffle/merge、reduce。</li>
<li><strong>MapReduce系统</strong> ，一个完整的后端系统，用来运行用户的MapReduce应用程序，管理集群资源，调度上千个并发脚本。</li>
</ul>
<p>这样的划分可以带来非常明显的优势，即最终用户只需关心MapReduce API，而让框架和后端系统去处理资源管理、容错、调度等细节。</p>
<p>目前，Apache Hadoop MapReduce系统由一个JobTracker和多个TaskTracker组成，也分别称他们为master和slave节点。</p>
<p><img src="http://hortonworks.com/wp-content/uploads/2012/08/MRArch.png" alt="MRArch.png"></p>
<p>JobTracker负责的工作包括资源管理（即管理工作节点TaskTracker），跟踪资源消耗和可用情况，以及每个脚本的生命周期（脚本调度，进度跟踪，容错等）。</p>
<p>TaskTracker的职责比较简单：根据JobTracker的指令来启动和关闭工作进程，并定时向JobTracker汇报处理进度。</p>
<p>其实很早我们就意识到Hadoop的MapReduce框架需要被拆解和调整，特别是JobTracker，我们需要提升它的可扩展性，提高对集群的利用率，让用户能够方便地进行升级（即用户需要的敏捷性），并能支持MapReduce以外的脚本类型。</p>
<p>长久以来，我们都在做修复和更新，如近期加入的JobTracker高可用和HDFS故障恢复（这两个特性都已包含在<a href="http://hortonworks.com/download/" target="_blank" rel="external">Hortonworks Data Platform v1</a>中）。但我们渐渐发现，这些特性会增加维护成本，而且并不能解决一些核心问题，如支持非MapReduce脚本，以及敏捷性。</p>
<h3 id="为什么要支持非MapReduce类型的脚本？"><a href="#为什么要支持非MapReduce类型的脚本？" class="headerlink" title="为什么要支持非MapReduce类型的脚本？"></a>为什么要支持非MapReduce类型的脚本？</h3><p>MapReduce对大部分应用程序来说已经足够，但仍有一些场景并不适用，如图形计算（<a href="http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html" target="_blank" rel="external">Google Pregel</a> / <a href="http://giraph.apache.org/" target="_blank" rel="external">Apache Giraph</a>）、交互式建模（<a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_blank" rel="external">MPI</a>）。当所有的企业数据都已存放在Hadoop HDFS中时，支持多种处理模型就变得额外重要。</p>
<p>此外，MapReduce本质上是以批量处理为核心的，对于日益增长的实时和近实时处理的客户需求，如流式计算以及CEPFresil等，就无能为力了。</p>
<p>如果Hadoop能够支持这一特性，企业会从对Hadoop的投资中得到更多回报，因为他们可以减少数据迁移所需要的管理和维护成本。</p>
<h3 id="为何要提升可扩展性？"><a href="#为何要提升可扩展性？" class="headerlink" title="为何要提升可扩展性？"></a>为何要提升可扩展性？</h3><p>根据摩尔定律，同样的价格所能购买到的计算能力一直在大幅上升。让我们看看以下两组数字：</p>
<ul>
<li>2009年：8核CPU，16GB内存，4x1TB硬盘；</li>
<li>2012年：16核以上的CPU，48至96GB内存，12x2TB或12x3TB的硬盘。</li>
</ul>
<p>同样价格的服务器，其各方面的计算能力要比两到三年以前提升了两倍。Hadoop的MapReduce在2009年便能支持约5000台节点，所以随着机器性能的提升，对其高可扩的要求也与日俱增。</p>
<h3 id="集群资源利用率不高的典型症候是？"><a href="#集群资源利用率不高的典型症候是？" class="headerlink" title="集群资源利用率不高的典型症候是？"></a>集群资源利用率不高的典型症候是？</h3><p>在现有的系统中，集群由节点组成，节点上有map槽位和reduce槽位，两者不能互相替代。这样一来，很有可能map槽位已经耗尽，而reduce还是空闲的，反之亦然。修复这一问题对于提升集群资源利用率来说是必不可少的。</p>
<h3 id="敏捷性为何重要？"><a href="#敏捷性为何重要？" class="headerlink" title="敏捷性为何重要？"></a>敏捷性为何重要？</h3><p>在现实应用中，Hadoop通常会部署在共享的、多租户的系统上。所以，对Hadoop进行升级时会影响很大一部分甚至是所有的应用。基于这一点，用户会对升级持保守态度，因为不想因此引发一系列的问题。所以，一个支持多版本Hadoop的架构就变得非常重要。</p>
<h2 id="Apache-Hadoop-YARN-诞生"><a href="#Apache-Hadoop-YARN-诞生" class="headerlink" title="Apache Hadoop YARN 诞生"></a>Apache Hadoop YARN 诞生</h2><p>YARN的核心思想是将JobTracker的两个职能，即资源管理和脚本调度/监控，分解为两个独立的组件：全局ResourceManager以及按应用拆分的ApplicationMaster（AM）。</p>
<p>主节点的ResourceManager以及其它节电的NodeManager（NM），形成了一个新的更为通用的分布式应用管理模式。</p>
<p>ResourceManager负责应用程序的资源分配。ApplicationMaster会和ResourceManager进行协商，并与节点上的NodeManager协作，运行和监控每个工作进程。</p>
<p>ResourceManager的调度器是可定制的，能够根据计算能力、队列大小进行资源调配。调度器不包含任何对工作进程的监控和跟踪，不会去重启失败的脚本。调度器会根据应用程序申请的资源进行分配，它是建立在一个资源容器抽象层（Resource Container）之上的，其中包括了内存、CPU、硬盘、网络等要素信息。</p>
<p>NodeManager运行在每个节点之上，负责运行应用程序的工作进程，监控它们的资源占用情况，并向ResourceManager汇报。</p>
<p>每个应用都会有一个专属的ApplicationMaster，它会负责和调度器协商资源分配，跟踪工作进程的状态和进度。ApplicationMaster本身也是以一个工作进程来运行的。</p>
<p>以下是YARN的架构图：</p>
<p><img src="http://hortonworks.com/wp-content/uploads/2012/08/YARNArch.png" alt="YARNArch.png"></p>
<p>值得一提的是，我们在为YARN开发MapReduce API时没有做任何较大的改动，所以现有的程序可以很方便地进行迁移。关于这点我们会在以后的文章中详述。</p>
<p>下一节我们会深入了解YARN的架构，阐述它所带来的各种优点，如高可扩、支持多类型脚本（MapReduce、MPI等），以及它是如何提升集群资源利用率的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/&quot;&gt;http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;日前，Apache Hadoop YARN已被提升为Apache软件基金会的子项目，这是一个值得庆祝的里程碑。这里我们也第一时间为各位献上Apache Hadoop YARN项目的系列介绍文章。YARN是一个普适的、分布式的应用管理框架，运行于Hadoop集群之上，用以替代传统的Apache Hadoop MapReduce框架。&lt;/p&gt;
&lt;h2 id=&quot;MapReduce-模式&quot;&gt;&lt;a href=&quot;#MapReduce-模式&quot; class=&quot;headerlink&quot; title=&quot;MapReduce 模式&quot;&gt;&lt;/a&gt;MapReduce 模式&lt;/h2&gt;&lt;p&gt;本质上来说，MapReduce模型包含两个部分：一是Map过程，将数据拆分成若干份，分别处理，彼此之间没有依赖关系；二是Reduce过程，将中间结果汇总计算成最终结果。这是一种简单而又条件苛刻的模型，但也促使它成为高效和极易扩展的并行计算方式。&lt;/p&gt;
&lt;p&gt;Apache Hadoop MapReduce是当下最流行的开源MapReduce模型。&lt;/p&gt;
&lt;p&gt;特别地，当MapReduce配合分布式文件系统，类似Apache Hadoop HDFS，就能在大集群上提供高吞吐量的计算，这一经济效应是Hadoop得以流行的重要原因。&lt;/p&gt;
&lt;p&gt;这一模式成功的原因之一是，它使用的是“移动计算能力至数据节点”而非通过网络“移动数据至计算节点”的方式。具体来说，一个MapReduce任务会被调度到输入数据所在的HDFS节点执行，这会极大地减少I/O支出，因为大部分I/O会发生在本地磁盘或是同一机架中——这是核心优势。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
      <category term="hadoop" scheme="http://shzhangji.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Cascalog：基于Clojure的Hadoop查询语言</title>
    <link href="http://shzhangji.com/2013/05/01/introducing-cascalog-a-clojure-based-query-language-for-hado/"/>
    <id>http://shzhangji.com/2013/05/01/introducing-cascalog-a-clojure-based-query-language-for-hado/</id>
    <published>2013-05-01T10:01:00.000Z</published>
    <updated>2017-03-09T03:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html" target="_blank" rel="external">http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html</a></p>
<p>我非常兴奋地告诉大家，<a href="http://github.com/nathanmarz/cascalog" target="_blank" rel="external">Cascalog</a>开源了！Cascalog受<a href="http://en.wikipedia.org/wiki/Datalog" target="_blank" rel="external">Datalog</a>启发，是一种基于Clojure、运行于Hadoop平台上的查询语言。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul>
<li><strong>简单</strong> - 使用相同的语法编写函数、过滤规则、聚合运算；数据联合（join）变得简单而自然。</li>
<li><strong>表达能力强</strong> - 强大的逻辑组合条件，你可以在查询语句中任意编写Clojure函数。</li>
<li><strong>交互性</strong> - 可以在Clojure REPL中执行查询语句。</li>
<li><strong>可扩展</strong> - Cascalog的查询语句是一组MapReduce脚本。</li>
<li><strong>任意数据源</strong> - HDFS、数据库、本地数据、以及任何能够使用Cascading的<code>Tap</code>读取的数据。</li>
<li><strong>正确处理空值</strong> - 空值往往让事情变得棘手。Cascalog提供了内置的“非空变量”来自动过滤空值。</li>
<li><strong>与Cascading结合</strong> - 使用Cascalog定义的流程可以在Cascading中直接使用，反之亦然。</li>
<li><strong>与Clojure结合</strong> - 能够使用普通的Clojure函数来编写操作流程、过滤规则，又因为Cascalog是一种Clojure DSL，因此也能在其他Clojure代码中使用。</li>
</ul>
<a id="more"></a>
<p>好，下面就让我们开始Cascalog的学习之旅！我会用一系列的示例来介绍Cascalog。这些示例会使用到项目本身提供的“试验场”数据集。我建议你立刻下载Cascalog，一边阅读本文一边在REPL中操作。（安装启动过程只有几分钟，README中有步骤）</p>
<h2 id="基本查询"><a href="#基本查询" class="headerlink" title="基本查询"></a>基本查询</h2><p>首先让我们启动REPL，并加载“试验场”数据集：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">lein repl</div><div class="line">user=&gt; (use 'cascalog.playground) (bootstrap)</div></pre></td></tr></table></figure>
<p>以上语句会加载本文用到的所有模块和数据。你可以阅读项目中的<code>playground.clj</code>文件来查看这些数据。下面让我们执行第一个查询语句，找出年龄为25岁的人：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person] (age ?person 25))</div></pre></td></tr></table></figure>
<p>这条查询语句可以这样阅读：找出所有<code>age</code>等于25的<code>?person</code>。执行过程中你可以看到Hadoop输出的日志信息，几秒钟后就能看到查询结果。</p>
<p>好，让我们尝试稍复杂的例子。我们来做一个范围查询，找出年龄小于30的人：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person] (age ?person ?age) (&lt; ?age 30))</div></pre></td></tr></table></figure>
<p>看起来也不复杂。这条语句中，我们将人的年龄绑定到了<code>?age</code>变量中，并对该变量做出了“小于30”的限定。</p>
<p>我们重新执行这条语句，只是这次会将人的年龄也输出出来：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person ?age] (age ?person ?age)</div><div class="line">            (&lt; ?age 30))</div></pre></td></tr></table></figure>
<p>我们要做的仅仅是将<code>?age</code>添加到向量中去。</p>
<p>让我们执行另一条查询，找出艾米丽关注的所有男性：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person] (follows "emily" ?person)</div><div class="line">            (gender ?person "m"))</div></pre></td></tr></table></figure>
<p>可能你没有注意到，这条语句使用了联合查询。各个数据集中的<code>?person</code>值都必须对应，而<code>follows</code>和<code>gender</code>分属于不同的数据集，Cascalog便会使用联合查询。</p>
<h2 id="查询语句的结构"><a href="#查询语句的结构" class="headerlink" title="查询语句的结构"></a>查询语句的结构</h2><p>让我们分析一下查询语句的结构，以下面这条语句为例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- [stdout] [?person ?a2] (age ?person ?age)</div><div class="line">            (&lt; ?age 30) (* 2 ?age :&gt; ?a2))</div></pre></td></tr></table></figure>
<p><code>?&lt;-</code>操作符出现的频率很高，它能同时定义并执行一条查询。<code>?&lt;-</code>实际上是对<code>&lt;-</code>和<code>?-</code>的包装。我们之后会看到如何使用这些操作符编写更为复杂的查询语句。</p>
<p>首先，我们指定了查询结果的输出目的地，就是这里的<code>(stdout)</code>。<code>(stdout)</code>会创建一个Cascading的<code>tap</code>组件，它会在查询结束后将结果打印到标准输出中。我们可以使用任意一种Cascading的<code>tap</code>组件，也就是说输出结果的格式可以是序列文件（Sequence file）、文本文件等等；也可以输出到任何地方，如本地磁盘、HDFS、数据库等。</p>
<p>在定义了输出目的地后，我们使用Clojure的向量结构来定义输出结果所包含的内容。本例中，我们定义的是<code>?person</code>和<code>?a2</code>。</p>
<p>接下来，我们定义了一系列的约束条件。Cascalog有三种约束条件：</p>
<ol>
<li>生成器（Generator）：表示一个数据源，可以是以下两种类型：<ul>
<li>Cascading Tap：如HDFS上某个路径中的文件；</li>
<li>一个已经使用<code>&lt;-</code>定义的查询。</li>
</ul>
</li>
<li>操作器（Operation）：引入预定义的变量，将其绑定至新的变量，或是设定一个过滤条件。</li>
<li>集合器（Aggregator）：计数、求和、最小值、最大值等等。</li>
</ol>
<p>约束条件由名称、一组输入变量、以及一组输出变量构成。上述查询中的约束条件有：</p>
<ul>
<li>(age ?person ?age)</li>
<li>(&lt; ?age 30)</li>
<li>(* 2 ?age :&gt; ?a2)</li>
</ul>
<p>其中，<code>:&gt;</code>关键字用于将输入变量和输出变量隔开。如果没有这个关键字，那么该变量在操作器中就会被识别为输入变量，在生成器和集合器中会被认为是输出变量。</p>
<p><code>age</code>约束指向<code>playground.clj</code>中定义的一个<code>tap</code>，所以它是一个生成器，会输出<code>?person</code>和<code>?age</code>这两个数据。</p>
<p><code>&lt;</code>约束是一个Clojure函数，因为没有指定输出变量，所以这条约束会构成一个过滤器，将<code>?age</code>小于30的记录筛选出来。如果我们这样写：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="name"><span class="builtin-name">&lt;</span></span> ?age <span class="number">30</span> <span class="symbol">:&gt;</span> ?young)</div></pre></td></tr></table></figure>
<p>那么<code>&lt;</code>约束会将“年龄是否小于30”作为一个布尔值传递给<code>?young</code>变量。</p>
<p>约束之间的顺序不重要，因为Cascalog是声明式语言。</p>
<h2 id="变量替换为常量"><a href="#变量替换为常量" class="headerlink" title="变量替换为常量"></a>变量替换为常量</h2><p>变量是以<code>?</code>或<code>!</code>起始的标识。有时你不在意变量的值，可以直接用<code>_</code>代替。其他的变量则会在解析时替换成常量。我们已经在很多示例中用到这一特性了。下面这个示例中，我们将输出变量作为一种过滤条件：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="name"><span class="builtin-name">*</span></span> <span class="number">4</span> ?v2 <span class="symbol">:&gt;</span> <span class="number">100</span>)</div></pre></td></tr></table></figure>
<p>这里使用了两个常量：4和100。4是一个输入变量，100则是作为一个过滤条件，只有满足<code>?v2</code>乘以4等于100的记录才会被筛选出来。字符串、数字、以及其他基本类型和对象类型，只要在Hadoop有对应的序列化操作，都可以被作为常量使用。</p>
<p>让我们回到示例中。找出所有关注了比自己年龄小的用户的列表：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person1 ?person2]</div><div class="line">            (age ?person1 ?age1) (follows ?person1 ?person2)</div><div class="line">            (age ?person2 ?age2) (&lt; ?age2 ?age1))</div></pre></td></tr></table></figure>
<p>同时，我们将年龄差异也输出出来：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person1 ?person2 ?delta]</div><div class="line">            (age ?person1 ?age1) (follows ?person1 ?person2)</div><div class="line">            (age ?person2 ?age2) (- ?age2 ?age1 :&gt; ?delta)</div><div class="line">            (&lt; ?delta 0))</div></pre></td></tr></table></figure>
<h2 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h2><p>下面让我们看看聚合查询的使用方法。统计所有年龄小于30的用户人数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?count] (age _ ?a) (&lt; ?a 30)</div><div class="line">            (c/count ?count))</div></pre></td></tr></table></figure>
<p>这条查询会统计所有的记录。我们也可以只聚合部分记录。比如，让我们找出每个人所关注的用户的数量：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person ?count] (follows ?person _)</div><div class="line">            (c/count ?count))</div></pre></td></tr></table></figure>
<p>因为我们在输出结果中指定了<code>?person</code>这个变量，所以Cascalog会将数据记录按照用户来分组，然后使用<code>c/count</code>进行聚合运算。</p>
<p>你可以在单个查询中使用多个聚合条件，它们的分组方式是一致的。例如，我们可以计算每个国家的用户的平均年龄，使用计数和求和这两种聚合方式：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?country ?avg]</div><div class="line">            (location ?person ?country _ _) (age ?person ?age)</div><div class="line">            (c/count ?count) (c/sum ?age :&gt; ?sum)</div><div class="line">            (div ?sum ?count :&gt; ?avg))</div></pre></td></tr></table></figure>
<p>可以看到，我们对<code>?sum</code>和<code>?count</code>这两个聚合结果执行了<code>div</code>操作，该操作会在聚合过程结束后进行。</p>
<h2 id="自定义操作"><a href="#自定义操作" class="headerlink" title="自定义操作"></a>自定义操作</h2><p>下面我们来编写一个查询，统计几句话中每个单词的出现次数。首先，我们编写一个自定义操作：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">user=&gt; (defmapcatop split [sentence]</div><div class="line">         (seq (.split sentence "\\s+")))</div><div class="line">user=&gt; (?&lt;- (stdout) [?word ?count] (sentence ?s)</div><div class="line">            (split ?s :&gt; ?word) (c/count ?count))</div></pre></td></tr></table></figure>
<p><code>defmapcatop split</code>定义了一个方法，这个方法接收一个参数<code>sentence</code>，并会输出0个或多个元组（tuple）。<code>deffilterop</code>可以用来定义一个返回布尔型的方法，用来筛选记录；<code>defmapop</code>定义的函数会返回一个元组；<code>defaggregateop</code>定义一个聚合函数。这些函数都能在Cascalog工作流API中使用，我会在另一篇博客中叙述。</p>
<p>在上述查询中，如果单词字母大小写不一致，会被分别统计。我们用以下方法来修复这个问题：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">user=&gt; (defn lowercase [w] (.toLowerCase w))</div><div class="line">user=&gt; (?&lt;- (stdout) [?word ?count]</div><div class="line">            (sentence ?s) (split ?s :&gt; ?word1)</div><div class="line">            (lowercase ?word1 :&gt; ?word) (c/count ?count))</div></pre></td></tr></table></figure>
<p>可以看到，这里直接使用了纯Clojure编写的函数。当这个函数不包含输出变量时，会被作为过滤条件来执行；当包含一个返回值时，则会作为<code>defmapop</code>来解析。而对于返回0个或多个元组的函数，则必须使用<code>defmapcatop</code>来定义。</p>
<p>下面这个查询会按照性别和年龄范围来统计用户数量：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">user=&gt; (defn agebucket [age]</div><div class="line">         (find-first (partial &lt;= age) [17 25 35 45 55 65 100 200]))</div><div class="line">user=&gt; (?&lt;- (stdout) [?bucket ?gender ?count]</div><div class="line">            (age ?person ?age) (gender ?person ?gender)</div><div class="line">            (agebucket ?age :&gt; ?bucket) (c/count ?count))</div></pre></td></tr></table></figure>
<h2 id="非空变量"><a href="#非空变量" class="headerlink" title="非空变量"></a>非空变量</h2><p>Cascalog提供了“非空变量”这样的机制来帮助用户处理空值的情况。其实我们每个示例中都在使用这一特性。以<code>?</code>开头的变量都是非空变量，而以<code>!</code>开头的则是可空变量。Cascalog会在执行过程中将空值排除在外。</p>
<p>为了体验非空变量的效果，让我们对比下面这两条查询语句：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user=&gt; (?&lt;- (stdout) [?person ?city] (location ?person _ _ ?city)</div><div class="line">user=&gt; (?&lt;- (stdout) [?person !city] (location ?person _ _ !city)</div></pre></td></tr></table></figure>
<p>第二组查询结果中会包含空值。</p>
<h2 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h2><p>最后，我们来看看更为复杂的查询，我们会用到子查询这一特性。让我们找出关注了两人以上的用户列表，并找出这些用户之间的关注关系：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">user=&gt; (let [many-follows (&lt;- [?person] (follows ?person _)</div><div class="line">                              (c/count ?c) (&gt; ?c 2))]</div><div class="line">            (?&lt;- (stdout) [?person1 ?person2] (many-follows ?person1)</div><div class="line">                 (many-follows ?person2) (follows ?person1 ?person2)))</div></pre></td></tr></table></figure>
<p>这里，我们使用<code>let</code>来定义了一个子查询<code>many-follows</code>。这个子查询是用<code>&lt;-</code>定义的。之后，我们便可以在后续查询中使用这个子查询了。</p>
<p>我们还可以在一个查询中指定多个输出目的地。比如我们想要同时得到<code>many-follows</code>的查询结果：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">user=&gt; (let [many-follows (&lt;- [?person] (follows ?person _)</div><div class="line">                              (c/count ?c) (&gt; ?c 2))</div><div class="line">             active-follows (&lt;- [?p1 ?p2] (many-follows ?p1)</div><div class="line">                                (many-follows ?p2) (follows ?p1 ?p2))]</div><div class="line">            (?- (stdout) many-follows (stdout) active-follows))</div></pre></td></tr></table></figure>
<p>这里我们分别定义了两个查询，没有立刻执行它们，而是在后续的<code>?-</code>中将两个查询分别绑定到了两个<code>tap</code>上，并同时执行。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Cascalog目前在还不断的改进中，未来会增加更多查询特性，以及对查询过程的优化。</p>
<p>我非常希望能够得到你对Cascalog的反馈，如果你有任何评论、问题、或是顾虑，请留言，或者在<a href="http://twitter.com/nathanmarz" target="_blank" rel="external">Twitter</a>上联系我，给我发送邮件<a href="nathan.marz@gmail.com">nathan.marz@gmail.com</a>，或是在freenode的#cascading频道和我聊天。</p>
<p><a href="http://nathanmarz.com/blog/new-cascalog-features" target="_blank" rel="external">下一篇博客</a>会介绍Cascalog的外联合、排序、组合等特性。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html&quot;&gt;http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我非常兴奋地告诉大家，&lt;a href=&quot;http://github.com/nathanmarz/cascalog&quot;&gt;Cascalog&lt;/a&gt;开源了！Cascalog受&lt;a href=&quot;http://en.wikipedia.org/wiki/Datalog&quot;&gt;Datalog&lt;/a&gt;启发，是一种基于Clojure、运行于Hadoop平台上的查询语言。&lt;/p&gt;
&lt;h2 id=&quot;特点&quot;&gt;&lt;a href=&quot;#特点&quot; class=&quot;headerlink&quot; title=&quot;特点&quot;&gt;&lt;/a&gt;特点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简单&lt;/strong&gt; - 使用相同的语法编写函数、过滤规则、聚合运算；数据联合（join）变得简单而自然。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;表达能力强&lt;/strong&gt; - 强大的逻辑组合条件，你可以在查询语句中任意编写Clojure函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交互性&lt;/strong&gt; - 可以在Clojure REPL中执行查询语句。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可扩展&lt;/strong&gt; - Cascalog的查询语句是一组MapReduce脚本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任意数据源&lt;/strong&gt; - HDFS、数据库、本地数据、以及任何能够使用Cascading的&lt;code&gt;Tap&lt;/code&gt;读取的数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正确处理空值&lt;/strong&gt; - 空值往往让事情变得棘手。Cascalog提供了内置的“非空变量”来自动过滤空值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与Cascading结合&lt;/strong&gt; - 使用Cascalog定义的流程可以在Cascading中直接使用，反之亦然。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与Clojure结合&lt;/strong&gt; - 能够使用普通的Clojure函数来编写操作流程、过滤规则，又因为Cascalog是一种Clojure DSL，因此也能在其他Clojure代码中使用。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Translation/Big-Data/"/>
    
    
      <category term="clojure" scheme="http://shzhangji.com/tags/clojure/"/>
    
  </entry>
  
</feed>
