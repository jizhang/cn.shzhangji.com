<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张吉的博客</title>
  
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/cnblogs/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/cnblogs/"/>
  <updated>2020-10-04T11:11:18.877Z</updated>
  <id>http://shzhangji.com/cnblogs/</id>
  
  <author>
    <name>张吉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python 类型检查实践</title>
    <link href="http://shzhangji.com/cnblogs/2020/10/04/python-static-typing/"/>
    <id>http://shzhangji.com/cnblogs/2020/10/04/python-static-typing/</id>
    <published>2020-10-04T11:11:18.000Z</published>
    <updated>2020-10-04T11:11:18.877Z</updated>
    
    <content type="html"><![CDATA[<p>Python 作为一门动态类型语言，代码灵活度和开发效率都是非常高的。但随着项目代码逐渐变多，函数之间的调用变得更复杂，经常会出现参数或返回值类型不正确等问题。并且这些问题只能在运行时被发现，甚至会产生线上 Bug。那么如何能让 Python 像 Java 或 Go 这样的语言一样，在编译期就进行类型检查呢？从 3.5 版本开始，Python 就能支持静态类型检查了。本文整理自团队的一次内部分享，介绍了 Python 的这一新特性。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.002.jpeg" width="60%"></p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>首先我们来快速看一下，Python 中要如何使用静态类型检查。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.003.jpeg" width="60%"></p><p>通过给函数的参数和返回值添加类型注解，配合使用 mypy 等工具，我们就可以对 Python 代码做静态类型检查了。上图中的 <code>greeting</code> 函数接收一个 <code>name</code> 参数，我们将其定义为 <code>str</code> 类型；函数的返回值也是 <code>str</code> 类型。使用 <code>pip install</code> 安装 mypy 后，就可以进行类型检查了。很显然，这个函数是能通过类型检查的，因为字符串之间可以进行 <code>+</code> 的操作。假设我们将 <code>name</code> 变量与一个整数相加，mypy 就会报告无法对 <code>str</code> 和 <code>int</code> 类型做 <code>+</code> 的操作；同样，如果调用 <code>greeting</code> 时传入的是整型，或者将 <code>greeting</code> 的返回值和整型相加，都会触发类型检查错误。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.004.jpeg" width="60%"></p><p>除了对函数参数做类型注解，我们也可以对本地变量做注解，语法是类似的。上图中的 <code>number</code> 变量我们声明为 <code>int</code> 类型，在将字符串赋值给它的时候就会引发报错；同样，如果将 <code>number</code> 作为函数返回值，也会触发报错。</p><p>除了 <code>int</code> 和 <code>str</code> 这类基本类型，我们也可以对容器类进行注解。更进一步，我们可以去规定容器中可以存放什么类型的变量。图中的 <code>items</code> 变量被定义为了 <code>list</code> 类型，因此无法赋值一个整型。但是，<code>list</code> 类型没有规定容器中元素的类型，因此 <code>[1, 2, 3]</code> 和 <code>[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]</code> 都是可以赋值给 <code>items</code> 变量的。如果我们想定义一个只包含整型元素的 <code>list</code>，就需要使用 <code>typing</code> 模块提供的 <code>List</code> 类型。比如 <code>nums</code> 变量就是一个只包含整型元素的列表，如果我们想要将字符串加入这个列表中，就会引发类型检查错误；字典类型 <code>Dict</code> 也是类似的作用。需要注意的是，这些代码在执行时是不会报错的，因为 Python 仍然是动态类型语言，运行期不会进行类型检查，只有用 mypy 等工具去检查时才会有效。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.005.jpeg" width="60%"></p><p>那是不是所有的变量都需要做类型注解呢？答案当然是否定的，因为 mypy 有类型推断机制。比如上图中的 <code>nums</code> 变量，虽然没有声明类型，但是因为赋值给它的是一个整型列表，所以该变量会被推断成 <code>List[int]</code> 类型，也就无法添加字符串类型的元素了。绕过这一机制的方式当然是显式地去指定类型，比如当指定为 <code>List[object]</code> 类型时，就可以写入字符串、整型等任意元素了。</p><p>最后我们看一下如何给类成员变量注解类型。两种方式：第一种是图中的 <code>suffix</code> 变量，在声明的时候进行注解；第二种是利用类型推断，比如构造函数中，<code>date</code> 参数传入的是 <code>str</code> 类型，并赋值给了 <code>self.date</code>，那么 <code>self.date</code> 就会被推断为 <code>str</code> 类型。这样一来，<code>run</code> 函数中的两条语句都会产生类型检查错误。</p><h2 id="为什么要引入类型检查"><a href="#为什么要引入类型检查" class="headerlink" title="为什么要引入类型检查"></a>为什么要引入类型检查</h2><p>我们可以看到，静态类型检查是会增加额外的工作量的，那么我们为什么要引入静态类型检查，它的优点有哪些呢？</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.007.jpeg" width="60%"></p><p>我之前在分享 <em>Learn Go in 10 Minutes</em> 的时候提到过，Go 语言的优点之一是其静态类型系统。静态类型可以在编译期就发现错误，而不用等到程序运行出错时才去修复；而且研究表明，静态类型系统是可以减少 Bug 的数量的，研究者使用的对象是 TypeScript 和 Flow，这两者都为 JavaScript 语言提供了类型检查机制；最后，静态类型系统对程序性能的提升也是有帮助的。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.008.jpeg" width="60%"></p><p>先来梳理几个概念，我们一直会听到静态类型、动态类型、强类型、弱类型等术语。其中，静态类型和动态类型比较容易区分：前者在编译期进行类型检查，后者在运行期进行检查。像 Java、C、Golang 等是比较常见的静态类型语言，而 JavaScript、PHP、Python 等则是动态类型的。强类型和弱类型则比较难以区分了，判断标准是：允许隐式类型转换的程度。比如 JavaScript 是最弱的一门弱类型语言，因为任何类型的变量都可以进行相加等操作，执行引擎会自动做隐式转换；PHP 也是弱类型语言，不过在转换时会报一些 Warning 信息；Python 则是强类型语言，因为当我们将字符串和整型做相加操作时会直接报错。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.009.jpeg" width="60%"></p><p>这里列举了静态类型语言的优点，我挑选几条重点讲一下。首先是提升代码可读性，可以看到，函数的参数类型被清楚地标注出来了，如果传入了类型不正确的参数，在编译阶段就会报错。以前我们调用函数时要通过阅读文档或代码来确定参数的类型，有了标注后就可以省去这一步。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.010.jpeg" width="60%"></p><p>类型标注也可以增强 IDE 的代码提示功能。比如将 <code>value</code> 标注为字符串类型，就可以 <code>.</code> 出对应的方法了。同样的，对于函数的调用提示，IDE 也可以展示参数类型信息。Python 的标准库都已经增加了类型注解。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.011.jpeg" width="60%"></p><p>最后是性能提升。首先说明一下，Python 的类型注解在运行期是不会被执行的，目前 Python 的解释器也没有使用类型信息对代码进行优化，所以这里我用 Clojure 作为例子。可以看到，对变量 <code>x</code> 进行类型注解后，运行速度得到了很大提升，因为 Clojure 也是动态类型语言，原本需要通过反射来获取变量的类型，有了注解后这一步就可以省去了，从而提升了速度。</p><p>当然，类型注解也不是没有缺点的。比如需要编写和维护额外的代码，而且变量有了类型之后，Python 的灵活性也会打一些折扣。所以一般来说，小项目或者平时写的简单脚本，是不需要用类型注解的；只有当项目达到一定规模，才需要考虑引入这个工具。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.012.jpeg" width="60%"></p><p>其他动态类型语言也在引入静态类型检查。比如 JavaScript 对应的 TypeScript，以及 PHP 对应的 Hack 语言。它们的方式也和 Python 类似，类型注解是可选的，没有注解的变量不做检查，从而让开发人员可以平滑地切换到静态类型语言。</p><h2 id="Python-类型检查功能详解"><a href="#Python-类型检查功能详解" class="headerlink" title="Python 类型检查功能详解"></a>Python 类型检查功能详解</h2><p><img src="/cnblogs/images/python-static-typing/python-static-typing.014.jpeg" width="60%"></p><p>Python 语言引入新特性时都会有相应的 PEP，这里列举了一些和静态类型检查相关且比较重要的 PEP。可以看到，Python 3.0 开始引入了函数参数和返回值的类型注解，但当时并不是专门为静态类型检查设计的。直到 Python 3.5，我们才能正式使用静态类型检查，<code>typing</code> 模块也是从这个版本开始引入的。Python 3.6 则是增加了本地变量的类型注解语法，让类型检查功能更加完整。我们目前使用的也是 3.6 版本。3.7 开始类型注解会被延迟处理，一是解决性能问题，因为类型注解虽然在运行期不进行检查，但解释器还是会去解析；二是解决类型注解过早引用的问题，这点我们在介绍如何对类方法进行注解时会说明。Python 3.8 则引入了更多的高级特性，比如支持 Duck typing、定义 Dict 键值列表、以及 <code>final</code> 关键字等，这些内容我会在最后一部分做个简单介绍。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.015.jpeg" width="60%"></p><p>Python 的类型检查是可选的，mypy 只会检查有类型标注的代码，这种做法我们称之为渐进式类型检查。没有标注类型的变量会被定义为 <code>Any</code> 类型，我们可以将任意类型的数据赋值给它，它也能够被当作任意类型使用。<code>Any</code> 和 <code>object</code> 是有区别的，虽然 <code>object</code> 也可以满足第一条，但它无法作为任意类型使用。比如例子中的 <code>item</code> 变量，因为它是 <code>object</code> 类型，所以调用 <code>bar()</code> 方法时就会报错，<code>Any</code> 则不会。需要额外注意的是，如果你的函数没有参数也没有返回值，一定要标注返回值为 <code>None</code>，否则 mypy 会认为这个函数没有做类型注解，从而跳过检查。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.016.jpeg" width="60%"></p><p>类型标注有三种方式，除了前面一直使用的注解外，还可以通过注释和接口文件来进行。注释的方式主要是为了兼容 Python 2.x，因为低版本的 Python 没有引入注解语法，所以用注释来代替。<code>typing</code> 模块也有 2.x 的版本，需要单独安装。注解和注释的方式都需要修改源代码，而接口文件的方式就可以在无法修改源码的时候对代码进行注解，这点在使用第三方库时很有用，其实 Python 的标准库也用了相同的做法，我们下面会讲。接口文件是以 <code>.pyi</code> 结尾的，内容只有类和函数的定义，方法体则是用 <code>...</code> 做占位。接口文件可以和源码放在同一个目录中，也可以放在独立的目录中，通过配置参数告知 mypy 该去哪里加载。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.017.jpeg" width="60%"></p><p>Python 官方维护了一个名为 typeshed 的项目，里面包含了 Python 标准库的接口文件，以及其他一些常用的三方库的类型注解，比如 Flask, requests 等。安装 mypy 时会自动安装 typeshed 项目。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.018.jpeg" width="60%"></p><p>Python 中常用的类型包括 <code>int</code>、<code>str</code> 等基础类型，<code>List</code>、<code>Dict</code> 等容器类型，以及用来标注函数参数和返回值的 <code>Callback</code> 回调类型。当然，类本身也是一种类型，我们下一节会讲。此外还有一些特殊类型，比如 <code>Union[int, str]</code> 可以用来标注变量可以是整型或字符串类型；<code>Optional</code> 表示该变量可能为 <code>None</code>。这个类型在标注函数返回值时非常有用：用户必须写代码去判断函数的返回值是否为空，否则 mypy 会提示错误。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.019.jpeg" width="60%"></p><p>类本身也是一种类型，mypy 能够识别类的继承关系，类也能在容器中使用，比如上图中的 <code>cats</code> 变量就是一个元素为 <code>Cat</code> 的数组。如果我们觉得类型定义过长，可以给它起一个别名，<code>CatList</code> 和 <code>List[Cat]</code> 是等价的。对于一些静态方法，比如工厂模式需要返回类本身，定义时类本身还没有被定义完全，因此会报类不存在的错误。这就是之前提到的过早引用问题，解决方法是将类型注解表达成字符串。最后一个例子是 Python 3.8 引入的 <code>TypedDict</code>，变量本身是一个普通的 <code>dict</code>，但通过注解就可以对键值列表进行类型检查了。</p><p>以上就是 Python 类型检查最基础的知识了，有了这些模块和工具，我们就可以在项目中立刻用起来了，也不用担心会对已有代码产生破坏。</p><h2 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h2><p>最后我们来看一些类型检查的高级特性，这些特性在官方文档中有详细说明，这里我只是简单介绍一下，而且实际项目中也不太会用到。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.021.jpeg" width="60%"></p><p>当我们自己定义一个容器类的时候，就需要用到泛型，因为我们不关心容器里装的是什么。比如示例中的 <code>LoggedVar</code> 类被定义为可以接收任意类型的变量，在变量被修改时打出一行日志。此外，泛型是可以设置下界的，意思是传入的类型必须是某个类型的子类。比如示例中，<code>longer</code> 函数要求传入的变量必须是 <code>Sized</code> 的子类型，也就是必须包含 <code>__len__()</code> 方法。比如列表和集合都可以获取到长度，因此可以被同时传入 <code>longer</code>。字符串虽然也有长度，但类型推断会将其判断为 <code>object</code>，解决方法是显式声明字符串为 <code>Sequence</code> 类型，这样就能和 <code>List</code> 一起传给 <code>longer</code> 函数了。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.022.jpeg" width="60%"></p><p>Duck typing 是比较常见的一个概念，当你走路像鸭子、叫声像鸭子，你就是一只鸭子，也就是说我们检查的是类的行为。Python 预定义了一些常用的类型，比如前面提到的 <code>Sized</code>，它并不是一个真正的类，而且是表示类含有 <code>__len__()</code> 方法。这些类型定义在 <code>collections.abc</code> 模块中，<code>typing</code> 模块中定义的则是相应的泛型版本。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.023.jpeg" width="60%"></p><p>先看左边的例子，上方的代码是显式指定了 <code>Bucket</code> 类所继承的类型，这是过去常用的做法；而下方则只是定义了这两个类型对应的方法 <code>__len__</code> 和 <code>__iter__</code>，当我们将 <code>Bucket</code> 实例赋值给 <code>Iterator[int]</code> 类型的变量时是可以通过类型检查的，因为 <code>Bucket</code> 实例包含了 <code>Iterator[int]</code> 所需要的 <code>__iter__</code> 方法。这样一来，类的定义就会更加简洁。右边的例子则是用户自定义类型，同样的，<code>Resource</code> 不需要声明自己继承自 <code>Closeable</code> 类型，只需要提供合乎规定的 <code>close</code> 方法即可。</p><p><img src="/cnblogs/images/python-static-typing/python-static-typing.024.jpeg" width="60%"></p><p>Python 的静态类型检查时在编译期发生的，如果想在运行期去检查就得自己写代码实现了，比如使用 <code>isinstance</code> 函数。当然，借助第三方库，我们可以将这个检查自动化，比如 typeguard 可以在函数被调用时检查传入的参数类型；pydantic 则能够检查类成员变量是否符合类型注解。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.python.org/dev/peps/pep-0483/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0483/</a></li><li><a href="http://wphomes.soic.indiana.edu/jsiek/what-is-gradual-typing/" target="_blank" rel="noopener">http://wphomes.soic.indiana.edu/jsiek/what-is-gradual-typing/</a></li><li><a href="https://www.bernat.tech/the-state-of-type-hints-in-python/" target="_blank" rel="noopener">https://www.bernat.tech/the-state-of-type-hints-in-python/</a></li><li><a href="https://blog.zulip.com/2016/10/13/static-types-in-python-oh-mypy/" target="_blank" rel="noopener">https://blog.zulip.com/2016/10/13/static-types-in-python-oh-mypy/</a></li><li><a href="https://mypy.readthedocs.io/en/latest/index.html#overview-type-system-reference" target="_blank" rel="noopener">https://mypy.readthedocs.io/en/latest/index.html#overview-type-system-reference</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python 作为一门动态类型语言，代码灵活度和开发效率都是非常高的。但随着项目代码逐渐变多，函数之间的调用变得更复杂，经常会出现参数或返回值类型不正确等问题。并且这些问题只能在运行时被发现，甚至会产生线上 Bug。那么如何能让 Python 像 Java 或 Go 这样的语言一样，在编译期就进行类型检查呢？从 3.5 版本开始，Python 就能支持静态类型检查了。本文整理自团队的一次内部分享，介绍了 Python 的这一新特性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/python-static-typing/python-static-typing.002.jpeg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>使用 Kubernetes 部署 Flink 应用</title>
    <link href="http://shzhangji.com/cnblogs/2019/08/25/deploy-flink-job-cluster-on-kubernetes/"/>
    <id>http://shzhangji.com/cnblogs/2019/08/25/deploy-flink-job-cluster-on-kubernetes/</id>
    <published>2019-08-25T03:02:22.000Z</published>
    <updated>2020-08-22T12:06:11.270Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a> 是目前非常流行的容器编排系统，在其之上可以运行 Web 服务、大数据处理等各类应用。这些应用被打包在一个个非常轻量的容器中，我们通过声明的方式来告知 Kubernetes 要如何部署和扩容这些程序，并对外提供服务。<a href="https://flink.apache.org/" target="_blank" rel="noopener">Flink</a> 同样是非常流行的分布式处理框架，它也可以运行在 Kubernetes 之上。将两者相结合，我们就可以得到一个健壮和高可扩的数据处理应用，并且能够更安全地和其它服务共享一个 Kubernetes 集群。</p><p><img src="/cnblogs/images/flink-on-kubernetes.png" alt="Flink on Kubernetes"></p><p>在 Kubernetes 上部署 Flink 有两种方式：会话集群（Session Cluster）和脚本集群（Job Cluster）。会话集群和独立部署一个 Flink 集群类似，只是底层资源换成了 K8s 容器，而非直接运行在操作系统上。该集群可以提交多个脚本，因此适合运行那些短时脚本和即席查询。脚本集群则是为单个脚本部署一整套服务，包括 JobManager 和 TaskManager，运行结束后这些资源也随即释放。我们需要为每个脚本构建专门的容器镜像，分配独立的资源，因而这种方式可以更好地和其他脚本隔离开，同时便于扩容或缩容。文本将以脚本集群为例，演示如何在 K8s 上运行 Flink 实时处理程序，主要步骤如下：</p><ul><li>编译并打包 Flink 脚本 Jar 文件；</li><li>构建 Docker 容器镜像，添加 Flink 运行时库和上述 Jar 包；</li><li>使用 Kubernetes Job 部署 Flink JobManager 组件；</li><li>使用 Kubernetes Service 将 JobManager 服务端口开放到集群中；</li><li>使用 Kubernetes Deployment 部署 Flink TaskManager；</li><li>配置 Flink JobManager 高可用，需使用 ZooKeeper 和 HDFS；</li><li>借助 Flink SavePoint 机制来停止和恢复脚本。</li></ul><a id="more"></a><h2 id="Kubernetes-实验环境"><a href="#Kubernetes-实验环境" class="headerlink" title="Kubernetes 实验环境"></a>Kubernetes 实验环境</h2><p>如果手边没有 K8s 实验环境，我们可以用 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 快速搭建一个，以 MacOS 系统为例：</p><ul><li>安装 <a href="https://www.virtualbox.org" target="_blank" rel="noopener">VirtualBox</a>，Minikube 将在虚拟机中启动 K8s 集群；</li><li>下载 <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64" target="_blank" rel="noopener">Minikube 程序</a>，权限修改为可运行，并加入到 PATH 环境变量中；</li><li>执行 <code>minikube start</code>，该命令会下载虚拟机镜像，安装 <code>kubelet</code> 和 <code>kubeadm</code> 程序，并构建一个完整的 K8s 集群。如果你在访问网络时遇到问题，可以配置一个代理，并告知 <a href="https://minikube.sigs.k8s.io/docs/reference/networking/proxy/" target="_blank" rel="noopener">Minikube 使用它</a>；</li><li>下载并安装 <a href="https://storage.googleapis.com/kubernetes-release/release/v1.15.0/bin/darwin/amd64/kubectl" target="_blank" rel="noopener">kubectl 程序</a>，Minikube 已经将该命令指向虚拟机中的 K8s 集群了，所以可以直接运行 <code>kubectl get pods -A</code> 来显示当前正在运行的 K8s Pods：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   kube-apiserver-minikube            1/1     Running   0          16m</span><br><span class="line">kube-system   etcd-minikube                      1/1     Running   0          15m</span><br><span class="line">kube-system   coredns-5c98db65d4-d4t2h           1/1     Running   0          17m</span><br></pre></td></tr></table></figure><h2 id="Flink-实时处理脚本示例"><a href="#Flink-实时处理脚本示例" class="headerlink" title="Flink 实时处理脚本示例"></a>Flink 实时处理脚本示例</h2><p>我们可以编写一个简单的实时处理脚本，该脚本会从某个端口中读取文本，分割为单词，并且每 5 秒钟打印一次每个单词出现的次数。以下代码是从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/datastream_api.html#example-program" target="_blank" rel="noopener">Flink 官方文档</a> 上获取来的，完整的示例项目可以到 <a href="https://github.com/jizhang/flink-on-kubernetes" target="_blank" rel="noopener">GitHub</a> 上查看。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">    .socketTextStream(<span class="string">"192.168.99.1"</span>, <span class="number">9999</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">    .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">dataStream.print();</span><br></pre></td></tr></table></figure><p>K8s 容器中的程序可以通过 IP <code>192.168.99.1</code> 来访问 Minikube 宿主机上的服务。因此在运行上述代码之前，需要先在宿主机上执行 <code>nc -lk 9999</code> 命令打开一个端口。</p><p>接下来执行 <code>mvn clean package</code> 命令，打包好的 Jar 文件路径为 <code>target/flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar</code>。</p><h2 id="构建-Docker-容器镜像"><a href="#构建-Docker-容器镜像" class="headerlink" title="构建 Docker 容器镜像"></a>构建 Docker 容器镜像</h2><p>Flink 提供了一个官方的容器镜像，可以从 <a href="https://hub.docker.com/_/flink" target="_blank" rel="noopener">DockerHub</a> 上下载。我们将以这个镜像为基础，构建独立的脚本镜像，将打包好的 Jar 文件放置进去。此外，新版 Flink 已将 Hadoop 依赖从官方发行版中剥离，因此我们在打镜像时也需要包含进去。</p><p>简单看一下官方镜像的 <a href="https://github.com/docker-flink/docker-flink/blob/master/1.8/scala_2.12-debian/Dockerfile" target="_blank" rel="noopener">Dockerfile</a>，它做了以下几件事情：</p><ul><li>将 OpenJDK 1.8 作为基础镜像；</li><li>下载并安装 Flink 至 <code>/opt/flink</code> 目录中；</li><li>添加 <code>flink</code> 用户和组；</li><li>指定入口文件，不过我们会在 K8s 配置中覆盖此项。</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> openjdk:<span class="number">8</span>-jre</span><br><span class="line"><span class="keyword">ENV</span> FLINK_HOME=/opt/flink</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$FLINK_HOME</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> useradd flink &amp;&amp; \</span></span><br><span class="line"><span class="bash">  wget -O flink.tgz <span class="string">"<span class="variable">$FLINK_TGZ_URL</span>"</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">  tar -xf flink.tgz</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/docker-entrypoint.sh"</span>]</span></span><br></pre></td></tr></table></figure><p>在此基础上，我们编写新的 Dockerfile：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> flink:<span class="number">1.8</span>.<span class="number">1</span>-scala_2.<span class="number">12</span></span><br><span class="line"><span class="keyword">ARG</span> hadoop_jar</span><br><span class="line"><span class="keyword">ARG</span> job_jar</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --chown=flink:flink <span class="variable">$hadoop_jar</span> <span class="variable">$job_jar</span> <span class="variable">$FLINK_HOME</span>/lib/</span></span><br><span class="line"><span class="keyword">USER</span> flink</span><br></pre></td></tr></table></figure><p>在构建镜像之前，我们需要安装 Docker 命令行工具，并将其指向 Minikube 中的 Docker 服务，这样打出来的镜像才能被 K8s 使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew install docker</span><br><span class="line">$ <span class="built_in">eval</span> $(minikube docker-env)</span><br></pre></td></tr></table></figure><p>下载 <a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar" target="_blank" rel="noopener">Hadoop Jar 包</a>，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /path/to/Dockerfile</span><br><span class="line">$ cp /path/to/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar hadoop.jar</span><br><span class="line">$ cp /path/to/flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar job.jar</span><br><span class="line">$ docker build --build-arg hadoop_jar=hadoop.jar --build-arg job_jar=job.jar --tag flink-on-kubernetes:0.0.1 .</span><br></pre></td></tr></table></figure><p>脚本镜像打包完毕，可用于部署：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker image ls</span><br><span class="line">REPOSITORY           TAG    IMAGE ID      CREATED         SIZE</span><br><span class="line">flink-on-kubernetes  0.0.1  505d2f11cc57  10 seconds ago  618MB</span><br></pre></td></tr></table></figure><h2 id="部署-JobManager"><a href="#部署-JobManager" class="headerlink" title="部署 JobManager"></a>部署 JobManager</h2><p>首先，我们通过创建 Kubernetes Job 对象来部署 Flink JobManager。Job 和 Deployment 是 K8s 中两种不同的管理方式，他们都可以通过启动和维护多个 Pod 来执行任务。不同的是，Job 会在 Pod 执行完成后自动退出，而 Deployment 则会不断重启 Pod，直到手工删除。Pod 成功与否是通过命令行返回状态判断的，如果异常退出，Job 也会负责重启它。因此，Job 更适合用来部署 Flink 应用，当我们手工关闭一个 Flink 脚本时，K8s 就不会错误地重新启动它。</p><p>以下是 <a href="https://github.com/jizhang/flink-on-kubernetes/blob/master/docker/jobmanager.yml" target="_blank" rel="noopener"><code>jobmanager.yml</code></a> 配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">               <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dparallelism.default=1"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dblob.server.port=6124"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dqueryable-state.server.ports=6125"</span><span class="string">]</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br></pre></td></tr></table></figure><ul><li><code>${JOB}</code> 变量可以使用 <code>envsubst</code> 命令来替换，这样同一份配置文件就能够为多个脚本使用了；</li><li>容器的入口修改为了 <code>standalone-job.sh</code>，这是 Flink 的官方脚本，会以前台模式启动 JobManager，扫描类加载路径中的 <code>Main-Class</code> 作为脚本入口，我们也可以使用 <code>-j</code> 参数来指定完整的类名。之后，这个脚本会被自动提交到集群中。</li><li>JobManager 的 RPC 地址修改为了 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies" target="_blank" rel="noopener">Kubernetes Service</a> 的名称，我们将在下文创建。集群中的其他组件将通过这个名称来访问 JobManager。</li><li>Flink Blob Server &amp; Queryable State Server 的端口号默认是随机的，为了方便将其开放到集群中，我们修改为了固定端口。</li></ul><p>使用 <code>kubectl</code> 命令创建对象，并查看状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> JOB=flink-on-kubernetes</span><br><span class="line">$ envsubst &lt;jobmanager.yml | kubectl create -f -</span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager-kc4kq   1/1     Running   0          2m26s</span><br></pre></td></tr></table></figure><p>随后，我们创建一个 K8s Service 来将 JobManager 的端口开放出来，以便 TaskManager 前来注册：</p><p><code>service.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br></pre></td></tr></table></figure><p>这里 <code>type: NodePort</code> 是必要的，因为通过这项配置，我们可以在 K8s 集群之外访问 JobManager UI 和 RESTful API。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ envsubst &lt;service.yml | kubectl create -f -</span><br><span class="line">$ kubectl get service</span><br><span class="line">NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                      AGE</span><br><span class="line">flink-on-kubernetes-jobmanager   NodePort    10.109.78.143   &lt;none&gt;        6123:31476/TCP,6124:32268/TCP,6125:31602/TCP,8081:31254/TCP  15m</span><br></pre></td></tr></table></figure><p>我们可以看到，Flink Dashboard 开放在了虚拟机的 31254 端口上。Minikube 提供了一个命令，可以获取到 K8s 服务的访问地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ minikube service <span class="variable">$JOB</span>-jobmanager --url</span><br><span class="line">http://192.168.99.108:31476</span><br><span class="line">http://192.168.99.108:32268</span><br><span class="line">http://192.168.99.108:31602</span><br><span class="line">http://192.168.99.108:31254</span><br></pre></td></tr></table></figure><h2 id="部署-TaskManager"><a href="#部署-TaskManager" class="headerlink" title="部署 TaskManager"></a>部署 TaskManager</h2><p><code>taskmanager.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">      instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/taskmanager.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span> <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>通过修改 <code>replicas</code> 配置，我们可以开启多个 TaskManager。镜像中的 <code>taskmanager.numberOfTaskSlots</code> 参数默认为 <code>1</code>，这也是我们推荐的配置，因为扩容缩容方面的工作应该交由 K8s 来完成，而非直接使用 TaskManager 的槽位机制。</p><p>至此，Flink 脚本集群已经在运行中了。我们在之前已经打开的 <code>nc</code> 命令窗口中输入一些文本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ nc -lk 9999</span><br><span class="line">hello world</span><br><span class="line">hello flink</span><br></pre></td></tr></table></figure><p>打开另一个终端，查看 TaskManager 的标准输出日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs -f -l instance=<span class="variable">$JOB</span>-taskmanager</span><br><span class="line">(hello,2)</span><br><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure><h2 id="开启高可用模式"><a href="#开启高可用模式" class="headerlink" title="开启高可用模式"></a>开启高可用模式</h2><p>可用性方面，上述配置中的 TaskManager 如果发生故障退出，K8s 会自动进行重启，Flink 会从上一个 Checkpoint 中恢复工作。但是，JobManager 仍然存在单点问题，因此需要开启 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">HA 模式</a>，配合 ZooKeeper 和分布式文件系统（如 HDFS）来实现 JobManager 的高可用。在独立集群中，我们需要运行多个 JobManager，作为主备服务器。然而在 K8s 模式下，我们只需开启一个 JobManager，当其异常退出后，K8s 会负责重启，新的 JobManager 将从 ZooKeeper 和 HDFS 中读取最近的工作状态，自动恢复运行。</p><p>开启 HA 模式需要修改 JobManager 和 TaskManager 的启动命令：</p><p><code>jobmanager-ha.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dparallelism.default=1"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dblob.server.port=6124"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dqueryable-state.server.ports=6125"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability=zookeeper"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.quorum=192.168.99.1:2181"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.path.root=/flink"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.cluster-id=/$&#123;JOB&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.storageDir=hdfs://192.168.99.1:9000/flink/recovery"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.jobmanager.port=6123"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p><code>taskmanager-ha.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/taskmanager.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">"-Dhigh-availability=zookeeper"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.quorum=192.168.99.1:2181"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.path.root=/flink"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.cluster-id=/$&#123;JOB&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.storageDir=hdfs://192.168.99.1:9000/flink/recovery"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><ul><li>准备好 ZooKeeper 和 HDFS 测试环境，该配置中使用的是宿主机上的 <code>2181</code> 和 <code>9000</code> 端口；</li><li>Flink 集群基本信息会存储在 ZooKeeper 的 <code>/flink/${JOB}</code> 目录下；</li><li>Checkpoint 数据会存储在 HDFS 的 <code>/flink/recovery</code> 目录下。使用前，请先确保 Flink 有权限访问 HDFS 的 <code>/flink</code> 目录；</li><li><code>jobmanager.rpc.address</code> 选项从 TaskManager 的启动命令中去除了，是因为在 HA 模式下，TaskManager 会通过访问 ZooKeeper 来获取到当前 JobManager 的连接信息。需要注意的是，HA 模式下的 JobManager RPC 端口默认是随机的，我们需要使用 <code>high-availability.jobmanager.port</code> 配置项将其固定下来，方便在 K8s Service 中开放。</li></ul><h2 id="管理-Flink-脚本"><a href="#管理-Flink-脚本" class="headerlink" title="管理 Flink 脚本"></a>管理 Flink 脚本</h2><p>我们可以通过 RESTful API 来与 Flink 集群交互，其端口号默认与 Dashboard UI 一致。在宿主机上安装 Flink 命令行工具，传入 <code>-m</code> 参数来指定目标集群：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink list -m 192.168.99.108:30206</span><br><span class="line">------------------ Running/Restarting Jobs -------------------</span><br><span class="line">24.08.2019 12:50:28 : 00000000000000000000000000000000 : Window WordCount (RUNNING)</span><br><span class="line">--------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>在 HA 模式下，Flink 脚本 ID 默认为 <code>00000000000000000000000000000000</code>，我们可以使用这个 ID 来手工停止脚本，并生成一个 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/savepoints.html" target="_blank" rel="noopener">SavePoint</a> 快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink cancel -m 192.168.99.108:30206 -s hdfs://192.168.99.1:9000/flink/savepoints/ 00000000000000000000000000000000</span><br><span class="line">Cancelled job 00000000000000000000000000000000. Savepoint stored <span class="keyword">in</span> hdfs://192.168.99.1:9000/flink/savepoints/savepoint-000000-f776c8e50a0c.</span><br></pre></td></tr></table></figure><p>执行完毕后，可以看到 K8s Job 对象的状态变为了已完成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get job</span><br><span class="line">NAME                             COMPLETIONS   DURATION   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager   1/1           4m40s      7m14s</span><br></pre></td></tr></table></figure><p>重新启动脚本前，我们需要先将配置从 K8s 中删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete job <span class="variable">$JOB</span>-jobmanager</span><br><span class="line">$ kubectl delete deployment <span class="variable">$JOB</span>-taskmanager</span><br></pre></td></tr></table></figure><p>然后在 JobManager 的启动命令中加入 <code>--fromSavepoint</code> 参数：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">...</span></span><br><span class="line">       <span class="string">"--fromSavepoint"</span><span class="string">,</span> <span class="string">"$&#123;SAVEPOINT&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p>使用刚才得到的 SavePoint 路径替换该变量，并启动 JobManager：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> SAVEPOINT=hdfs://192.168.99.1:9000/flink/savepoints/savepoint-000000-f776c8e50a0c</span><br><span class="line">$ envsubst &lt;jobmanager-savepoint.yml | kubectl create -f -</span><br></pre></td></tr></table></figure><p>需要注意的是，SavePoint 必须和 HA 模式配合使用，因为当 JobManager 异常退出、K8s 重启它时，都会传入 <code>--fromSavepoint</code>，使脚本进入一个异常的状态。而在开启 HA 模式时，JobManager 会优先读取最近的 CheckPoint 并从中恢复，忽略命令行中传入的 SavePoint。</p><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>有两种方式可以对 Flink 脚本进行扩容。第一种方式是用上文提到的 SavePoint 机制：手动关闭脚本，并使用新的 <code>replicas</code> 和 <code>parallelism.default</code> 参数进行重启；另一种方式则是使用 <code>flink modify</code> 命令行工具，该工具的工作机理和人工操作类似，也是先用 SavePoint 停止脚本，然后以新的并发度启动。在使用第二种方式前，我们需要在启动命令中指定默认的 SavePoint 路径：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">...</span></span><br><span class="line">       <span class="string">"-Dstate.savepoints.dir=hdfs://192.168.99.1:9000/flink/savepoints/"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p>然后，使用 <code>kubectl scale</code> 命令调整 TaskManager 的个数；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale --replicas=2 deployment/<span class="variable">$JOB</span>-taskmanager</span><br><span class="line">deployment.extensions/flink-on-kubernetes-taskmanager scaled</span><br></pre></td></tr></table></figure><p>最后，使用 <code>flink modify</code> 调整脚本并发度：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink modify 755877434b676ce9dae5cfb533ed7f33 -m 192.168.99.108:30206 -p 2</span><br><span class="line">Modify job 755877434b676ce9dae5cfb533ed7f33.</span><br><span class="line">Rescaled job 755877434b676ce9dae5cfb533ed7f33. Its new parallelism is 2.</span><br></pre></td></tr></table></figure><p>但是，因为存在一个尚未解决的 <a href="https://issues.apache.org/jira/browse/FLINK-11997" target="_blank" rel="noopener">Issue</a>，我们无法使用 <code>flink modify</code> 命令来对 HA 模式下的 Flink 集群进行扩容，因此还请使用人工的方式操作。</p><h2 id="Flink-将原生支持-Kubernetes"><a href="#Flink-将原生支持-Kubernetes" class="headerlink" title="Flink 将原生支持 Kubernetes"></a>Flink 将原生支持 Kubernetes</h2><p>Flink 有着非常活跃的开源社区，他们不断改进自身设计（<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a>），以适应现今的云原生环境。他们也注意到了 Kubernetes 的蓬勃发展，对 K8s 集群的原生支持也在开发中。我们知道，Flink 可以直接运行在 YARN 或 Mesos 资源管理框架上。以 YARN 为例，Flink 首先启动一个 ApplicationMaster，作为 JobManager，分析提交的脚本需要多少资源，并主动向 YARN ResourceManager 申请，开启对应的 TaskManager。当脚本的并行度改变后，Flink 会自动新增或释放 TaskManager 容器，达到扩容缩容的目的。这种主动管理资源的模式，社区正在开发针对 Kubernetes 的版本（<a href="https://issues.apache.org/jira/browse/FLINK-9953" target="_blank" rel="noopener">FLINK-9953</a>），今后我们便可以使用简单的命令来将 Flink 部署到 K8s 上了。</p><p>此外，另一种资源管理模式也在开发中，社区称为响应式容器管理（<a href="https://issues.apache.org/jira/browse/FLINK-10407" target="_blank" rel="noopener">FLINK-10407 Reactive container mode</a>）。简单来说，当 JobManager 发现手中有多余的 TaskManager 时，会自动将运行中的脚本扩容到相应的并发度。以上文中的操作为例，我们只需使用 <code>kubectl scale</code> 命令修改 TaskManager Deployment 的 <code>replicas</code> 个数，就能够达到扩容和缩容的目的，无需再执行 <code>flink modify</code>。相信不久的将来我们就可以享受到这些便利的功能。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/kubernetes.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/kubernetes.html</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/</a></li><li><a href="https://jobs.zalando.com/tech/blog/running-apache-flink-on-kubernetes/" target="_blank" rel="noopener">https://jobs.zalando.com/tech/blog/running-apache-flink-on-kubernetes/</a></li><li><a href="https://www.slideshare.net/tillrohrmann/redesigning-apache-flinks-distributed-architecture-flink-forward-2017" target="_blank" rel="noopener">https://www.slideshare.net/tillrohrmann/redesigning-apache-flinks-distributed-architecture-flink-forward-2017</a></li><li><a href="https://www.slideshare.net/tillrohrmann/future-of-apache-flink-deployments-containers-kubernetes-and-more-flink-forward-2019-sf" target="_blank" rel="noopener">https://www.slideshare.net/tillrohrmann/future-of-apache-flink-deployments-containers-kubernetes-and-more-flink-forward-2019-sf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes&lt;/a&gt; 是目前非常流行的容器编排系统，在其之上可以运行 Web 服务、大数据处理等各类应用。这些应用被打包在一个个非常轻量的容器中，我们通过声明的方式来告知 Kubernetes 要如何部署和扩容这些程序，并对外提供服务。&lt;a href=&quot;https://flink.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Flink&lt;/a&gt; 同样是非常流行的分布式处理框架，它也可以运行在 Kubernetes 之上。将两者相结合，我们就可以得到一个健壮和高可扩的数据处理应用，并且能够更安全地和其它服务共享一个 Kubernetes 集群。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/flink-on-kubernetes.png&quot; alt=&quot;Flink on Kubernetes&quot;&gt;&lt;/p&gt;
&lt;p&gt;在 Kubernetes 上部署 Flink 有两种方式：会话集群（Session Cluster）和脚本集群（Job Cluster）。会话集群和独立部署一个 Flink 集群类似，只是底层资源换成了 K8s 容器，而非直接运行在操作系统上。该集群可以提交多个脚本，因此适合运行那些短时脚本和即席查询。脚本集群则是为单个脚本部署一整套服务，包括 JobManager 和 TaskManager，运行结束后这些资源也随即释放。我们需要为每个脚本构建专门的容器镜像，分配独立的资源，因而这种方式可以更好地和其他脚本隔离开，同时便于扩容或缩容。文本将以脚本集群为例，演示如何在 K8s 上运行 Flink 实时处理程序，主要步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;编译并打包 Flink 脚本 Jar 文件；&lt;/li&gt;
&lt;li&gt;构建 Docker 容器镜像，添加 Flink 运行时库和上述 Jar 包；&lt;/li&gt;
&lt;li&gt;使用 Kubernetes Job 部署 Flink JobManager 组件；&lt;/li&gt;
&lt;li&gt;使用 Kubernetes Service 将 JobManager 服务端口开放到集群中；&lt;/li&gt;
&lt;li&gt;使用 Kubernetes Deployment 部署 Flink TaskManager；&lt;/li&gt;
&lt;li&gt;配置 Flink JobManager 高可用，需使用 ZooKeeper 和 HDFS；&lt;/li&gt;
&lt;li&gt;借助 Flink SavePoint 机制来停止和恢复脚本。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="flink" scheme="http://shzhangji.com/cnblogs/tags/flink/"/>
    
      <category term="kubernetes" scheme="http://shzhangji.com/cnblogs/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Hive ACID 事务表</title>
    <link href="http://shzhangji.com/cnblogs/2019/06/11/understanding-hive-acid-transactional-table/"/>
    <id>http://shzhangji.com/cnblogs/2019/06/11/understanding-hive-acid-transactional-table/</id>
    <published>2019-06-11T12:40:55.000Z</published>
    <updated>2020-08-22T12:06:11.270Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a> 0.13 版本引入了事务特性，能够在 Hive 表上实现 ACID 语义，包括 INSERT/UPDATE/DELETE/MERGE 语句、增量数据抽取等。Hive 3.0 又对该特性进行了优化，包括改进了底层的文件组织方式，减少了对表结构的限制，以及支持条件下推和向量化查询。Hive 事务表的介绍和使用方法可以参考 <a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">Hive Wiki</a> 和 <a href="https://hortonworks.com/tutorial/using-hive-acid-transactions-to-insert-update-and-delete-data/" target="_blank" rel="noopener">各类教程</a>，本文将重点讲述 Hive 事务表是如何在 HDFS 上存储的，及其读写过程是怎样的。</p><h2 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h2><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee (<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, salary <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC TBLPROPERTIES (<span class="string">'transactional'</span> = <span class="string">'true'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employee <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">5000</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'Tom'</span>,   <span class="number">8000</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">'Kate'</span>,  <span class="number">6000</span>);</span><br></pre></td></tr></table></figure><p>INSERT 语句会在一个事务中运行。它会创建名为 <code>delta</code> 的目录，存放事务的信息和表的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/_orc_acid_version</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000</span><br></pre></td></tr></table></figure><p>目录名称的格式为 <code>delta_minWID_maxWID_stmtID</code>，即 delta 前缀、写事务的 ID 范围、以及语句 ID。具体来说：</p><ul><li>所有 INSERT 语句都会创建 <code>delta</code> 目录。UPDATE 语句也会创建 <code>delta</code> 目录，但会先创建一个 <code>delete</code> 目录，即先删除、后插入。<code>delete</code> 目录的前缀是 delete_delta；</li><li>Hive 会为所有的事务生成一个全局唯一的 ID，包括读操作和写操作。针对写事务（INSERT、DELETE 等），Hive 还会创建一个写事务 ID（Write ID），该 ID 在表范围内唯一。写事务 ID 会编码到 <code>delta</code> 和 <code>delete</code> 目录的名称中；</li><li>语句 ID（Statement ID）则是当一个事务中有多条写入语句时使用的，用作唯一标识。</li></ul><a id="more"></a><p>再看文件内容，<code>_orc_acid_version</code> 的内容是 2，即当前 ACID 版本号是 2。它和版本 1 的主要区别是 UPDATE 语句采用了 split-update 特性，即上文提到的先删除、后插入。这个特性能够使 ACID 表支持条件下推等功能，具体可以查看 <a href="https://jira.apache.org/jira/browse/HIVE-14035" target="_blank" rel="noopener">HIVE-14035</a>。<code>bucket_00000</code> 文件则是写入的数据内容。由于这张表没有分区和分桶，所以只有这一个文件。事务表都以 <a href="https://orc.apache.org/" target="_blank" rel="noopener">ORC</a> 格式存储的，我们可以使用 <a href="https://orc.apache.org/docs/java-tools.html" target="_blank" rel="noopener">orc-tools</a> 来查看文件的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ orc-tools data bucket_00000</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:0,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;Jerry&quot;,&quot;salary&quot;:5000&#125;&#125;</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:1,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;,&quot;salary&quot;:8000&#125;&#125;</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:2,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;Kate&quot;,&quot;salary&quot;:6000&#125;&#125;</span><br></pre></td></tr></table></figure><p>输出内容被格式化为了一行行的 JSON 字符串，我们可以看到具体数据是在 <code>row</code> 这个键中的，其它键则是 Hive 用来实现事务特性所使用的，具体含义为：</p><ul><li><code>operation</code> 0 表示插入，1 表示更新，2 表示删除。由于使用了 split-update，UPDATE 是不会出现的；</li><li><code>originalTransaction</code> 是该条记录的原始写事务 ID。对于 INSERT 操作，该值和 <code>currentTransaction</code> 是一致的。对于 DELETE，则是该条记录第一次插入时的写事务 ID；</li><li><code>bucket</code> 是一个 32 位整型，由 <code>BucketCodec</code> 编码，各个二进制位的含义为：<ul><li>1-3 位：编码版本，当前是 <code>001</code>；</li><li>4 位：保留；</li><li>5-16 位：分桶 ID，由 0 开始。分桶 ID 是由 CLUSTERED BY 子句所指定的字段、以及分桶的数量决定的。该值和 <code>bucket_N</code> 中的 N 一致；</li><li>17-20 位：保留；</li><li>21-32 位：语句 ID；</li><li>举例来说，整型 <code>536936448</code> 的二进制格式为 <code>00100000000000010000000000000000</code>，即它是按版本 1 的格式编码的，分桶 ID 为 1；</li></ul></li><li><code>rowId</code> 是一个自增的唯一 ID，在写事务和分桶的组合中唯一；</li><li><code>currentTransaction</code> 当前的写事务 ID；</li><li><code>row</code> 具体数据。对于 DELETE 语句，则为 <code>null</code>。</li></ul><p>我们可以注意到，文件中的数据会按 (<code>originalTransaction</code>, <code>bucket</code>, <code>rowId</code>) 进行排序，这点对后面的读取操作非常关键。</p><p>这些信息还可以通过 <code>row__id</code> 这个虚拟列进行查看：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> row__id, <span class="keyword">id</span>, <span class="keyword">name</span>, salary <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:0&#125;    1       Jerry   5000</span><br><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:1&#125;    2       Tom     8000</span><br><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:2&#125;    3       Kate    6000</span><br></pre></td></tr></table></figure><h4 id="增量数据抽取-API-V2"><a href="#增量数据抽取-API-V2" class="headerlink" title="增量数据抽取 API V2"></a>增量数据抽取 API V2</h4><p>Hive 3.0 还改进了先前的 <a href="https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest+V2" target="_blank" rel="noopener">增量抽取 API</a>，通过这个 API，用户或第三方工具（Flume 等）就可以利用 ACID 特性持续不断地向 Hive 表写入数据了。这一操作同样会生成 <code>delta</code> 目录，但更新和删除操作不再支持。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StreamingConnection connection = HiveStreamingConnection.newBuilder().connect();</span><br><span class="line">connection.beginTransaction();</span><br><span class="line">connection.write(<span class="string">"11,val11,Asia,China"</span>.getBytes());</span><br><span class="line">connection.write(<span class="string">"12,val12,Asia,India"</span>.getBytes());</span><br><span class="line">connection.commitTransaction();</span><br><span class="line">connection.close();</span><br></pre></td></tr></table></figure><h3 id="更新数据"><a href="#更新数据" class="headerlink" title="更新数据"></a>更新数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> employee <span class="keyword">SET</span> salary = <span class="number">7000</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> = <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>这条语句会先查询出所有符合条件的记录，获取它们的 <code>row__id</code> 信息，然后分别创建 <code>delete</code> 和 <code>delta</code> 目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000</span><br><span class="line">/user/hive/warehouse/employee/delete_delta_0000002_0000002_0000/bucket_00000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0000/bucket_00000</span><br></pre></td></tr></table></figure><p><code>delete_delta_0000002_0000002_0000/bucket_00000</code> 包含了删除的记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;operation&quot;:2,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:1,&quot;currentTransaction&quot;:2,&quot;row&quot;:null&#125;</span><br></pre></td></tr></table></figure><p><code>delta_0000002_0000002_0000/bucket_00000</code> 包含更新后的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:2,&quot;bucket&quot;:536870912,&quot;rowId&quot;:0,&quot;currentTransaction&quot;:2,&quot;row&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;,&quot;salary&quot;:7000&#125;&#125;</span><br></pre></td></tr></table></figure><p>DELETE 语句的工作方式类似，同样是先查询，后生成 <code>delete</code> 目录。</p><h3 id="合并表"><a href="#合并表" class="headerlink" title="合并表"></a>合并表</h3><p>MERGE 语句和 MySQL 的 INSERT ON UPDATE 功能类似，它可以将来源表的数据合并到目标表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_update (<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employee_update <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">2</span>, <span class="string">'Tom'</span>,  <span class="number">7000</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="string">'Mary'</span>, <span class="number">9000</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> employee <span class="keyword">AS</span> a</span><br><span class="line"><span class="keyword">USING</span> employee_update <span class="keyword">AS</span> b <span class="keyword">ON</span> a.id = b.id</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span> <span class="keyword">UPDATE</span> <span class="keyword">SET</span> salary = b.salary</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span> <span class="keyword">INSERT</span> <span class="keyword">VALUES</span> (b.id, b.name, b.salary);</span><br></pre></td></tr></table></figure><p>这条语句会更新 Tom 的薪资字段，并插入一条 Mary 的新记录。多条 WHEN 子句会被视为不同的语句，有各自的语句 ID（Statement ID）。INSERT 子句会创建 <code>delta_0000002_0000002_0000</code> 文件，内容是 Mary 的数据；UPDATE 语句则会创建 <code>delete_delta_0000002_0000002_0001</code> 和 <code>delta_0000002_0000002_0001</code> 两个文件，删除并新增 Tom 的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0000</span><br><span class="line">/user/hive/warehouse/employee/delete_delta_0000002_0000002_0001</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0001</span><br></pre></td></tr></table></figure><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>随着写操作的积累，表中的 <code>delta</code> 和 <code>delete</code> 文件会越来越多。事务表的读取过程中需要合并所有文件，数量一多势必会影响效率。此外，小文件对 HDFS 这样的文件系统也是不够友好的。因此，Hive 引入了压缩（Compaction）的概念，分为 Minor 和 Major 两类。</p><p>Minor Compaction 会将所有的 <code>delta</code> 文件压缩为一个文件，<code>delete</code> 也压缩为一个。压缩后的结果文件名中会包含写事务 ID 范围，同时省略掉语句 ID。压缩过程是在 Hive Metastore 中运行的，会根据一定阈值自动触发。我们也可以使用如下语句人工触发：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> employee <span class="keyword">COMPACT</span> <span class="string">'minor'</span>;</span><br></pre></td></tr></table></figure><p>以上文中的 MERGE 语句的结果举例，在运行了一次 Minor Compaction 后，文件目录结构将变为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delete_delta_0000001_0000002</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000002</span><br></pre></td></tr></table></figure><p>在 <code>delta_0000001_0000002/bucket_00000</code> 文件中，数据会被排序和合并起来，因此文件中将包含两行 Tom 的数据。Minor Compaction 不会删除任何数据。</p><p>Major Compaction 则会将所有文件合并为一个文件，以 <code>base_N</code> 的形式命名，其中 N 表示最新的写事务 ID。已删除的数据将在这个过程中被剔除。<code>row__id</code> 则按原样保留。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/base_0000002</span><br></pre></td></tr></table></figure><p>需要注意的是，在 Minor 或 Major Compaction 执行之后，原来的文件不会被立刻删除。这是因为删除的动作是在另一个名为 Cleaner 的线程中执行的。因此，表中可能同时存在不同事务 ID 的文件组合，这在读取过程中需要做特殊处理。</p><h2 id="读取过程"><a href="#读取过程" class="headerlink" title="读取过程"></a>读取过程</h2><p>我们可以看到 ACID 事务表中会包含三类文件，分别是 <code>base</code>、<code>delta</code>、以及 <code>delete</code>。文件中的每一行数据都会以 <code>row__id</code> 作为标识并排序。从 ACID 事务表中读取数据就是对这些文件进行合并，从而得到最新事务的结果。这一过程是在 <code>OrcInputFormat</code> 和 <code>OrcRawRecordMerger</code> 类中实现的，本质上是一个合并排序的算法。</p><p>以下列文件为例，产生这些文件的操作为：插入三条记录，进行一次 Major Compaction，然后更新两条记录。<code>1-0-0-1</code> 是对 <code>originalTransaction</code> - <code>bucketId</code> - <code>rowId</code> - <code>currentTransaction</code> 的缩写。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------+    +----------+    +----------+</span><br><span class="line">| base_1   |    | delete_2 |    | delta_2  |</span><br><span class="line">+----------+    +----------+    +----------+</span><br><span class="line">| 1-0-0-1  |    | 1-0-1-2  |    | 2-0-0-2  |</span><br><span class="line">| 1-0-1-1  |    | 1-0-2-2  |    | 2-0-1-2  |</span><br><span class="line">| 1-0-2-1  |    +----------+    +----------+</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>合并过程为：</p><ul><li>对所有数据行按照 (<code>originalTransaction</code>, <code>bucketId</code>, <code>rowId</code>) 正序排列，(<code>currentTransaction</code>) 倒序排列，即：<ul><li><code>1-0-0-1</code></li><li><code>1-0-1-2</code></li><li><code>1-0-1-1</code></li><li>…</li><li><code>2-0-1-2</code></li></ul></li><li>获取第一条记录；</li><li>如果当前记录的 <code>row__id</code> 和上条数据一样，则跳过；</li><li>如果当前记录的操作类型为 DELETE，也跳过；<ul><li>通过以上两条规则，对于 <code>1-0-1-2</code> 和 <code>1-0-1-1</code>，这条记录会被跳过；</li></ul></li><li>如果没有跳过，记录将被输出给下游；</li><li>重复以上过程。</li></ul><p>合并过程是流式的，即 Hive 会将所有文件打开，预读第一条记录，并将 <code>row__id</code> 信息存入到 <code>ReaderKey</code> 类型中。该类型实现了 <code>Comparable</code> 接口，因此可以按照上述规则自定义排序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordIdentifier</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">RecordIdentifier</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> writeId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> bucketId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> rowId;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">int</span> <span class="title">compareToInternal</span><span class="params">(RecordIdentifier other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (other == <span class="keyword">null</span>) &#123; <span class="keyword">return</span> -<span class="number">1</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (writeId != other.writeId) &#123; <span class="keyword">return</span> writeId &lt; other.writeId ? -<span class="number">1</span> : <span class="number">1</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (bucketId != other.bucketId) &#123; <span class="keyword">return</span> bucketId &lt; other.bucketId ? - <span class="number">1</span> : <span class="number">1</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (rowId != other.rowId) &#123; <span class="keyword">return</span> rowId &lt; other.rowId ? -<span class="number">1</span> : <span class="number">1</span>; &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReaderKey</span> <span class="keyword">extends</span> <span class="title">RecordIdentifier</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> currentWriteId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">boolean</span> isDeleteEvent = <span class="keyword">false</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(RecordIdentifier other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sup = compareToInternal(other);</span><br><span class="line">    <span class="keyword">if</span> (sup == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (other.getClass() == ReaderKey.class) &#123;</span><br><span class="line">        ReaderKey oth = (ReaderKey) other;</span><br><span class="line">        <span class="keyword">if</span> (currentWriteId != oth.currentWriteId) &#123; <span class="keyword">return</span> currentWriteId &lt; oth.currentWriteId ? +<span class="number">1</span> : -<span class="number">1</span>; &#125;</span><br><span class="line">        <span class="keyword">if</span> (isDeleteEvent != oth.isDeleteEvent) &#123; <span class="keyword">return</span> isDeleteEvent ? -<span class="number">1</span> : +<span class="number">1</span>; &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sup;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后，<code>ReaderKey</code> 会和文件句柄一起存入到 <code>TreeMap</code> 结构中。根据该结构的特性，我们每次获取第一个元素时就能得到排序后的结果，并读取数据了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrcRawRecordMerger</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> TreeMap&lt;ReaderKey, ReaderPair&gt; readers = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">next</span><span class="params">(RecordIdentifier recordIdentifier, OrcStruct prev)</span> </span>&#123;</span><br><span class="line">    Map.Entry&lt;ReaderKey, ReaderPair&gt; entry = readers.pollFirstEntry();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="选择文件"><a href="#选择文件" class="headerlink" title="选择文件"></a>选择文件</h3><p>上文中提到，事务表目录中会同时存在多个事务的快照文件，因此 Hive 首先要选择出反映了最新事务结果的文件集合，然后再进行合并。举例来说，下列文件是一系列操作后的结果：两次插入，一次 Minor Compaction，一次 Major Compaction，一次删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">delta_0000001_0000001_0000</span><br><span class="line">delta_0000002_0000002_0000</span><br><span class="line">delta_0000001_0000002</span><br><span class="line">base_0000002</span><br><span class="line">delete_delta_0000003_0000003_0000</span><br></pre></td></tr></table></figure><p>过滤过程为：</p><ul><li>从 Hive Metastore 中获取所有成功提交的写事务 ID 列表；</li><li>从文件名中解析出文件类型、写事务 ID 范围、以及语句 ID；</li><li>选取写事务 ID 最大且合法的那个 <code>base</code> 目录，如果存在的话；</li><li>对 <code>delta</code> 和 <code>delete</code> 文件进行排序：<ul><li><code>minWID</code> 较小的优先；</li><li>如果 <code>minWID</code> 相等，则 <code>maxWID</code> 较大的优先；</li><li>如果都相等，则按 <code>stmtID</code> 排序；没有 <code>stmtID</code> 的会排在前面；</li></ul></li><li>将 <code>base</code> 文件中的写事务 ID 作为当前 ID，循环过滤所有 <code>delta</code> 文件：<ul><li>如果 <code>maxWID</code> 大于当前 ID，则保留这个文件，并以此更新当前 ID；</li><li>如果 ID 范围相同，也会保留这个文件；</li><li>重复上述步骤。</li></ul></li></ul><p>过滤过程中还会处理一些特别的情况，如没有 <code>base</code> 文件，有多条语句，包含原始文件（即不含 <code>row__id</code> 信息的文件，一般是通过 LOAD DATA 导入的），以及 ACID 版本 1 格式的文件等。具体可以参考 <code>AcidUtils#getAcidState</code> 方法。</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>在 Map-Reduce 模式下运行 Hive 时，多个 Mapper 是并行执行的，这就需要将 <code>delta</code> 文件按一定的规则组织好。简单来说，<code>base</code> 和 <code>delta</code> 文件会被分配到不同的分片（Split）中，但所有分片都需要能够读取所有的 <code>delete</code> 文件，从而根据它们忽略掉已删除的记录。</p><p><img src="/cnblogs/images/hive-acid/parallel-execution.png" alt="Parallel Execution"></p><h3 id="向量化查询"><a href="#向量化查询" class="headerlink" title="向量化查询"></a>向量化查询</h3><p>当 <a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution" target="_blank" rel="noopener">向量化查询</a> 特性开启时，Hive 会尝试将所有的 <code>delete</code> 文件读入内存，并维护一个特定的数据结构，能够快速地对数据进行过滤。如果内存放不下，则会像上文提到的过程一样，逐步读取 <code>delete</code> 文件，使用合并排序的算法进行过滤。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VectorizedOrcAcidRowBatchReader</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> DeleteEventRegistry deleteEventRegistry;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">static</span> <span class="class"><span class="keyword">interface</span> <span class="title">DeleteEventRegistry</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findDeletedRecords</span><span class="params">(ColumnVector[] cols, <span class="keyword">int</span> size, BitSet selectedBitSet)</span></span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnizedDeleteEventRegistry</span> <span class="keyword">implements</span> <span class="title">DeleteEventRegistry</span> </span>&#123;&#125;</span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SortMergedDeleteEventRegistry</span> <span class="keyword">implements</span> <span class="title">DeleteEventRegistry</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">next</span><span class="params">(NullWritable key, VectorizedRowBatch value)</span> </span>&#123;</span><br><span class="line">    BitSet selectedBitSet = <span class="keyword">new</span> BitSet(vectorizedRowBatchBase.size);</span><br><span class="line">    <span class="keyword">this</span>.deleteEventRegistry.findDeletedRecords(innerRecordIdColumnVector,</span><br><span class="line">        vectorizedRowBatchBase.size, selectedBitSet);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> setBitIndex = selectedBitSet.nextSetBit(<span class="number">0</span>), selectedItr = <span class="number">0</span>;</span><br><span class="line">        setBitIndex &gt;= <span class="number">0</span>;</span><br><span class="line">        setBitIndex = selectedBitSet.nextSetBit(setBitIndex+<span class="number">1</span>), ++selectedItr) &#123;</span><br><span class="line">      value.selected[selectedItr] = setBitIndex;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="事务管理"><a href="#事务管理" class="headerlink" title="事务管理"></a>事务管理</h2><p>为了实现 ACID 事务机制，Hive 还引入了新的事务管理器 <code>DbTxnManager</code>，它能够在查询计划中分辨出 ACID 事务表，联系 Hive Metastore 打开新的事务，完成后提交事务。它也同时实现了过去的读写锁机制，用来支持非事务表的情形。</p><p><img src="/cnblogs/images/hive-acid/transaction-management.png" alt="Transaction Management"></p><p>Hive Metastore 负责分配新的事务 ID。这一过程是在一个数据库事务中完成的，从而避免多个 Metastore 实例冲突的情况。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TxnHandler</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> List&lt;Long&gt; <span class="title">openTxns</span><span class="params">(Connection dbConn, Statement stmt, OpenTxnRequest rqst)</span> </span>&#123;</span><br><span class="line">    String s = sqlGenerator.addForUpdateClause(<span class="string">"select ntxn_next from NEXT_TXN_ID"</span>);</span><br><span class="line">    s = <span class="string">"update NEXT_TXN_ID set ntxn_next = "</span> + (first + numTxns);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> i = first; i &lt; first + numTxns; i++) &#123;</span><br><span class="line">      txnIds.add(i);</span><br><span class="line">      rows.add(i + <span class="string">","</span> + quoteChar(TXN_OPEN) + <span class="string">","</span> + now + <span class="string">","</span> + now + <span class="string">","</span></span><br><span class="line">          + quoteString(rqst.getUser()) + <span class="string">","</span> + quoteString(rqst.getHostname()) + <span class="string">","</span> + txnType.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;String&gt; queries = sqlGenerator.createInsertValuesStmt(</span><br><span class="line">        <span class="string">"TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host, txn_type)"</span>, rows);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">Hive Transactions</a></li><li><a href="https://www.slideshare.net/Hadoop_Summit/transactional-operations-in-apache-hive-present-and-future-102803358" target="_blank" rel="noopener">Transactional Operations in Apache Hive</a></li><li><a href="https://orc.apache.org/docs/acid.html" target="_blank" rel="noopener">ORCFile ACID Support</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://hive.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Hive&lt;/a&gt; 0.13 版本引入了事务特性，能够在 Hive 表上实现 ACID 语义，包括 INSERT/UPDATE/DELETE/MERGE 语句、增量数据抽取等。Hive 3.0 又对该特性进行了优化，包括改进了底层的文件组织方式，减少了对表结构的限制，以及支持条件下推和向量化查询。Hive 事务表的介绍和使用方法可以参考 &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hive Wiki&lt;/a&gt; 和 &lt;a href=&quot;https://hortonworks.com/tutorial/using-hive-acid-transactions-to-insert-update-and-delete-data/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;各类教程&lt;/a&gt;，本文将重点讲述 Hive 事务表是如何在 HDFS 上存储的，及其读写过程是怎样的。&lt;/p&gt;
&lt;h2 id=&quot;文件结构&quot;&gt;&lt;a href=&quot;#文件结构&quot; class=&quot;headerlink&quot; title=&quot;文件结构&quot;&gt;&lt;/a&gt;文件结构&lt;/h2&gt;&lt;h3 id=&quot;插入数据&quot;&gt;&lt;a href=&quot;#插入数据&quot; class=&quot;headerlink&quot; title=&quot;插入数据&quot;&gt;&lt;/a&gt;插入数据&lt;/h3&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;TABLE&lt;/span&gt; employee (&lt;span class=&quot;keyword&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;string&lt;/span&gt;, salary &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;STORED&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; ORC TBLPROPERTIES (&lt;span class=&quot;string&quot;&gt;&#39;transactional&#39;&lt;/span&gt; = &lt;span class=&quot;string&quot;&gt;&#39;true&#39;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;INTO&lt;/span&gt; employee &lt;span class=&quot;keyword&quot;&gt;VALUES&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Jerry&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5000&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Tom&#39;&lt;/span&gt;,   &lt;span class=&quot;number&quot;&gt;8000&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Kate&#39;&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6000&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;INSERT 语句会在一个事务中运行。它会创建名为 &lt;code&gt;delta&lt;/code&gt; 的目录，存放事务的信息和表的数据。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000/_orc_acid_version&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;目录名称的格式为 &lt;code&gt;delta_minWID_maxWID_stmtID&lt;/code&gt;，即 delta 前缀、写事务的 ID 范围、以及语句 ID。具体来说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有 INSERT 语句都会创建 &lt;code&gt;delta&lt;/code&gt; 目录。UPDATE 语句也会创建 &lt;code&gt;delta&lt;/code&gt; 目录，但会先创建一个 &lt;code&gt;delete&lt;/code&gt; 目录，即先删除、后插入。&lt;code&gt;delete&lt;/code&gt; 目录的前缀是 delete_delta；&lt;/li&gt;
&lt;li&gt;Hive 会为所有的事务生成一个全局唯一的 ID，包括读操作和写操作。针对写事务（INSERT、DELETE 等），Hive 还会创建一个写事务 ID（Write ID），该 ID 在表范围内唯一。写事务 ID 会编码到 &lt;code&gt;delta&lt;/code&gt; 和 &lt;code&gt;delete&lt;/code&gt; 目录的名称中；&lt;/li&gt;
&lt;li&gt;语句 ID（Statement ID）则是当一个事务中有多条写入语句时使用的，用作唯一标识。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="hadoop" scheme="http://shzhangji.com/cnblogs/tags/hadoop/"/>
    
      <category term="hive" scheme="http://shzhangji.com/cnblogs/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>使用 Apache Flink 开发实时 ETL</title>
    <link href="http://shzhangji.com/cnblogs/2018/12/30/real-time-exactly-once-etl-with-apache-flink/"/>
    <id>http://shzhangji.com/cnblogs/2018/12/30/real-time-exactly-once-etl-with-apache-flink/</id>
    <published>2018-12-30T04:39:06.000Z</published>
    <updated>2020-10-06T07:26:18.695Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 是大数据领域又一新兴框架。它与 Spark 的不同之处在于，它是使用流式处理来模拟批量处理的，因此能够提供亚秒级的、符合 Exactly-once 语义的实时处理能力。Flink 的使用场景之一是构建实时的数据通道，在不同的存储之间搬运和转换数据。本文将介绍如何使用 Flink 开发实时 ETL 程序，并介绍 Flink 是如何保证其 Exactly-once 语义的。</p><p><img src="/cnblogs/images/flink/arch.png" alt="Apache Flink"></p><h2 id="示例程序"><a href="#示例程序" class="headerlink" title="示例程序"></a>示例程序</h2><p>让我们来编写一个从 Kafka 抽取数据到 HDFS 的程序。数据源是一组事件日志，其中包含了事件发生的时间，以时间戳的方式存储。我们需要将这些日志按事件时间分别存放到不同的目录中，即按日分桶。时间日志示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;timestamp&quot;:1545184226.432,&quot;event&quot;:&quot;page_view&quot;,&quot;uuid&quot;:&quot;ac0e50bf-944c-4e2f-bbf5-a34b22718e0c&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184602.640,&quot;event&quot;:&quot;adv_click&quot;,&quot;uuid&quot;:&quot;9b220808-2193-44d1-a0e9-09b9743dec55&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184608.969,&quot;event&quot;:&quot;thumbs_up&quot;,&quot;uuid&quot;:&quot;b44c3137-4c91-4f36-96fb-80f56561c914&quot;&#125;</span><br></pre></td></tr></table></figure><p>产生的目录结构为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flink/event_log/dt=20181219/part-0-1</span><br><span class="line">/user/flink/event_log/dt=20181220/part-1-9</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p>Flink 应用程序需要使用 Java 8 编写，我们可以使用 Maven 模板创建项目：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.flink \</span><br><span class="line">  -DarchetypeArtifactId=flink-quickstart-java \</span><br><span class="line">  -DarchetypeVersion=1.7.0</span><br></pre></td></tr></table></figure><p>将生成好的代码导入到 IDE 中，可以看到名为 <code>StreamingJob</code> 的文件，我们由此开始编写程序。</p><h3 id="Kafka-数据源"><a href="#Kafka-数据源" class="headerlink" title="Kafka 数据源"></a>Kafka 数据源</h3><p>Flink 对 Kafka 数据源提供了 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/kafka.html" target="_blank" rel="noopener">原生支持</a>，我们需要选择正确的 Kafka 依赖版本，将其添加到 POM 文件中：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>测试过程中，我们需要一个能够运行的 Kafka 服务，读者可以参照 <a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">官方文档</a> 搭建本地服务。在 Flink 中初始化 Kafka 数据源时，传入服务器名和主题名就可以了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(</span><br><span class="line">    <span class="string">"flink_test"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br></pre></td></tr></table></figure><p>Flink 会连接本地的 Kafka 服务，读取 <code>flink_test</code> 主题中的数据，转换成字符串后返回。除了 <code>SimpleStringSchema</code>，Flink 还提供了其他内置的反序列化方式，如 JSON、Avro 等，我们也可以编写自定义逻辑。</p><h3 id="流式文件存储"><a href="#流式文件存储" class="headerlink" title="流式文件存储"></a>流式文件存储</h3><p><code>StreamingFileSink</code> 替代了先前的 <code>BucketingSink</code>，用来将上游数据存储到 HDFS 的不同目录中。它的核心逻辑是分桶，默认的分桶方式是 <code>DateTimeBucketAssigner</code>，即按照处理时间分桶。处理时间指的是消息到达 Flink 程序的时间，这点并不符合我们的需求。因此，我们需要自己编写代码将事件时间从消息体中解析出来，按规则生成分桶的名称：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">    JsonNode node = mapper.readTree(element);</span><br><span class="line">    <span class="keyword">long</span> date = (<span class="keyword">long</span>) (node.path(<span class="string">"timestamp"</span>).floatValue() * <span class="number">1000</span>);</span><br><span class="line">    String partitionValue = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(<span class="keyword">new</span> Date(date));</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码会使用 Jackson 库对消息体进行解析，将时间戳转换成日期字符串，添加前缀后返回。如此一来，<code>StreamingFileSink</code> 就能知道应该将当前记录放置到哪个目录中了。完整代码可以参考 GitHub（<a href="https://github.com/jizhang/flink-sandbox/blob/blog-etl/src/main/java/com/shzhangji/flinksandbox/kafka/EventTimeBucketAssigner.java" target="_blank" rel="noopener">链接</a>）。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">    .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"/tmp/kafka-loader"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">    .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">    .build();</span><br><span class="line">stream.addSink(sink);</span><br></pre></td></tr></table></figure><p><code>forRowFormat</code> 表示输出的文件是按行存储的，对应的有 <code>forBulkFormat</code>，可以将输出结果用 Parquet 等格式进行压缩存储。</p><p>关于 <code>StreamingFileSink</code> 还有一点要注意，它只支持 Hadoop 2.7 以上的版本，因为需要用到高版本文件系统提供的 <code>truncate</code> 方法来实现故障恢复，这点下文会详述。</p><h3 id="开启检查点"><a href="#开启检查点" class="headerlink" title="开启检查点"></a>开启检查点</h3><p>代码编写到这里，其实已经可以通过 <code>env.execute()</code> 来运行了。但是，它只能保证 At-least-once 语义，即消息有可能会被重复处理。要做到 Exactly-once，我们还需要开启 Flink 的检查点功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(<span class="number">60_000</span>);</span><br><span class="line">env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"/tmp/flink/checkpoints"</span>));</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">    ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br></pre></td></tr></table></figure><p>检查点（Checkpoint）是 Flink 的故障恢复机制，同样会在下文详述。代码中，我们将状态存储方式由 <code>MemoryStateBackend</code> 修改为了 <code>FsStateBackend</code>，即使用外部文件系统，如 HDFS，来保存应用程序的中间状态，这样当 Flink JobManager 宕机时，也可以恢复过来。Flink 还支持 <code>RocksDBStateBackend</code>，用来存放较大的中间状态，并能支持增量的状态更新。</p><h3 id="提交与管理脚本"><a href="#提交与管理脚本" class="headerlink" title="提交与管理脚本"></a>提交与管理脚本</h3><p>Flink 程序可以直接在 IDE 中调试。我们也可以搭建一个本地的 Flink 集群，并通过 Flink CLI 命令行工具来提交脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure><p>脚本的运行状态可以在 Flink 仪表盘中查看：</p><p><img src="/cnblogs/images/flink/dashboard.png" alt="Flink Dashboard"></p><h4 id="使用暂存点来停止和恢复脚本"><a href="#使用暂存点来停止和恢复脚本" class="headerlink" title="使用暂存点来停止和恢复脚本"></a>使用暂存点来停止和恢复脚本</h4><p>当需要暂停脚本、或对程序逻辑进行修改时，我们需要用到 Flink 的暂存点机制（Savepoint）。暂存点和检查点类似，同样保存的是 Flink 各个算子的状态数据（Operator State）。不同的是，暂存点主要用于人为的脚本更替，而检查点则主要由 Flink 控制，用来实现故障恢复。<code>flink cancel -s</code> 命令可以在停止脚本的同时创建一个暂存点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink cancel -s /tmp/flink/savepoints 1253cc85e5c702dbe963dd7d8d279038</span><br><span class="line">Cancelled job 1253cc85e5c702dbe963dd7d8d279038. Savepoint stored in file:/tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee.</span><br></pre></td></tr></table></figure><p>具体到我们的 ETL 示例程序，暂存点中保存了当前 Kafka 队列的消费位置、正在写入的文件名等。当需要从暂存点恢复执行时，可以使用 <code>flink run -s</code> 传入目录位置。Flink 会从指定偏移量读取消息队列，并处理好中间结果文件，确保没有缺失或重复的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -s /tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure><h4 id="在-YARN-上运行"><a href="#在-YARN-上运行" class="headerlink" title="在 YARN 上运行"></a>在 YARN 上运行</h4><p>要将脚本提交到 YARN 集群上运行，同样是使用 <code>flink run</code> 命令。首先将代码中指定文件目录的部分添加上 HDFS 前缀，如 <code>hdfs://localhost:9000/</code>，重新打包后执行下列命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export HADOOP_CONF_DIR=/path/to/hadoop/conf</span><br><span class="line">$ bin/flink run -m yarn-cluster -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br><span class="line">Submitted application application_1545534487726_0001</span><br></pre></td></tr></table></figure><p>Flink 仪表盘会在 YARN Application Master 中运行，我们可以通过 ResourceManager 界面进入。返回的应用 ID 可以用来管理脚本，添加 <code>-yid</code> 参数即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s hdfs://localhost:9000/tmp/flink/savepoints -yid application_1545534487726_0001 84de00a5e193f26c937f72a9dc97f386</span><br></pre></td></tr></table></figure><h2 id="Flink-如何保证-Exactly-once-语义"><a href="#Flink-如何保证-Exactly-once-语义" class="headerlink" title="Flink 如何保证 Exactly-once 语义"></a>Flink 如何保证 Exactly-once 语义</h2><p>Flink 实时处理程序可以分为三个部分，数据源、处理流程、以及输出。不同的数据源和输出提供了不同的语义保证，Flink 统称为 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/guarantees.html" target="_blank" rel="noopener">连接器</a>。处理流程则能提供 Exactly-once 或 At-least-once 语义，需要看检查点是否开启。</p><h3 id="实时处理与检查点"><a href="#实时处理与检查点" class="headerlink" title="实时处理与检查点"></a>实时处理与检查点</h3><p>Flink 的检查点机制是基于 Chandy-Lamport 算法的：Flink 会定时在数据流中安插轻量的标记信息（Barrier），将消息流切割成一组组记录；当某个算子处理完一组记录后，就将当前状态保存为一个检查点，提交给 JobManager，该组的标记信息也会传递给下游；当末端的算子（通常是 Sink）处理完这组记录并提交检查点后，这个检查点将被标记为“已完成”；当脚本出现问题时，就会从最后一个“已完成”的检查点开始重放记录。</p><p><img src="/cnblogs/images/flink/stream-barrier.png" alt="Stream Barrier"></p><p>如果算子有多个上游，Flink 会使用一种称为“消息对齐”的机制：如果某个上游出现延迟，当前算子会停止从其它上游消费消息，直到延迟的上游赶上进度，这样就保证了算子中的状态不会包含下一批次的记录。显然，这种方式会引入额外的延迟，因此除了这种 <code>EXACTLY_ONCE</code> 模式，我们也可将检查点配置为 <code>AT_LEAST_ONCE</code>，以获得更高的吞吐量。具体方式请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html" target="_blank" rel="noopener">官方文档</a>。</p><h3 id="可重放的数据源"><a href="#可重放的数据源" class="headerlink" title="可重放的数据源"></a>可重放的数据源</h3><p>当出错的脚本需要从上一个检查点恢复时，Flink 必须对数据进行重放，这就要求数据源支持这一功能。Kafka 是目前使用得较多的消息队列，且支持从特定位点进行消费。具体来说，<code>FlinkKafkaConsumer</code> 类实现了 <code>CheckpointedFunction</code> 接口，会在检查点中存放主题名、分区名、以及偏移量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span> <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> </span>&#123;</span><br><span class="line">    OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        OFFSETS_STATE_NAME,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">        restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> </span>&#123;</span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">      unionOffsetStates.add(Tuple2.of(kafkaTopicPartitionLongEntry.getKey(),</span><br><span class="line">          kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当数据源算子从检查点或暂存点恢复时，我们可以在 TaskManager 的日志中看到以下信息，表明当前消费的偏移量是从算子状态中恢复出来的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2018-12-23 10:56:47,380 INFO FlinkKafkaConsumerBase</span><br><span class="line">  Consumer subtask 0 will start reading 2 partitions with offsets in restored state:</span><br><span class="line">    &#123;KafkaTopicPartition&#123;topic=&apos;flink_test&apos;, partition=1&#125;=725,</span><br><span class="line">     KafkaTopicPartition&#123;topic=&apos;flink_test&apos;, partition=0&#125;=721&#125;</span><br></pre></td></tr></table></figure><h3 id="恢复写入中的文件"><a href="#恢复写入中的文件" class="headerlink" title="恢复写入中的文件"></a>恢复写入中的文件</h3><p>程序运行过程中，<code>StreamingFileSink</code> 首先会将结果写入中间文件，以 <code>.</code> 开头、<code>in-progress</code> 结尾。这些中间文件会在符合一定条件后更名为正式文件，取决于用户配置的 <code>RollingPolicy</code>，默认策略是基于时间（60 秒）和基于大小（128 MB）。当脚本出错或重启时，中间文件会被直接关闭；在恢复时，由于检查点中保存了中间文件名和成功写入的长度，程序会重新打开这些文件，切割到指定长度（Truncate），然后继续写入。这样一来，文件中就不会包含检查点之后的记录了，从而实现 Exactly-once。</p><p>以 Hadoop 文件系统举例，恢复的过程是在 <code>HadoopRecoverableFsDataOutputStream</code> 类的构造函数中进行的。它会接收一个 <code>HadoopFsRecoverable</code> 类型的结构，里面包含了中间文件的路径和长度。这个对象是 <code>BucketState</code> 的成员，会被保存在检查点中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HadoopRecoverableFsDataOutputStream(FileSystem fs, HadoopFsRecoverable recoverable) &#123;</span><br><span class="line">  <span class="keyword">this</span>.tempFile = checkNotNull(recoverable.tempFile());</span><br><span class="line">  truncate(fs, tempFile, recoverable.offset());</span><br><span class="line">  out = fs.append(tempFile);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Apache Flink 构建在实时处理之上，从设计之初就充分考虑了中间状态的保存，而且能够很好地与现有 Hadoop 生态环境结合，因而在大数据领域非常有竞争力。它还在高速发展之中，近期也引入了 Table API、流式 SQL、机器学习等功能，像阿里巴巴这样的公司也在大量使用和贡献代码。Flink 的应用场景众多，有很大的发展潜力，值得一试。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Flink 是大数据领域又一新兴框架。它与 Spark 的不同之处在于，它是使用流式处理来模拟批量处理的，因此能够提供亚秒级的、符合 Exactly-once 语义的实时处理能力。Flink 的使用场景之一是构建实时的数据通道，在不同的存储之间搬运和转换数据。本文将介绍如何使用 Flink 开发实时 ETL 程序，并介绍 Flink 是如何保证其 Exactly-once 语义的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/flink/arch.png&quot; alt=&quot;Apache Flink&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;示例程序&quot;&gt;&lt;a href=&quot;#示例程序&quot; class=&quot;headerlink&quot; title=&quot;示例程序&quot;&gt;&lt;/a&gt;示例程序&lt;/h2&gt;&lt;p&gt;让我们来编写一个从 Kafka 抽取数据到 HDFS 的程序。数据源是一组事件日志，其中包含了事件发生的时间，以时间戳的方式存储。我们需要将这些日志按事件时间分别存放到不同的目录中，即按日分桶。时间日志示例如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184226.432,&amp;quot;event&amp;quot;:&amp;quot;page_view&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;ac0e50bf-944c-4e2f-bbf5-a34b22718e0c&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184602.640,&amp;quot;event&amp;quot;:&amp;quot;adv_click&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;9b220808-2193-44d1-a0e9-09b9743dec55&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184608.969,&amp;quot;event&amp;quot;:&amp;quot;thumbs_up&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;b44c3137-4c91-4f36-96fb-80f56561c914&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;产生的目录结构为：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/user/flink/event_log/dt=20181219/part-0-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/flink/event_log/dt=20181220/part-1-9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/cnblogs/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="kafka" scheme="http://shzhangji.com/cnblogs/tags/kafka/"/>
    
      <category term="hdfs" scheme="http://shzhangji.com/cnblogs/tags/hdfs/"/>
    
      <category term="flink" scheme="http://shzhangji.com/cnblogs/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark DataSource API V2</title>
    <link href="http://shzhangji.com/cnblogs/2018/12/09/spark-datasource-api-v2/"/>
    <id>http://shzhangji.com/cnblogs/2018/12/09/spark-datasource-api-v2/</id>
    <published>2018-12-09T03:10:59.000Z</published>
    <updated>2020-08-22T12:06:11.269Z</updated>
    
    <content type="html"><![CDATA[<p>Spark 1.3 引入了第一版的数据源 API，我们可以使用它将常见的数据格式整合到 Spark SQL 中。但是，随着 Spark 的不断发展，这一 API 也体现出了其局限性，故而 Spark 团队不得不加入越来越多的专有代码来编写数据源，以获得更好的性能。Spark 2.3 中，新一版的数据源 API 初见雏形，它克服了上一版 API 的种种问题，原来的数据源代码也在逐步重写。本文将演示这两版 API 的使用方法，比较它们的不同之处，以及新版 API 的优势在哪里。</p><h2 id="DataSource-V1-API"><a href="#DataSource-V1-API" class="headerlink" title="DataSource V1 API"></a>DataSource V1 API</h2><p>V1 API 由一系列的抽象类和接口组成，它们位于 <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala" target="_blank" rel="noopener">spark/sql/sources/interfaces.scala</a> 文件中。主要的内容有：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过实现 <code>RelationProvider</code> 接口，表明该类是一种新定义的数据源，可以供 Spark SQL 取数所用。传入 <code>createRelation</code> 方法的参数可以用来做初始化，如文件路径、权限信息等。<code>BaseRelation</code> 抽象类则用来定义数据源的表结构，它的来源可以是数据库、Parquet 文件等外部系统，也可以直接由用户指定。该类还必须实现某个 <code>Scan</code> 接口，Spark 会调用 <code>buildScan</code> 方法来获取数据源的 RDD，我们将在下文看到。</p><a id="more"></a><h3 id="JdbcSourceV1"><a href="#JdbcSourceV1" class="headerlink" title="JdbcSourceV1"></a>JdbcSourceV1</h3><p>下面我们来使用 V1 API 实现一个通过 JDBC 读取数据库的自定义数据源。为简单起见，表结构信息是直接写死在代码里的，我们先从整表扫描开始。完整的代码可以在 GitHub（<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV1.scala" target="_blank" rel="noopener">链接</a>）中找到，数据源表则可以在这个 <a href="https://github.com/jizhang/spark-sandbox/blob/master/data/employee.sql" target="_blank" rel="noopener">链接</a> 中查看。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcSourceV1</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRelationV1</span>(parameters(<span class="string">"url"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(url)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(): <span class="type">Iterator</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">    <span class="keyword">val</span> rs = stmt.executeQuery()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Row</span>] &#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = rs.next()</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">Row</span> = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>JdbcRDD#compute</code> 负责实际的读取操作，它从上游获取到数据库连接信息、选取的字段、以及过滤条件，拼装 SQL 后执行，并返回一个 <code>Row</code> 类型的迭代器对象，每一行数据的结构和请求的字段列表相符。定义好数据源后，我们就可以用 <code>DataFrame</code> 对象来直接操作了：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read</span><br><span class="line">  .format(<span class="string">"JdbcSourceV2"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/spark"</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>上述代码输出的内容是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- emp_name: string (nullable = true)</span><br><span class="line"> |-- dep_name: string (nullable = true)</span><br><span class="line"> |-- salary: decimal(7,2) (nullable = true)</span><br><span class="line"> |-- age: decimal(3,0) (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">| id|emp_name|  dep_name| salary|age|</span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">|  1| Matthew|Management|4500.00| 55|</span><br><span class="line">|  2|  Olivia|Management|4400.00| 61|</span><br><span class="line">|  3|   Grace|Management|4000.00| 42|</span><br><span class="line">|  4|     Jim|Production|3700.00| 35|</span><br><span class="line">|  5|   Alice|Production|3500.00| 24|</span><br><span class="line">+---+--------+----------+-------+---+</span><br></pre></td></tr></table></figure><h3 id="V1-API-的局限性"><a href="#V1-API-的局限性" class="headerlink" title="V1 API 的局限性"></a>V1 API 的局限性</h3><p>我们可以看到，V1 API 使用起来非常方便，因此能够满足 Spark SQL 初期的需求，但也不免存在很多局限性：</p><h4 id="依赖上层-API"><a href="#依赖上层-API" class="headerlink" title="依赖上层 API"></a>依赖上层 API</h4><p><code>createRelation</code> 接收 <code>SQLContext</code> 作为参数；<code>buildScan</code> 方法返回的是 <code>RDD[Row]</code> 类型；而在实现写操作时，<code>insert</code> 方法会直接接收 <code>DataFrame</code> 类型的参数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InsertableRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这些类型都属于较为上层的 Spark API，其中某些类已经发生了变化，如 <code>SQLContext</code> 已被 <code>SparkSession</code> 取代，而 <code>DataFrame</code> 也改为了 <code>Dataset[Row]</code> 类型的一个别称。这些改变不应该体现到底层的数据源 API 中。</p><h4 id="难以添加新的优化算子"><a href="#难以添加新的优化算子" class="headerlink" title="难以添加新的优化算子"></a>难以添加新的优化算子</h4><p>除了上文中的 <code>TableScan</code> 接口，V1 API 还提供了 <code>PrunedScan</code> 接口，用来裁剪不需要的字段；<code>PrunedFilteredScan</code> 接口则可以将过滤条件下推到数据源中。在 <code>JdbcSourceV1</code> 示例中，这类下推优化会体现在 SQL 语句里：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span> <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">PrunedFilteredScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRDD</span>(requiredColumns, filters)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">columns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]</span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> wheres = filters.flatMap &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">EqualTo</span>(attribute, value) =&gt; <span class="type">Some</span>(<span class="string">s"<span class="subst">$attribute</span> = '<span class="subst">$value</span>'"</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">s"SELECT <span class="subst">$&#123;columns.mkString(", ")&#125;</span> FROM employee WHERE <span class="subst">$&#123;wheres.mkString(" AND ")&#125;</span>"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果我们想添加新的优化算子（如 LIMIT 子句），就不免要引入一系列的 <code>Scan</code> 接口组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">LimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedFilteredLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="难以传递分区信息"><a href="#难以传递分区信息" class="headerlink" title="难以传递分区信息"></a>难以传递分区信息</h4><p>对于支持数据分区的数据源，如 HDFS、Kafka 等，V1 API 没有提供原生的支持，因而也不能利用数据局部性（Data Locality）。我们需要自己继承 RDD 来实现，比如下面的代码就对 Kafka 数据源进行了分区，并告知 Spark 可以将数据读取操作放入 Kafka Broker 所在的服务器上执行，以提升效率：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaPartition</span>(<span class="params">partitionId: <span class="type">Int</span>, leaderHost: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Partition</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>: <span class="type">Int</span> = partitionId</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRDD</span>(<span class="params">sc: <span class="type">SparkContext</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>](<span class="params">sc, <span class="type">Nil</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="type">Array</span>(</span><br><span class="line">    <span class="comment">// populate with Kafka PartitionInfo</span></span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">0</span>, <span class="string">"broker_0"</span>),</span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">1</span>, <span class="string">"broker_1"</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Seq</span>(</span><br><span class="line">    split.asInstanceOf[<span class="type">KafkaPartition</span>].leaderHost</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此外，类似 Cassandra 这样的数据库，会按主键对数据进行分片。那么，如果查询语句中包含了按该主键进行分组的子句，Spark 就可以省去一次 Shuffle 操作。这在 V1 API 中也是不支持的，而 V2 API 则提供了 <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java" target="_blank" rel="noopener"><code>SupportsReportPartitioning</code></a> 接口来支持。</p><h4 id="缺少事务性的写操作"><a href="#缺少事务性的写操作" class="headerlink" title="缺少事务性的写操作"></a>缺少事务性的写操作</h4><p>Spark 任务是有可能失败的，使用 V1 API 时就会留下部分写入的数据。当然，对于 HDFS 这样的文件系统来说问题不大，因为可以用 <code>_SUCCESS</code> 来标记该次写操作是否执行成功。但这一逻辑也需要最终用户来实现，而 V2 API 则提供了明确的接口来支持事务性的写操作。</p><h4 id="缺少列存储和流式计算支持"><a href="#缺少列存储和流式计算支持" class="headerlink" title="缺少列存储和流式计算支持"></a>缺少列存储和流式计算支持</h4><p>Spark SQL 目前已支持列存储和流式计算，但两者都不是用 V1 API 实现的。<code>ParquetFileFormat</code> 和 <code>KafkaSource</code> 等类型都使用了专有代码和内部 API。这些特性也在 V2 API 中得到支持。</p><h2 id="DataSource-V2-API"><a href="#DataSource-V2-API" class="headerlink" title="DataSource V2 API"></a>DataSource V2 API</h2><p>V2 API 首先使用了一个标记性的 <code>DataSourceV2</code> 接口，实现了该接口的类还必须实现 <code>ReadSupport</code> 或 <code>WriteSupport</code>，用来表示自身支持读或写操作。<code>ReadSupport</code> 接口中的方法会被用来创建 <code>DataSourceReader</code> 类，同时接收到初始化参数；该类继而创建 <code>DataReaderFactory</code> 和 <code>DataReader</code> 类，后者负责真正的读操作，接口中定义的方法和迭代器类似。此外，<code>DataSourceReader</code> 还可以实现各类 <code>Support</code> 接口，表明自己支持某些优化下推操作，如裁剪字段、过滤条件等。<code>WriteSupport</code> API 的层级结构与之类似。这些接口都是用 Java 语言编写，以获得更好的交互支持。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceV2</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ReadSupport</span> <span class="keyword">extends</span> <span class="title">DataSourceV2</span> </span>&#123;</span><br><span class="line">  <span class="function">DataSourceReader <span class="title">createReader</span><span class="params">(DataSourceOptions options)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function">StructType <span class="title">readSchema</span><span class="params">()</span></span>;</span><br><span class="line">  List&lt;DataReaderFactory&lt;Row&gt;&gt; createDataReaderFactories();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SupportsPushDownRequiredColumns</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">pruneColumns</span><span class="params">(StructType requiredSchema)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReaderFactory</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function">DataReader&lt;T&gt; <span class="title">createDataReader</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReader</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">boolean</span> <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function">T <span class="title">get</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可能你会注意到，<code>DataSourceReader#createDataReaderFactories</code> 仍然捆绑了 <code>Row</code> 类型，这是因为目前 V2 API 只支持 <code>Row</code> 类型的返回值，且这套 API 仍处于进化状态（Evolving）。</p><h3 id="JdbcSourceV2"><a href="#JdbcSourceV2" class="headerlink" title="JdbcSourceV2"></a>JdbcSourceV2</h3><p>让我们使用 V2 API 来重写 JDBC 数据源。下面是一个整表扫描的示例，完整代码可以在 GitHub（<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV2.scala" target="_blank" rel="noopener">链接</a>）上查看。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span> </span>= <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(url)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataReader</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rs: <span class="type">ResultSet</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">if</span> (rs == <span class="literal">null</span>) &#123;</span><br><span class="line">      conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">      <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">      rs = stmt.executeQuery()</span><br><span class="line">    &#125;</span><br><span class="line">    rs.next()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>() = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="裁剪字段"><a href="#裁剪字段" class="headerlink" title="裁剪字段"></a>裁剪字段</h4><p>通过实现 <code>SupportsPushDownRequiredColumns</code> 接口，Spark 会调用其 <code>pruneColumns</code> 方法，传入用户所指定的字段列表（<code>StructType</code>），<code>DataSourceReader</code> 可以将该信息传给 <code>DataReader</code> 使用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownRequiredColumns</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> requiredSchema = <span class="type">JdbcSourceV2</span>.schema</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pruneColumns</span></span>(requiredSchema: <span class="type">StructType</span>)  = &#123;</span><br><span class="line">    <span class="keyword">this</span>.requiredSchema = requiredSchema</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = requiredSchema.fields.map(_.name)</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(columns)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以用 <code>df.explain(true)</code> 来验证执行计划。例如，<code>SELECT emp_name, age FROM employee</code> 语句的执行计划在优化前后是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">emp_name: string, age: decimal(3,0)</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- SubqueryAlias employee</span><br><span class="line">   +- DataSourceV2Relation [id#0, emp_name#1, dep_name#2, salary#3, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- DataSourceV2Relation [emp_name#1, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br></pre></td></tr></table></figure><p>可以看到，字段裁剪被反映到了数据源中。如果我们将实际执行的 SQL 语句打印出来，也能看到字段裁剪下推的结果。</p><h4 id="过滤条件"><a href="#过滤条件" class="headerlink" title="过滤条件"></a>过滤条件</h4><p>类似的，实现 <code>SupportsPushDownFilters</code> 接口可以将过滤条件下推到数据源中：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownFilters</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> filters = <span class="type">Array</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">  <span class="keyword">var</span> wheres = <span class="type">Array</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> supported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> unsupported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> wheres = <span class="type">ListBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    filters.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> filter: <span class="type">EqualTo</span> =&gt; &#123;</span><br><span class="line">        supported += filter</span><br><span class="line">        wheres += <span class="string">s"<span class="subst">$&#123;filter.attribute&#125;</span> = '<span class="subst">$&#123;filter.value&#125;</span>'"</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> filter =&gt; unsupported += filter</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.filters = supported.toArray</span><br><span class="line">    <span class="keyword">this</span>.wheres = wheres.toArray</span><br><span class="line">    unsupported.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushedFilters</span> </span>= filters</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(wheres)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="多分区支持"><a href="#多分区支持" class="headerlink" title="多分区支持"></a>多分区支持</h4><p><code>createDataReaderFactories</code> 返回的是列表类型，每个读取器都会产生一个 RDD 分区。如果我们想开启多个读取任务，就可以生成多个读取器工厂，并为每个读取器限定主键范围：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">  <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">11</span>)).map &#123; <span class="keyword">case</span> (minId, maxId) =&gt;</span><br><span class="line">    <span class="keyword">val</span> partition = <span class="string">s"id BETWEEN <span class="subst">$minId</span> AND <span class="subst">$maxId</span>"</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(partition)</span><br><span class="line">  &#125;.asJava</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="事务性的写操作"><a href="#事务性的写操作" class="headerlink" title="事务性的写操作"></a>事务性的写操作</h3><p>V2 API 提供了两组 <code>commit</code> / <code>abort</code> 方法，用来实现事务性的写操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceWriter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataWriter</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">write</span><span class="params">(T record)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function">WriterCommitMessage <span class="title">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>DataSourceWriter</code> 在 Spark Driver 中执行，<code>DataWriter</code> 则运行在其他节点的 Spark Executor 上。当 <code>DataWriter</code> 成功执行了写操作，就会将提交信息传递给 Driver；当 <code>DataSourceWriter</code> 收集到了所有写任务的提交信息，就会执行最终的提交操作。如果某个写任务失败了，它的 <code>abort</code> 方法会得到执行；如果经过多轮重试后仍然失败，则所有写任务的 <code>abort</code> 方法都会被调用，进行数据清理操作。</p><h3 id="列存储与流式计算支持"><a href="#列存储与流式计算支持" class="headerlink" title="列存储与流式计算支持"></a>列存储与流式计算支持</h3><p>这两个特性仍处于实验性阶段，在 Spark 中还没有得到使用。简单来说，<code>DataSourceReader</code> 类可以实现 <code>SupportsScanColumnarBatch</code> 接口来声明自己会返回 <code>ColumnarBatch</code> 对象，这个对象是 Spark 内部用来存放列式数据的。对于流式数据，则有 <code>MicroBatchReader</code> 和 <code>ContinuousReader</code> 这两个接口，感兴趣的读者可以到 Spark <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2Suite.scala" target="_blank" rel="noopener">单元测试</a> 代码中查看。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/" target="_blank" rel="noopener">http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/</a></li><li><a href="https://databricks.com/session/apache-spark-data-source-v2" target="_blank" rel="noopener">https://databricks.com/session/apache-spark-data-source-v2</a></li><li><a href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a></li><li><a href="https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/" target="_blank" rel="noopener">https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark 1.3 引入了第一版的数据源 API，我们可以使用它将常见的数据格式整合到 Spark SQL 中。但是，随着 Spark 的不断发展，这一 API 也体现出了其局限性，故而 Spark 团队不得不加入越来越多的专有代码来编写数据源，以获得更好的性能。Spark 2.3 中，新一版的数据源 API 初见雏形，它克服了上一版 API 的种种问题，原来的数据源代码也在逐步重写。本文将演示这两版 API 的使用方法，比较它们的不同之处，以及新版 API 的优势在哪里。&lt;/p&gt;
&lt;h2 id=&quot;DataSource-V1-API&quot;&gt;&lt;a href=&quot;#DataSource-V1-API&quot; class=&quot;headerlink&quot; title=&quot;DataSource V1 API&quot;&gt;&lt;/a&gt;DataSource V1 API&lt;/h2&gt;&lt;p&gt;V1 API 由一系列的抽象类和接口组成，它们位于 &lt;a href=&quot;https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;spark/sql/sources/interfaces.scala&lt;/a&gt; 文件中。主要的内容有：&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;trait&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;RelationProvider&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;createRelation&lt;/span&gt;&lt;/span&gt;(sqlContext: &lt;span class=&quot;type&quot;&gt;SQLContext&lt;/span&gt;, parameters: &lt;span class=&quot;type&quot;&gt;Map&lt;/span&gt;[&lt;span class=&quot;type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;type&quot;&gt;String&lt;/span&gt;]): &lt;span class=&quot;type&quot;&gt;BaseRelation&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BaseRelation&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;sqlContext&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;type&quot;&gt;SQLContext&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;schema&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;type&quot;&gt;StructType&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;trait&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TableScan&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;buildScan&lt;/span&gt;&lt;/span&gt;(): &lt;span class=&quot;type&quot;&gt;RDD&lt;/span&gt;[&lt;span class=&quot;type&quot;&gt;Row&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过实现 &lt;code&gt;RelationProvider&lt;/code&gt; 接口，表明该类是一种新定义的数据源，可以供 Spark SQL 取数所用。传入 &lt;code&gt;createRelation&lt;/code&gt; 方法的参数可以用来做初始化，如文件路径、权限信息等。&lt;code&gt;BaseRelation&lt;/code&gt; 抽象类则用来定义数据源的表结构，它的来源可以是数据库、Parquet 文件等外部系统，也可以直接由用户指定。该类还必须实现某个 &lt;code&gt;Scan&lt;/code&gt; 接口，Spark 会调用 &lt;code&gt;buildScan&lt;/code&gt; 方法来获取数据源的 RDD，我们将在下文看到。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/cnblogs/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Flume 源码解析：HDFS Sink</title>
    <link href="http://shzhangji.com/cnblogs/2018/10/04/flume-source-code-hdfs-sink/"/>
    <id>http://shzhangji.com/cnblogs/2018/10/04/flume-source-code-hdfs-sink/</id>
    <published>2018-10-04T05:49:03.000Z</published>
    <updated>2020-08-22T12:06:11.269Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flume 数据流程的最后一部分是 Sink，它会将上游抽取并转换好的数据输送到外部存储中去，如本地文件、HDFS、ElasticSearch 等。本文将通过分析源码来展现 HDFS Sink 的工作流程。</p><h2 id="Sink-组件的生命周期"><a href="#Sink-组件的生命周期" class="headerlink" title="Sink 组件的生命周期"></a>Sink 组件的生命周期</h2><p><a href="http://shzhangji.com/cnblogs/2017/10/24/flume-source-code-component-lifecycle/">在上一篇文章中</a>, 我们了解到 Flume 组件都会实现 <code>LifecycleAware</code> 接口，并由 <code>LifecycleSupervisor</code> 实例管理和监控。不过，Sink 组件并不直接由它管理，而且被包装在了 <code>SinkRunner</code> 和 <code>SinkProcessor</code> 这两个类中。Flume 支持三种 <a href="https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors" target="_blank" rel="noopener">Sink 处理器</a>，该处理器会将 Channel 和 Sink 以不同的方式连接起来。这里我们只讨论 <code>DefaultSinkProcessor</code> 的情况，即一个 Channel 只会连接一个 Sink。同时，我们也将略过对 Sink 分组的讨论。</p><p><img src="/cnblogs/images/flume/sink-component-lifecycle.png" alt="Sink Component LifeCycle"></p><a id="more"></a><h2 id="HDFS-Sink-模块中的类"><a href="#HDFS-Sink-模块中的类" class="headerlink" title="HDFS Sink 模块中的类"></a>HDFS Sink 模块中的类</h2><p>HDFS Sink 模块的源码在 <code>flume-hdfs-sink</code> 子目录中，主要由以下几个类组成：</p><p><img src="/cnblogs/images/flume/hdfs-sink-classes.png" alt="HDFS Sink Classes"></p><p><code>HDFSEventSink</code> 类实现了生命周期的各个方法，包括 <code>configure</code>、<code>start</code>、<code>process</code>、<code>stop</code> 等。它启动后会维护一组 <code>BucketWriter</code> 实例，每个实例对应一个 HDFS 输出文件路径，上游的消息会传递给它，并写入 HDFS。通过不同的 <code>HDFSWriter</code> 实现，它可以将数据写入文本文件、压缩文件、或是 <code>SequenceFile</code>。</p><h2 id="配置与启动"><a href="#配置与启动" class="headerlink" title="配置与启动"></a>配置与启动</h2><p>Flume 配置文件加载时，会实例化各个组件，并调用它们的 <code>configure</code> 方法，其中就包括 Sink 组件。在 <code>HDFSEventSink#configure</code> 方法中，程序会读取配置文件中以 <code>hdfs.</code> 为开头的项目，为其提供默认值，并做基本的参数校验。如，<code>batchSize</code> 必须大于零，<code>fileType</code> 指定为 <code>CompressedStream</code> 时 <code>codeC</code> 参数也必须指定等等。同时，程序还会初始化一个 <code>SinkCounter</code>，用于统计运行过程中的各项指标。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">  filePath = Preconditions.checkNotNull(</span><br><span class="line">      context.getString(<span class="string">"hdfs.path"</span>), <span class="string">"hdfs.path is required"</span>);</span><br><span class="line">  rollInterval = context.getLong(<span class="string">"hdfs.rollInterval"</span>, defaultRollInterval);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (sinkCounter == <span class="keyword">null</span>) &#123;</span><br><span class="line">    sinkCounter = <span class="keyword">new</span> SinkCounter(getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>HDFSEventSink#start</code> 方法中会创建两个线程池：<code>callTimeoutPool</code> 线程池会在 <code>BucketWriter#callWithTimeout</code> 方法中使用，用来限定 HDFS 远程调用的请求时间，如 <a href="http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FileSystem.html" target="_blank" rel="noopener"><code>FileSystem#create</code></a> 或 <a href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FSDataOutputStream.html" target="_blank" rel="noopener"><code>FSDataOutputStream#hflush</code></a> 都有可能超时；<code>timedRollerPool</code> 则用于对文件进行滚动，前提是用户配置了 <code>rollInterval</code> 选项，我们将在下一节详细说明。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  callTimeoutPool = Executors.newFixedThreadPool(threadsPoolSize,</span><br><span class="line">      <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(timeoutName).build());</span><br><span class="line">  timedRollerPool = Executors.newScheduledThreadPool(rollTimerPoolSize,</span><br><span class="line">      <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(rollerName).build());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h2><p><code>process</code> 方法包含了 HDFS Sink 的主要逻辑，也就是从上游的 Channel 中获取数据，并写入指定的 HDFS 文件，流程图如下：</p><p><img src="/cnblogs/images/flume/process-method-flow-chart.png" alt="Process Method Flow Chart"></p><h3 id="Channel-事务"><a href="#Channel-事务" class="headerlink" title="Channel 事务"></a>Channel 事务</h3><p>处理逻辑的外层是一个 Channel 事务，并提供了异常处理。以 Kafka Channel 为例：事务开始时，程序会从 Kafka 中读取数据，但不会立刻提交变动后的偏移量。只有当这些消息被成功写入 HDFS 文件之后，偏移量才会提交给 Kafka，下次循环将从新的偏移量开始消费。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Channel channel = getChannel();</span><br><span class="line">Transaction transaction = channel.getTransaction();</span><br><span class="line">transaction.begin()</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  event = channel.take();</span><br><span class="line">  bucketWriter.append(event);</span><br><span class="line">  transaction.commit()</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable th) &#123;</span><br><span class="line">  transaction.rollback();</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  transaction.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查找或创建-BucketWriter"><a href="#查找或创建-BucketWriter" class="headerlink" title="查找或创建 BucketWriter"></a>查找或创建 <code>BucketWriter</code></h3><p><code>BucketWriter</code> 实例和 HDFS 文件一一对应，文件路径是通过配置生成的，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.access_log.hdfs.path = /user/flume/access_log/dt=%Y%m%d</span><br><span class="line">a1.sinks.access_log.hdfs.filePrefix = events.%[localhost]</span><br><span class="line">a1.sinks.access_log.hdfs.inUsePrefix = .</span><br><span class="line">a1.sinks.access_log.hdfs.inUseSuffix = .tmp</span><br><span class="line">a1.sinks.access_log.hdfs.rollInterval = 300</span><br><span class="line">a1.sinks.access_log.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.access_log.hdfs.codeC = lzop</span><br></pre></td></tr></table></figure><p>以上配置生成的临时文件和目标文件路径为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flume/access_log/dt=20180925/.events.hostname1.1537848761307.lzo.tmp</span><br><span class="line">/user/flume/access_log/dt=20180925/events.hostname1.1537848761307.lzo</span><br></pre></td></tr></table></figure><p>配置中的占位符会由 <a href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/formatter/output/BucketPath.html" target="_blank" rel="noopener"><code>BucketPath#escapeString</code></a> 方法替换，Flume 支持三类占位符：</p><ul><li><code>%{...}</code>：使用消息中的头信息进行替换；</li><li><code>%[...]</code>：目前仅支持 <code>%[localhost]</code>、<code>%[ip]</code>、以及 <code>%[fqdn]</code>；</li><li><code>%x</code>：日期占位符，通过头信息中的 <code>timestamp</code> 来生成，或者使用 <code>useLocalTimeStamp</code> 配置项。</li></ul><p>文件的前后缀则是在 <code>BucketWriter#open</code> 方法中追加的。代码中的 <code>counter</code> 是当前文件的创建时间戳，<code>lzo</code> 则是当前压缩格式的默认文件后缀。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String fullFileName = fileName + <span class="string">"."</span> + counter;</span><br><span class="line">fullFileName += fileSuffix;</span><br><span class="line">fullFileName += codeC.getDefaultExtension();</span><br><span class="line">bucketPath = filePath + <span class="string">"/"</span> + inUsePrefix + fullFileName + inUseSuffix;</span><br><span class="line">targetPath = filePath + <span class="string">"/"</span> + fullFileName;</span><br></pre></td></tr></table></figure><p>如果指定路径没有对应的 <code>BucketWriter</code> 实例，程序会创建一个，并根据 <code>fileType</code> 配置项来生成对应的 <code>HDFSWriter</code> 实例。Flume 支持的三种类型是：<code>HDFSSequenceFile</code>、<code>HDFSDataStream</code>、以及 <code>HDFSCompressedDataStream</code>，写入 HDFS 的动作是由这些类中的代码完成的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bucketWriter = sfWriters.get(lookupPath);</span><br><span class="line"><span class="keyword">if</span> (bucketWriter == <span class="keyword">null</span>) &#123;</span><br><span class="line">  hdfsWriter = writerFactory.getWriter(fileType);</span><br><span class="line">  bucketWriter = <span class="keyword">new</span> BucketWriter(hdfsWriter);</span><br><span class="line">  sfWriters.put(lookupPath, bucketWriter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="写入数据并刷新"><a href="#写入数据并刷新" class="headerlink" title="写入数据并刷新"></a>写入数据并刷新</h3><p>在写入数据之前，<code>BucketWriter</code> 首先会检查文件是否已经打开，如未打开则会命关联的 <code>HDFSWriter</code> 类开启新的文件，以 <code>HDFSCompressedDataStream</code> 为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath, CompressionCodec codec)</span> </span>&#123;</span><br><span class="line">  FileSystem hdfs = dstPath.getFileSystem(conf);</span><br><span class="line">  fsOut = hdfs.append(dstPath)</span><br><span class="line">  compressor = CodedPool.getCompressor(codec, conf);</span><br><span class="line">  cmpOut = codec.createOutputStream(fsOut, compressor);</span><br><span class="line">  serializer = EventSerializerFactory.getInstance(serializerType, cmpOut);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IO Exception </span>&#123;</span><br><span class="line">  serializer.write(event);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flume 默认的 <code>serializerType</code> 配置是 <code>TEXT</code>，即使用 <a href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/serialization/BodyTextEventSerializer.html" target="_blank" rel="noopener">BodyTextEventSerializer</a> 来序列化数据，不做加工，直接写进输出流：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  out.write(e.getBody());</span><br><span class="line">  <span class="keyword">if</span> (appendNewline) &#123;</span><br><span class="line">    out.write(<span class="string">'\n'</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 <code>BucketWriter</code> 需要关闭或重开时会调用 <code>HDFSWriter#sync</code> 方法，进而执行序列化实例和输出流实例上的 <code>flush</code> 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  serializer.flush();</span><br><span class="line">  compOut.finish();</span><br><span class="line">  fsOut.flush();</span><br><span class="line">  hflushOrSync(fsOut);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 Hadoop 0.21.0 开始，<a href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/Syncable.html" target="_blank" rel="noopener"><code>Syncable#sync</code></a> 拆分成了 <code>hflush</code> 和 <code>hsync</code> 两个方法，前者只是将数据从客户端的缓存中刷新出去，后者则会保证数据已被写入 HDFS 本地磁盘。为了兼容新老 API，Flume 会通过 Java 反射机制来确定 <code>hflush</code> 是否存在，不存在则调用 <code>sync</code> 方法。上述代码中的 <code>flushOrSync</code> 正是做了这样的判断。</p><h3 id="文件滚动"><a href="#文件滚动" class="headerlink" title="文件滚动"></a>文件滚动</h3><p>HDFS Sink 支持三种滚动方式：按文件大小、按消息数量、以及按时间间隔。按大小和按数量的滚动是在 <code>BucketWriter#shouldRotate</code> 方法中判断的，每次 <code>append</code> 时都会调用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">shouldRotate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> doRotate = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) &#123;</span><br><span class="line">    doRotate = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) &#123;</span><br><span class="line">    doRotate = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> doRotate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按时间滚动则是使用了上文提到的 <code>timedRollerPool</code> 线程池，通过启动一个定时线程来实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (rollInterval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    Callable&lt;Void&gt; action = <span class="keyword">new</span> Callable&lt;Void&gt;() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Void <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        close(<span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    timedRollFuture = timedRollerPool.schedule(action, rollInterval);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="关闭与停止"><a href="#关闭与停止" class="headerlink" title="关闭与停止"></a>关闭与停止</h2><p>当 <code>HDFSEventSink#close</code> 被触发时，它会遍历所有的 <code>BucketWriter</code> 实例，调用它们的 <code>close</code> 方法，进而关闭下属的 <code>HDFSWriter</code>。这个过程和 <code>flush</code> 类似，只是还会做一些额外操作，如关闭后的 <code>BucketWriter</code> 会将自身从 <code>sfWriters</code> 哈希表中移除：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">boolean</span> callCloseCallback)</span> </span>&#123;</span><br><span class="line">  writer.close();</span><br><span class="line">  timedRollFuture.cancel(<span class="keyword">false</span>);</span><br><span class="line">  onCloseCallback.run(onCloseCallbackPath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>onCloseCallback</code> 回调函数是在 <code>HDFSEventSink</code> 初始化 <code>BucketWriter</code> 时传入的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">WriterCallback closeCallback = <span class="keyword">new</span> WriterCallback() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String bucketPath)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">synchronized</span> (sfWritersLock) &#123;</span><br><span class="line">        sfWriters.remove(bucketPath);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">bucketWriter = <span class="keyword">new</span> BucketWriter(lookPath, closeCallback);</span><br></pre></td></tr></table></figure><p>最后，<code>HDFSEventSink</code> 会关闭 <code>callTimeoutPool</code> 和 <code>timedRollerPool</code> 线程池，整个组件随即停止。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ExecutorService[] toShutdown = &#123; callTimeoutPool, timedRollerPool &#125;;</span><br><span class="line"><span class="keyword">for</span> (ExecutorService execService : toShutdown) &#123;</span><br><span class="line">  execService.shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://flume.apache.org/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html#hdfs-sink</a></li><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li><li><a href="https://data-flair.training/blogs/flume-sink-processors/" target="_blank" rel="noopener">https://data-flair.training/blogs/flume-sink-processors/</a></li><li><a href="http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html" target="_blank" rel="noopener">http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Flume 数据流程的最后一部分是 Sink，它会将上游抽取并转换好的数据输送到外部存储中去，如本地文件、HDFS、ElasticSearch 等。本文将通过分析源码来展现 HDFS Sink 的工作流程。&lt;/p&gt;
&lt;h2 id=&quot;Sink-组件的生命周期&quot;&gt;&lt;a href=&quot;#Sink-组件的生命周期&quot; class=&quot;headerlink&quot; title=&quot;Sink 组件的生命周期&quot;&gt;&lt;/a&gt;Sink 组件的生命周期&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://shzhangji.com/cnblogs/2017/10/24/flume-source-code-component-lifecycle/&quot;&gt;在上一篇文章中&lt;/a&gt;, 我们了解到 Flume 组件都会实现 &lt;code&gt;LifecycleAware&lt;/code&gt; 接口，并由 &lt;code&gt;LifecycleSupervisor&lt;/code&gt; 实例管理和监控。不过，Sink 组件并不直接由它管理，而且被包装在了 &lt;code&gt;SinkRunner&lt;/code&gt; 和 &lt;code&gt;SinkProcessor&lt;/code&gt; 这两个类中。Flume 支持三种 &lt;a href=&quot;https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sink 处理器&lt;/a&gt;，该处理器会将 Channel 和 Sink 以不同的方式连接起来。这里我们只讨论 &lt;code&gt;DefaultSinkProcessor&lt;/code&gt; 的情况，即一个 Channel 只会连接一个 Sink。同时，我们也将略过对 Sink 分组的讨论。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/flume/sink-component-lifecycle.png&quot; alt=&quot;Sink Component LifeCycle&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="flume" scheme="http://shzhangji.com/cnblogs/tags/flume/"/>
    
      <category term="hdfs" scheme="http://shzhangji.com/cnblogs/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Java 空指针异常的若干解决方案</title>
    <link href="http://shzhangji.com/cnblogs/2018/09/22/how-to-avoid-null-pointer-exception/"/>
    <id>http://shzhangji.com/cnblogs/2018/09/22/how-to-avoid-null-pointer-exception/</id>
    <published>2018-09-22T12:27:36.000Z</published>
    <updated>2020-08-22T12:06:11.269Z</updated>
    
    <content type="html"><![CDATA[<p>Java 中任何对象都有可能为空，当我们调用空对象的方法时就会抛出 <code>NullPointerException</code> 空指针异常，这是一种非常常见的错误类型。我们可以使用若干种方法来避免产生这类异常，使得我们的代码更为健壮。本文将列举这些解决方案，包括传统的空值检测、编程规范、以及使用现代 Java 语言引入的各类工具来作为辅助。</p><h2 id="运行时检测"><a href="#运行时检测" class="headerlink" title="运行时检测"></a>运行时检测</h2><p>最显而易见的方法就是使用 <code>if (obj == null)</code> 来对所有需要用到的对象来进行检测，包括函数参数、返回值、以及类实例的成员变量。当你检测到 <code>null</code> 值时，可以选择抛出更具针对性的异常类型，如 <code>IllegalArgumentException</code>，并添加消息内容。我们可以使用一些库函数来简化代码，如 Java 7 开始提供的 <a href="https://docs.oracle.com/javase/7/docs/api/java/util/Objects.html" target="_blank" rel="noopener"><code>Objects#requireNonNull</code></a> 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testObjects</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  Object checked = Objects.requireNonNull(arg, <span class="string">"arg must not be null"</span>);</span><br><span class="line">  checked.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Guava 的 <a href="https://github.com/google/guava/wiki/PreconditionsExplained" target="_blank" rel="noopener"><code>Preconditions</code></a> 类中也提供了一系列用于检测参数合法性的工具函数，其中就包含空值检测：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testGuava</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  Object checked = Preconditions.checkNotNull(arg, <span class="string">"%s must not be null"</span>, <span class="string">"arg"</span>);</span><br><span class="line">  checked.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们还可以使用 <a href="https://projectlombok.org/features/NonNull" target="_blank" rel="noopener">Lombok</a> 来生成空值检测代码，并抛出带有提示信息的空指针异常：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLombok</span><span class="params">(@NonNull Object arg)</span> </span>&#123;</span><br><span class="line">  arg.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生成的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLombokGenerated</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (arg == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(<span class="string">"arg is marked @NonNull but is null"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  arg.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个注解还可以用在类实例的成员变量上，所有的赋值操作会自动进行空值检测。</p><a id="more"></a><h2 id="编程规范"><a href="#编程规范" class="headerlink" title="编程规范"></a>编程规范</h2><p>通过遵守某些编程规范，也可以从一定程度上减少空指针异常的发生。</p><ul><li>使用那些已经对 <code>null</code> 值做过判断的方法，如 <code>String#equals</code>、<code>String#valueOf</code>、以及三方库中用来判断字符串和集合是否为空的函数：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (str != <span class="keyword">null</span> &amp;&amp; str.equals(<span class="string">"text"</span>)) &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> (<span class="string">"text"</span>.equals(str)) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123; obj.toString(); &#125;</span><br><span class="line">String.valueOf(obj); <span class="comment">// "null"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// from spring-core</span></span><br><span class="line">StringUtils.isEmpty(str);</span><br><span class="line">CollectionUtils.isEmpty(col);</span><br><span class="line"><span class="comment">// from guava</span></span><br><span class="line">Strings.isNullOrEmpty(str);</span><br><span class="line"><span class="comment">// from commons-collections4</span></span><br><span class="line">CollectionUtils.isEmpty(col);</span><br></pre></td></tr></table></figure><ul><li>如果函数的某个参数可以接收 <code>null</code> 值，考虑改写成两个函数，使用不同的函数签名，这样就可以强制要求每个参数都不为空了：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">methodA</span><span class="params">(Object arg1)</span> </span>&#123;</span><br><span class="line">  methodB(arg1, <span class="keyword">new</span> Object[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">methodB</span><span class="params">(Object arg1, Object[] arg2)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (Object obj : arg2) &#123;&#125; <span class="comment">// no null check</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>如果函数的返回值是集合类型，当结果为空时，不要返回 <code>null</code> 值，而是返回一个空的集合；如果返回值类型是对象，则可以选择抛出异常。Spring JdbcTemplate 正是使用了这种处理方式：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 当查询结果为空时，返回 new ArrayList&lt;&gt;()</span></span><br><span class="line">jdbcTemplate.queryForList(<span class="string">"SELECT * FROM person"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若找不到该条记录，则抛出 EmptyResultDataAccessException</span></span><br><span class="line">jdbcTemplate.queryForObject(<span class="string">"SELECT age FROM person WHERE id = 1"</span>, Integer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 支持泛型集合</span></span><br><span class="line"><span class="keyword">public</span> &lt;T&gt; <span class="function">List&lt;T&gt; <span class="title">testReturnCollection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="静态代码分析"><a href="#静态代码分析" class="headerlink" title="静态代码分析"></a>静态代码分析</h2><p>Java 语言有许多静态代码分析工具，如 Eclipse IDE、SpotBugs、Checker Framework 等，它们可以帮助程序员检测出编译期的错误。结合 <code>@Nullable</code> 和 <code>@Nonnull</code> 等注解，我们就可以在程序运行之前发现可能抛出空指针异常的代码。</p><p>但是，空值检测注解还没有得到标准化。虽然 2006 年 9 月社区提出了 <a href="https://jcp.org/en/jsr/detail?id=305" target="_blank" rel="noopener">JSR 305</a> 规范，但它长期处于搁置状态。很多第三方库提供了类似的注解，且得到了不同工具的支持，其中使用较多的有：</p><ul><li><code>javax.annotation.Nonnull</code>：由 JSR 305 提出，其参考实现为 <code>com.google.code.findbugs.jsr305</code>；</li><li><code>org.eclipse.jdt.annotation.NonNull</code>：Eclipse IDE 原生支持的空值检测注解；</li><li><code>edu.umd.cs.findbugs.annotations.NonNull</code>：SpotBugs 使用的注解，基于 <code>findbugs.jsr305</code>；</li><li><code>org.springframework.lang.NonNull</code>：Spring Framework 5.0 开始提供；</li><li><code>org.checkerframework.checker.nullness.qual.NonNull</code>：Checker Framework 使用；</li><li><code>android.support.annotation.NonNull</code>：集成在安卓开发工具中；</li></ul><p>我建议使用一种跨 IDE 的解决方案，如 SpotBugs 或 Checker Framework，它们都能和 Maven 结合得很好。</p><h3 id="SpotBugs-与-NonNull、-CheckForNull"><a href="#SpotBugs-与-NonNull、-CheckForNull" class="headerlink" title="SpotBugs 与 @NonNull、@CheckForNull"></a>SpotBugs 与 <code>@NonNull</code>、<code>@CheckForNull</code></h3><p>SpotBugs 是 FindBugs 的后继者。通过在方法的参数和返回值上添加 <code>@NonNull</code> 和 <code>@CheckForNull</code> 注解，SpotBugs 可以帮助我们进行编译期的空值检测。需要注意的是，SpotBugs 不支持 <code>@Nullable</code> 注解，必须用 <code>@CheckForNull</code> 代替。如官方文档中所说，仅当需要覆盖 <code>@ParametersAreNonnullByDefault</code> 时才会用到 <code>@Nullable</code>。</p><p><a href="https://spotbugs.readthedocs.io/en/latest/maven.html" target="_blank" rel="noopener">官方文档</a> 中说明了如何将 SpotBugs 应用到 Maven 和 Eclipse 中去。我们还需要将 <code>spotbugs-annotations</code> 加入到项目依赖中，以便使用对应的注解。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.spotbugs<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spotbugs-annotations<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>以下是对不同使用场景的说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@NonNull</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 错误：returnNonNull() 可能返回空值，但其已声明为 @Nonnull</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@CheckForNull</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReturnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Object obj = returnNullable();</span><br><span class="line">  <span class="comment">// 错误：方法的返回值可能为空</span></span><br><span class="line">  System.out.println(obj.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">argumentNonNull</span><span class="params">(@NonNull Object arg)</span> </span>&#123;</span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testArgumentNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 错误：不能将 null 传递给非空参数</span></span><br><span class="line">  argumentNonNull(<span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testNullableArgument</span><span class="params">(@CheckForNull Object arg)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 错误：参数可能为空</span></span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于 Eclipse 用户，还可以使用 IDE 内置的空值检测工具，只需将默认的注解 <code>org.eclipse.jdt.annotation.Nullable</code> 替换为 SpotBugs 的注解即可：</p><p><img src="/cnblogs/images/java-npe/eclipse.png" alt="Eclipse null analysis"></p><h3 id="Checker-Framework-与-NonNull、-Nullable"><a href="#Checker-Framework-与-NonNull、-Nullable" class="headerlink" title="Checker Framework 与 @NonNull、@Nullable"></a>Checker Framework 与 <code>@NonNull</code>、<code>@Nullable</code></h3><p>Checker Framework 能够作为 <code>javac</code> 编译器的插件运行，对代码中的数据类型进行检测，预防各类问题。我们可以参照 <a href="https://checkerframework.org/manual/#maven" target="_blank" rel="noopener">官方文档</a>，将 Checker Framework 与 <code>maven-compiler-plugin</code> 结合，之后每次执行 <code>mvn compile</code> 时就会进行检查。Checker Framework 的空值检测程序支持几乎所有的注解，包括 JSR 305、Eclipse、甚至 <code>lombok.NonNull</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.checkerframework.checker.nullness.qual.Nullable;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Nullable</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReturnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Object obj = returnNullable();</span><br><span class="line">  <span class="comment">// 错误：obj 可能为空</span></span><br><span class="line">  System.out.println(obj.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Checker Framework 默认会将 <code>@NonNull</code> 应用到所有的函数参数和返回值上，因此，即使不添加这个注解，以下程序也是无法编译通过的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 错误：方法声明为 @NonNull，但返回的是 null。</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">argumentNonNull</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testArgumentNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 错误：参数声明为 @NonNull，但传入的是 null。</span></span><br><span class="line">  argumentNonNull(<span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Checker Framework 对使用 Spring Framework 5.0 以上的用户非常有用，因为 Spring 提供了内置的空值检测注解，且能够被 Checker Framework 支持。一方面我们无需再引入额外的 Jar 包，更重要的是 Spring Framework 代码本身就使用了这些注解，这样我们在调用它的 API 时就能有效地处理空值了。举例来说，<code>StringUtils</code> 类里可以传入空值的函数、以及会返回空值的函数都添加了 <code>@Nullable</code> 注解，而未添加的方法则继承了整个框架的 <code>@NonNull</code> 注解，因此，下列代码中的空指针异常就可以被 Checker Framework 检测到了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这是 spring-core 中定义的类和方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StringUtils</span> </span>&#123;</span><br><span class="line">  <span class="comment">// str 参数继承了全局的 @NonNull 注解</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">capitalize</span><span class="params">(String str)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Nullable</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getFilename</span><span class="params">(@Nullable String path)</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误：参数声明为 @NonNull，但传入的是 null。</span></span><br><span class="line">StringUtils.capitalize(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">String filename = StringUtils.getFilename(<span class="string">"/path/to/file"</span>);</span><br><span class="line"><span class="comment">// 错误：filename 可能为空。</span></span><br><span class="line">System.out.println(filename.length());</span><br></pre></td></tr></table></figure><h2 id="Optional-类型"><a href="#Optional-类型" class="headerlink" title="Optional 类型"></a><code>Optional</code> 类型</h2><p>Java 8 引入了 <code>Optional&lt;T&gt;</code> 类型，我们可以用它来对函数的返回值进行包装。这种方式的优点是可以明确定义该方法是有可能返回空值的，因此调用方必须做好相应处理，这样也就不会引发空指针异常。但是，也不可避免地需要编写更多代码，而且会产生很多垃圾对象，增加 GC 的压力，因此在使用时需要酌情考虑。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Optional&lt;String&gt; opt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建</span></span><br><span class="line">opt = Optional.empty();</span><br><span class="line">opt = Optional.of(<span class="string">"text"</span>);</span><br><span class="line">opt = Optional.ofNullable(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断并读取</span></span><br><span class="line"><span class="keyword">if</span> (opt.isPresent()) &#123;</span><br><span class="line">  opt.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认值</span></span><br><span class="line">opt.orElse(<span class="string">"default"</span>);</span><br><span class="line">opt.orElseGet(() -&gt; <span class="string">"default"</span>);</span><br><span class="line">opt.orElseThrow(() -&gt; <span class="keyword">new</span> NullPointerException());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 相关操作</span></span><br><span class="line">opt.ifPresent(value -&gt; &#123;</span><br><span class="line">  System.out.println(value);</span><br><span class="line">&#125;);</span><br><span class="line">opt.filter(value -&gt; value.length() &gt; <span class="number">5</span>);</span><br><span class="line">opt.map(value -&gt; value.trim());</span><br><span class="line">opt.flatMap(value -&gt; &#123;</span><br><span class="line">  String trimmed = value.trim();</span><br><span class="line">  <span class="keyword">return</span> trimmed.isEmpty() ? Optional.empty() : Optional.of(trimmed);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>方法的链式调用很容易引发空指针异常，但如果返回值都用 <code>Optional</code> 包装起来，就可以用 <code>flatMap</code> 方法来实现安全的链式调用了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String zipCode = getUser()</span><br><span class="line">    .flatMap(User::getAddress)</span><br><span class="line">    .flatMap(Address::getZipCode)</span><br><span class="line">    .orElse(<span class="string">""</span>);</span><br></pre></td></tr></table></figure><p>Java 8 <a href="https://www.oracle.com/technetwork/articles/java/ma14-java-se-8-streams-2177646.html" target="_blank" rel="noopener">Stream API</a> 同样使用了 <code>Optional</code> 作为返回类型：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stringList.stream().findFirst().orElse(<span class="string">"default"</span>);</span><br><span class="line">stringList.stream()</span><br><span class="line">    .max(Comparator.naturalOrder())</span><br><span class="line">    .ifPresent(System.out::println);</span><br></pre></td></tr></table></figure><p>此外，Java 8 还针对基础类型提供了单独的 <code>Optional</code> 类，如 <code>OptionalInt</code>、<code>OptionalDouble</code> 等，在性能要求比较高的场景下很适用。</p><h2 id="其它-JVM-语言中的空指针异常"><a href="#其它-JVM-语言中的空指针异常" class="headerlink" title="其它 JVM 语言中的空指针异常"></a>其它 JVM 语言中的空指针异常</h2><p>Scala 语言中的 <a href="https://www.scala-lang.org/api/current/scala/Option.html" target="_blank" rel="noopener"><code>Option</code></a> 类可以对标 Java 8 的 <code>Optional</code>。它有两个子类型，<code>Some</code> 表示有值，<code>None</code> 表示空。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> opt: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">Some</span>(<span class="string">"text"</span>)</span><br><span class="line">opt.getOrElse(<span class="string">"default"</span>)</span><br></pre></td></tr></table></figure><p>除了使用 <code>Option#isEmpty</code> 判断，还可以使用 Scala 的模式匹配：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">opt <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(text) =&gt; println(text)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">"default"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Scala 的集合处理函数库非常强大，<code>Option</code> 则可直接作为集合进行操作，如 <code>filer</code>、<code>map</code>、以及列表解析（for-comprehension）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">opt.map(_.trim).filter(_.length &gt; <span class="number">0</span>).map(_.toUpperCase).getOrElse(<span class="string">"DEFAULT"</span>)</span><br><span class="line"><span class="keyword">val</span> upper = <span class="keyword">for</span> &#123;</span><br><span class="line">  text &lt;- opt</span><br><span class="line">  trimmed &lt;- <span class="type">Some</span>(text.trim())</span><br><span class="line">  upper &lt;- <span class="type">Some</span>(trimmed) <span class="keyword">if</span> trimmed.length &gt; <span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">yield</span> upper</span><br><span class="line">upper.getOrElse(<span class="string">"DEFAULT"</span>)</span><br></pre></td></tr></table></figure><p>Kotlin 使用了另一种方式，用户在定义变量时就需要明确区分 <a href="https://kotlinlang.org/docs/reference/java-interop.html#nullability-annotations" target="_blank" rel="noopener">可空和不可空类型</a>。当可空类型被使用时，就必须进行空值检测。</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a: String = <span class="string">"text"</span></span><br><span class="line">a = <span class="literal">null</span> <span class="comment">// 错误：无法将 null 赋值给非空 String 类型。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> b: String? = <span class="string">"text"</span></span><br><span class="line"><span class="comment">// 错误：操作可空类型时必须使用安全操作符（?.）或强制忽略（!!.）。</span></span><br><span class="line">println(b.length)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> l: <span class="built_in">Int</span>? = b?.length <span class="comment">// 安全操作</span></span><br><span class="line">b!!.length <span class="comment">// 强制忽略，可能引发空值异常</span></span><br></pre></td></tr></table></figure><p>Kotlin 的特性之一是与 Java 的可互操作性，但 Kotlin 编译器无法知晓 Java 类型是否为空，这就需要在 Java 代码中使用注解了，而 Kotlin 支持的 <a href="https://kotlinlang.org/docs/reference/java-interop.html#nullability-annotations" target="_blank" rel="noopener">注解</a> 也非常广泛。Spring Framework 5.0 起原生支持 Kotlin，其空值检测也是通过注解进行的，使得 Kotlin 可以安全地调用 Spring Framework 的所有 API。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在以上这些方案中，我比较推荐使用注解来预防空指针异常，因为这种方式十分有效，对代码的侵入性也较小。所有的公共 API 都应该使用 <code>@Nullable</code> 和 <code>@NonNull</code> 进行注解，这样就能强制调用方对空指针异常进行预防，让我们的程序更为健壮。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://howtodoinjava.com/java/exception-handling/how-to-effectively-handle-nullpointerexception-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/java/exception-handling/how-to-effectively-handle-nullpointerexception-in-java/</a></li><li><a href="http://jmri.sourceforge.net/help/en/html/doc/Technical/SpotBugs.shtml" target="_blank" rel="noopener">http://jmri.sourceforge.net/help/en/html/doc/Technical/SpotBugs.shtml</a></li><li><a href="https://dzone.com/articles/features-to-avoid-null-reference-exceptions-java-a" target="_blank" rel="noopener">https://dzone.com/articles/features-to-avoid-null-reference-exceptions-java-a</a></li><li><a href="https://medium.com/@fatihcoskun/kotlin-nullable-types-vs-java-optional-988c50853692" target="_blank" rel="noopener">https://medium.com/@fatihcoskun/kotlin-nullable-types-vs-java-optional-988c50853692</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java 中任何对象都有可能为空，当我们调用空对象的方法时就会抛出 &lt;code&gt;NullPointerException&lt;/code&gt; 空指针异常，这是一种非常常见的错误类型。我们可以使用若干种方法来避免产生这类异常，使得我们的代码更为健壮。本文将列举这些解决方案，包括传统的空值检测、编程规范、以及使用现代 Java 语言引入的各类工具来作为辅助。&lt;/p&gt;
&lt;h2 id=&quot;运行时检测&quot;&gt;&lt;a href=&quot;#运行时检测&quot; class=&quot;headerlink&quot; title=&quot;运行时检测&quot;&gt;&lt;/a&gt;运行时检测&lt;/h2&gt;&lt;p&gt;最显而易见的方法就是使用 &lt;code&gt;if (obj == null)&lt;/code&gt; 来对所有需要用到的对象来进行检测，包括函数参数、返回值、以及类实例的成员变量。当你检测到 &lt;code&gt;null&lt;/code&gt; 值时，可以选择抛出更具针对性的异常类型，如 &lt;code&gt;IllegalArgumentException&lt;/code&gt;，并添加消息内容。我们可以使用一些库函数来简化代码，如 Java 7 开始提供的 &lt;a href=&quot;https://docs.oracle.com/javase/7/docs/api/java/util/Objects.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;Objects#requireNonNull&lt;/code&gt;&lt;/a&gt; 方法：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testObjects&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Object checked = Objects.requireNonNull(arg, &lt;span class=&quot;string&quot;&gt;&quot;arg must not be null&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  checked.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Guava 的 &lt;a href=&quot;https://github.com/google/guava/wiki/PreconditionsExplained&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;Preconditions&lt;/code&gt;&lt;/a&gt; 类中也提供了一系列用于检测参数合法性的工具函数，其中就包含空值检测：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testGuava&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Object checked = Preconditions.checkNotNull(arg, &lt;span class=&quot;string&quot;&gt;&quot;%s must not be null&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;arg&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  checked.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;我们还可以使用 &lt;a href=&quot;https://projectlombok.org/features/NonNull&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lombok&lt;/a&gt; 来生成空值检测代码，并抛出带有提示信息的空指针异常：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testLombok&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(@NonNull Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  arg.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;生成的代码如下：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testLombokGenerated&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (arg == &lt;span class=&quot;keyword&quot;&gt;null&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; NullPointerException(&lt;span class=&quot;string&quot;&gt;&quot;arg is marked @NonNull but is null&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  arg.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这个注解还可以用在类实例的成员变量上，所有的赋值操作会自动进行空值检测。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="spring" scheme="http://shzhangji.com/cnblogs/tags/spring/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="eclipse" scheme="http://shzhangji.com/cnblogs/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>是否需要使用 ESLint jsx-no-bind 规则？</title>
    <link href="http://shzhangji.com/cnblogs/2018/09/14/is-it-necessary-to-apply-eslint-jsx-no-bind-rule/"/>
    <id>http://shzhangji.com/cnblogs/2018/09/14/is-it-necessary-to-apply-eslint-jsx-no-bind-rule/</id>
    <published>2018-09-14T01:18:52.000Z</published>
    <updated>2020-08-22T12:06:11.268Z</updated>
    
    <content type="html"><![CDATA[<p>在使用 <a href="https://github.com/yannickcr/eslint-plugin-react" target="_blank" rel="noopener">ESLint React</a> 插件时，有一条名为 <a href="https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md" target="_blank" rel="noopener"><code>jsx-no-bind</code></a> 的检测规则，它会禁止我们在 JSX 属性中使用 <code>.bind</code> 方法和箭头函数。比如下列代码，ESLint 会提示 <code>onClick</code> 属性中的箭头函数不合法：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListArrow</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.state.items.map(<span class="function"><span class="params">item</span> =&gt;</span> (</span><br><span class="line">          &lt;li key=&#123;item.id&#125; onClick=&#123;() =&gt; &#123; alert(item.id) &#125;&#125;&gt;&#123;item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        ))&#125;</span></span><br><span class="line"><span class="regexp">      &lt;/u</span>l&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这条规则的引入原因有二。首先，每次执行 <code>render</code> 方法时都会生成一个新的匿名函数对象，这样就会对垃圾回收器造成负担；其次，属性中的箭头函数会影响渲染过程：当你使用了 <code>PureComponent</code>，或者自己实现了 <code>shouldComponentUpdate</code> 方法，使用对象比较的方式来决定是否要重新渲染组件，那么组件属性中的箭头函数就会让该方法永远返回真值，引起不必要的重复渲染。</p><p>然而，反对的声音认为这两个原因还不足以要求我们在所有代码中应用该规则，特别是当需要引入更多代码、并牺牲一定可读性的情况下。在 <a href="https://github.com/airbnb/javascript/blob/eslint-config-airbnb-v17.1.0/packages/eslint-config-airbnb/rules/react.js#L93" target="_blank" rel="noopener">Airbnb ESLint</a> 预设规则集中，只禁止了 <code>.bind</code> 方法的使用，而允许在属性（props）或引用（refs）中使用箭头函数。对此我翻阅了文档，阅读了一些关于这个话题的博客，也认为这条规则有些过于严格。甚至还有博主称该规则是一种过早优化（premature optimization），我们需要先做基准测试，再着手修改代码。下文中，我将简要叙述箭头函数是如何影响渲染过程的，有哪些可行的解决方案，以及它为何不太重要。</p><a id="more"></a><h2 id="不同类型的-React-组件"><a href="#不同类型的-React-组件" class="headerlink" title="不同类型的 React 组件"></a>不同类型的 React 组件</h2><p>通常我们会通过继承 <code>React.Component</code> 类并实现 <code>render</code> 方法来创建一个 React 组件。另一个内置的组件基类是 <code>React.PureComponent</code>，它的区别在于已经为我们实现了 <code>shouldComponentUpdate</code> 方法。在普通的 React 组件中，该方法默认返回 <code>true</code>，也就是说当属性（props）或状态（state）发生改变时，一定会重新进行渲染。而 <code>PureComponent</code> 实现的该方法中，会对新、旧属性和状态的键值做一个等值比较，只有当内容发生改变时才会重新渲染。下面定义的这两个组件产生的效果是一致的：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PureChild</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">PureComponent</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;&#123;<span class="keyword">this</span>.props.message&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">class RegularChildB extends React.Component &#123;</span></span><br><span class="line"><span class="regexp">  shouldComponentUpdate(nextProps, nextStates) &#123;</span></span><br><span class="line"><span class="regexp">    return this.props.message !== nextProps.message</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  render() &#123;</span></span><br><span class="line"><span class="regexp">    return (</span></span><br><span class="line"><span class="regexp">      &lt;div&gt;&#123;this.props.message&#125;&lt;/</span>div&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当它们的属性发生变化时，会检查 <code>message</code> 变量中的值是否和原来相等。属性和状态都是 <code>object</code> 类型，React 会遍历所有键值进行 <code>===</code> 等值比较。在 JavaScript 中，只有基础类型之间的比较、或同一个对象和自身比较时才能通过。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> === <span class="number">1</span></span><br><span class="line"><span class="string">'hello world'</span> === <span class="string">'hello world'</span></span><br><span class="line">[] !== []</span><br><span class="line">(<span class="function"><span class="params">()</span> =&gt;</span> &#123;&#125;) !== <span class="function">(<span class="params">(</span>) =&gt;</span> &#123;&#125;)</span><br></pre></td></tr></table></figure><p>显然，箭头函数是无法通过这个等值检查的。如果父组件将箭头函数作为属性传入 <code>PureComponent</code>，那么每次渲染都会引发子组件的渲染。相反地，如果我们没有使用 <code>PureComponent</code>，或进行类似的等值比较，那么组件一定会进行更新，也就没有应用该规则的必要了。</p><p>另一个种较为流行的组件定义方式是“无状态函数式组件（SFC）”。这类组件好比一个数学函数，其渲染结果完全依赖于输入的属性值。不过，它本质上是一个普通的组件，并没有实现 <code>shouldComponentUpdate</code> 方法，且组件的定义方式也不允许我们自己来实现。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> StatelessChild = <span class="function">(<span class="params">props</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> (</span><br><span class="line">    &lt;div&gt;&#123;props.message&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">  )</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="如何修复-jsx-no-bind-错误警告"><a href="#如何修复-jsx-no-bind-错误警告" class="headerlink" title="如何修复 jsx-no-bind 错误警告"></a>如何修复 <code>jsx-no-bind</code> 错误警告</h2><p>箭头函数通常会用作事件处理。如果我们直接使用普通的函数或类方法，<code>this</code> 关键字将无法正确绑定到当前实例，它的值是 <code>undefined</code>。只有使用了 <code>.bind</code> 方法或箭头函数时，我们才能在函数体中通过 <code>this</code> 来访问到类的其他成员，只是这样就会触发 <code>jsx-no-bind</code> 报警。解决方法是在构造函数中对方法进行绑定，或者使用尚在草案阶段的类属性语法，并通过 <a href="https://babeljs.io/docs/plugins/transform-class-properties/" target="_blank" rel="noopener">Babel</a> 进行转换。更多信息可以查阅 <a href="https://reactjs.org/docs/handling-events.html" target="_blank" rel="noopener">React 官方文档</a>，以下用 <a href="https://github.com/jizhang/jsx-no-bind/blob/master/src/components/NoArgument.js" target="_blank" rel="noopener">代码</a> 演示不同的做法。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">NoArgument</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  <span class="keyword">constructor</span>() &#123;</span><br><span class="line">    <span class="keyword">this</span>.handleClickBoundA = <span class="keyword">this</span>.handleClickUnbound.bind(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">this</span>.handleClickBoundC = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.setState() &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  handleClickUnbound() &#123; <span class="comment">/* "this" 的值是未定义 */</span> &#125;</span><br><span class="line">  handleClickBoundB = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.setState() &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;</span><br><span class="line">        <span class="built_in">Error</span>: jsx-no-bind</span><br><span class="line">        &lt;button onClick=&#123;() =&gt; &#123; <span class="keyword">this</span>.setState() &#125;&#125;&gt;ArrowA&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;() =&gt; &#123; this.handleClickUnbound() &#125;&#125;&gt;ArrowB&lt;/</span>button&gt;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.handleClickUnbound.bind(<span class="keyword">this</span>)&#125;&gt;Bind&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        No error:</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.handleClickBoundA&#125;&gt;BoundA&lt;/</span>button&gt;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.handleClickBoundB&#125;&gt;BoundB&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.handleClickBoundC&#125;&gt;BoundC&lt;/</span>button&gt;</span><br><span class="line">      &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><p>如果事件处理需要用到额外的参数，比如渲染列表时捕捉每一项的点击事件，就不那么容易了。有两种解决方案，一是将列表项作为独立的组件拆分出来，通过组件属性来传递事件处理函数和它的参数，示例如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Item</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">PureComponent</span> </span>&#123;</span><br><span class="line">  handleClick = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.props.onClick(<span class="keyword">this</span>.props.item.id) &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;li onClick=&#123;<span class="keyword">this</span>.handleClick&#125;&gt;&#123;<span class="keyword">this</span>.props.item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">export default class ListSeparate extends React.Component &#123;</span></span><br><span class="line"><span class="regexp">  handleClick = (itemId) =&gt; &#123; alert(itemId) &#125;</span></span><br><span class="line"><span class="regexp">  render() &#123;</span></span><br><span class="line"><span class="regexp">    return (</span></span><br><span class="line"><span class="regexp">      &lt;ul&gt;</span></span><br><span class="line"><span class="regexp">        &#123;this.props.items.map(item =&gt; (</span></span><br><span class="line"><span class="regexp">          &lt;Item key=&#123;item.id&#125; item=&#123;item&#125; onClick=&#123;this.handleClick&#125; /</span>&gt;</span><br><span class="line">        ))&#125;</span><br><span class="line">      &lt;<span class="regexp">/ul&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><p>这种方式也称之为关注点分离（separation of concerns），因为 <code>List</code> 组件只需负责遍历列表项，而由 <code>Item</code> 组件来负责渲染。不过这样一来也会增加许多模板代码，我们需要跟踪多个属性值来确定事件处理过程，因此降低了代码可读性。若直接使用箭头函数，事件处理和组件渲染是在一处的，便于理解，也是 React 社区所推崇的方式。</p><p>另一种方式是使用 DOM <a href="https://developer.mozilla.org/en/docs/Web/API/HTMLElement/dataset" target="_blank" rel="noopener"><code>dataset</code></a> 属性，也就是将需要传递的参数暂存在 HTML 标签的 <code>data-*</code> 属性中， 然后通过 <code>event</code> 变量来读取。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">ListDataset</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  handleClick = <span class="function">(<span class="params">event</span>) =&gt;</span> &#123; alert(event.target.dataset.itemId) &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.props.items.map(<span class="function"><span class="params">item</span> =&gt;</span> (</span><br><span class="line">          &lt;li key=&#123;item.id&#125; data-item-id=&#123;item.id&#125; onClick=&#123;<span class="keyword">this</span>.handleClick&#125;&gt;&#123;item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        ))&#125;</span></span><br><span class="line"><span class="regexp">      &lt;/u</span>l&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="虚拟-DOM-与-React-协调"><a href="#虚拟-DOM-与-React-协调" class="headerlink" title="虚拟 DOM 与 React 协调"></a>虚拟 DOM 与 React 协调</h2><p>上文说到，箭头函数会引发 <code>PureComponent</code> 不必要的渲染，这个结论只正确了一半。React 的渲染过程可以分为几个步骤：首先，调用 <code>render</code> 方法，返回一个 React 元素的树形结构；将该结构与内存中的虚拟 DOM 树进行对比，将差异部分应用到浏览器的真实 DOM 树中。这个过程在 React 中称为协调（<a href="https://reactjs.org/docs/reconciliation.html" target="_blank" rel="noopener">reconciliation</a>）。因此，即便 <code>render</code> 方法被调用了多次，如果其返回的 React 元素树都是相同的，那么也不会触发真实 DOM 渲染，而这个过程通常会比纯 JavaScript 要来得耗时。这样看来，如果一个组件的确需要频繁变动，那么继承了 <code>PureComponent</code> 反而会增加一次比对的消耗，得不偿失。</p><p><img src="/cnblogs/images/jsx-no-bind/should-component-update.png" alt="shouldComponentUpdate 生命周期方法"></p><p><a href="https://reactjs.org/docs/optimizing-performance.html#shouldcomponentupdate-in-action" target="_blank" rel="noopener">图片来源</a></p><p>此外，在事件绑定属性中使用箭头函数，一般也不会触发真实 DOM 的渲染，原因是 React 的事件监听器是绑定在顶层的 <code>document</code> 元素上的，当 <code>li</code> 上触发了 <code>onClick</code> 事件后，该事件会向上冒泡（bubble up）至顶层元素，由 React 事件管理系统接收和处理。</p><p><img src="/cnblogs/images/jsx-no-bind/top-level-delegation.jpg" alt="顶层事件委托"></p><p><a href="https://levelup.gitconnected.com/how-exactly-does-react-handles-events-71e8b5e359f2" target="_blank" rel="noopener">图片来源</a></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>可以看到，在修复 <code>jsx-no-bind</code> 的过程中，我们需要牺牲一定的代码可读性，而获得的性能收益也许是微不足道甚至是相反的。与其猜测箭头函数会引发性能问题，不如先用最自然的方式来编写代码，当遇到真正的性能瓶颈时加以测度，最终找出合适的技术方案。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md" target="_blank" rel="noopener">https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md</a></li><li><a href="https://cdb.reacttraining.com/react-inline-functions-and-performance-bdff784f5578" target="_blank" rel="noopener">https://cdb.reacttraining.com/react-inline-functions-and-performance-bdff784f5578</a></li><li><a href="https://maarten.mulders.it/blog/2017/07/no-bind-or-arrow-in-jsx-props-why-how.html" target="_blank" rel="noopener">https://maarten.mulders.it/blog/2017/07/no-bind-or-arrow-in-jsx-props-why-how.html</a></li><li><a href="https://reactjs.org/docs/faq-functions.html#example-passing-params-using-data-attributes" target="_blank" rel="noopener">https://reactjs.org/docs/faq-functions.html#example-passing-params-using-data-attributes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用 &lt;a href=&quot;https://github.com/yannickcr/eslint-plugin-react&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ESLint React&lt;/a&gt; 插件时，有一条名为 &lt;a href=&quot;https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;jsx-no-bind&lt;/code&gt;&lt;/a&gt; 的检测规则，它会禁止我们在 JSX 属性中使用 &lt;code&gt;.bind&lt;/code&gt; 方法和箭头函数。比如下列代码，ESLint 会提示 &lt;code&gt;onClick&lt;/code&gt; 属性中的箭头函数不合法：&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ListArrow&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  render() &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; (&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;lt;ul&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#123;&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.state.items.map(&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;item&lt;/span&gt; =&amp;gt;&lt;/span&gt; (&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;lt;li key=&amp;#123;item.id&amp;#125; onClick=&amp;#123;() =&amp;gt; &amp;#123; alert(item.id) &amp;#125;&amp;#125;&amp;gt;&amp;#123;item.text&amp;#125;&amp;lt;&lt;span class=&quot;regexp&quot;&gt;/li&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;regexp&quot;&gt;        ))&amp;#125;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;regexp&quot;&gt;      &amp;lt;/u&lt;/span&gt;l&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这条规则的引入原因有二。首先，每次执行 &lt;code&gt;render&lt;/code&gt; 方法时都会生成一个新的匿名函数对象，这样就会对垃圾回收器造成负担；其次，属性中的箭头函数会影响渲染过程：当你使用了 &lt;code&gt;PureComponent&lt;/code&gt;，或者自己实现了 &lt;code&gt;shouldComponentUpdate&lt;/code&gt; 方法，使用对象比较的方式来决定是否要重新渲染组件，那么组件属性中的箭头函数就会让该方法永远返回真值，引起不必要的重复渲染。&lt;/p&gt;
&lt;p&gt;然而，反对的声音认为这两个原因还不足以要求我们在所有代码中应用该规则，特别是当需要引入更多代码、并牺牲一定可读性的情况下。在 &lt;a href=&quot;https://github.com/airbnb/javascript/blob/eslint-config-airbnb-v17.1.0/packages/eslint-config-airbnb/rules/react.js#L93&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Airbnb ESLint&lt;/a&gt; 预设规则集中，只禁止了 &lt;code&gt;.bind&lt;/code&gt; 方法的使用，而允许在属性（props）或引用（refs）中使用箭头函数。对此我翻阅了文档，阅读了一些关于这个话题的博客，也认为这条规则有些过于严格。甚至还有博主称该规则是一种过早优化（premature optimization），我们需要先做基准测试，再着手修改代码。下文中，我将简要叙述箭头函数是如何影响渲染过程的，有哪些可行的解决方案，以及它为何不太重要。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="javascript" scheme="http://shzhangji.com/cnblogs/tags/javascript/"/>
    
      <category term="react" scheme="http://shzhangji.com/cnblogs/tags/react/"/>
    
      <category term="eslint" scheme="http://shzhangji.com/cnblogs/tags/eslint/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 模型如何对外提供服务</title>
    <link href="http://shzhangji.com/cnblogs/2018/05/14/serve-tensorflow-estimator-with-savedmodel/"/>
    <id>http://shzhangji.com/cnblogs/2018/05/14/serve-tensorflow-estimator-with-savedmodel/</id>
    <published>2018-05-14T05:23:14.000Z</published>
    <updated>2020-08-22T12:06:11.268Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a> 是目前最为流行的机器学习框架之一，通过它我们可以便捷地构建机器学习模型。使用 TensorFlow 模型对外提供服务有若干种方式，本文将介绍如何使用 SavedModel 机制来编写模型预测接口。</p><p><img src="/cnblogs/images/tf-logo.png" alt></p><h2 id="鸢尾花深层神经网络分类器"><a href="#鸢尾花深层神经网络分类器" class="headerlink" title="鸢尾花深层神经网络分类器"></a>鸢尾花深层神经网络分类器</h2><p>首先让我们使用 TensorFlow 的深层神经网络模型来构建一个鸢尾花的分类器。完整的教程可以在 TensorFlow 的官方文档中查看（<a href="https://www.tensorflow.org/get_started/premade_estimators" target="_blank" rel="noopener">Premade Estimators</a>），我也提供了一份示例代码，托管在 GitHub 上（<a href="https://github.com/jizhang/tf-serve/blob/master/iris_dnn.py" target="_blank" rel="noopener"><code>iris_dnn.py</code></a>），读者可以克隆到本地进行测试。以下是部分代码摘要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">feature_columns = [tf.feature_column.numeric_column(key=key)</span><br><span class="line">                   <span class="keyword">for</span> key <span class="keyword">in</span> train_x.keys()]</span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">    feature_columns=feature_columns,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    n_classes=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: train_input_fn(train_x, train_y, batch_size=BATCH_SIZE),</span><br><span class="line">    steps=STEPS)</span><br><span class="line"></span><br><span class="line">predictions = classifier.predict(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: eval_input_fn(predict_x, labels=<span class="literal">None</span>, batch_size=BATCH_SIZE))</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="将模型导出为-SavedModel-格式"><a href="#将模型导出为-SavedModel-格式" class="headerlink" title="将模型导出为 SavedModel 格式"></a>将模型导出为 SavedModel 格式</h2><p>TensorFlow 提供了 <a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators" target="_blank" rel="noopener">SavedModel</a> 机制，用以将训练好的模型导出为外部文件，供后续使用或对外提供服务。<code>Estimator</code> 类的 <code>export_savedmodel</code> 方法接收两个参数：导出目录和数据接收函数。该函数定义了导出的模型将会对何种格式的参数予以响应。通常，我们会使用 TensorFlow 的 <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/example/example.proto" target="_blank" rel="noopener"><code>Example</code></a> 类型来表示样本和特征。例如，鸢尾花样本可以用如下形式表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Example(</span><br><span class="line">    features=Features(</span><br><span class="line">        feature=&#123;</span><br><span class="line">            <span class="string">'SepalLength'</span>: Feature(float_list=FloatList(value=[<span class="number">5.1</span>])),</span><br><span class="line">            <span class="string">'SepalWidth'</span>: Feature(float_list=FloatList(value=[<span class="number">3.3</span>])),</span><br><span class="line">            <span class="string">'PetalLength'</span>: Feature(float_list=FloatList(value=[<span class="number">1.7</span>])),</span><br><span class="line">            <span class="string">'PetalWidth'</span>: Feature(float_list=FloatList(value=[<span class="number">0.5</span>])),</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>接收函数会收到序列化后的 <code>Example</code> 对象，将其转化成一组 Tensor 供模型消费。TensorFlow 提供了一些工具函数帮助我们完成这些转换。首先，我们将 <code>feature_columns</code> 数组转化成 <code>Feature</code> 字典，作为反序列化的规格标准，再用它生成接收函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [</span></span><br><span class="line"><span class="comment">#     _NumericColumn(key='SepalLength', shape=(1,), dtype=tf.float32),</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line">feature_columns = [tf.feature_column.numeric_column(key=key)</span><br><span class="line">                   <span class="keyword">for</span> key <span class="keyword">in</span> train_x.keys()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#     'SepalLength': FixedLenFeature(shape=(1,), dtype=tf.float32),</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line">feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建接收函数，并导出模型。</span></span><br><span class="line">serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)</span><br><span class="line">export_dir = classifier.export_savedmodel(<span class="string">'export'</span>, serving_input_receiver_fn)</span><br></pre></td></tr></table></figure><h2 id="使用命令行工具检测-SavedModel"><a href="#使用命令行工具检测-SavedModel" class="headerlink" title="使用命令行工具检测 SavedModel"></a>使用命令行工具检测 SavedModel</h2><p>每次导出模型都会生成一个带有时间戳的目录，里面包含了该模型的参数信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export/1524907728/saved_model.pb</span><br><span class="line">export/1524907728/variables</span><br><span class="line">export/1524907728/variables/variables.data-00000-of-00001</span><br><span class="line">export/1524907728/variables/variables.index</span><br></pre></td></tr></table></figure><p>TensorFlow 提供的命令行工具可用于检视导出模型的内容，甚至可以直接调用预测函数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ saved_model_cli show --dir <span class="built_in">export</span>/1524906774 \</span><br><span class="line">  --tag_set serve --signature_def serving_default</span><br><span class="line">The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">  inputs[<span class="string">'inputs'</span>] tensor_info:</span><br><span class="line">      dtype: DT_STRING</span><br><span class="line">      shape: (-1)</span><br><span class="line">The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">  outputs[<span class="string">'classes'</span>] tensor_info:</span><br><span class="line">      dtype: DT_STRING</span><br><span class="line">      shape: (-1, 3)</span><br><span class="line">  outputs[<span class="string">'scores'</span>] tensor_info:</span><br><span class="line">      dtype: DT_FLOAT</span><br><span class="line">      shape: (-1, 3)</span><br><span class="line">Method name is: tensorflow/serving/classify</span><br><span class="line"></span><br><span class="line">$ saved_model_cli run --dir <span class="built_in">export</span>/1524906774 \</span><br><span class="line">  --tag_set serve --signature_def serving_default \</span><br><span class="line">  --input_examples <span class="string">'inputs=[&#123;"SepalLength":[5.1],"SepalWidth":[3.3],"PetalLength":[1.7],"PetalWidth":[0.5]&#125;]'</span></span><br><span class="line">Result <span class="keyword">for</span> output key classes:</span><br><span class="line">[[b<span class="string">'0'</span> b<span class="string">'1'</span> b<span class="string">'2'</span>]]</span><br><span class="line">Result <span class="keyword">for</span> output key scores:</span><br><span class="line">[[9.9919027e-01 8.0969761e-04 1.2872645e-09]]</span><br></pre></td></tr></table></figure><h2 id="使用-contrib-predictor-提供服务"><a href="#使用-contrib-predictor-提供服务" class="headerlink" title="使用 contrib.predictor 提供服务"></a>使用 <code>contrib.predictor</code> 提供服务</h2><p><code>tf.contrib.predictor.from_saved_model</code> 方法能够将导出的模型加载进来，直接生成一个预测函数供使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从导出目录中加载模型，并生成预测函数。</span></span><br><span class="line">predict_fn = tf.contrib.predictor.from_saved_model(export_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Pandas 数据框定义测试数据。</span></span><br><span class="line">inputs = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">'SepalLength'</span>: [<span class="number">5.1</span>, <span class="number">5.9</span>, <span class="number">6.9</span>],</span><br><span class="line">    <span class="string">'SepalWidth'</span>: [<span class="number">3.3</span>, <span class="number">3.0</span>, <span class="number">3.1</span>],</span><br><span class="line">    <span class="string">'PetalLength'</span>: [<span class="number">1.7</span>, <span class="number">4.2</span>, <span class="number">5.4</span>],</span><br><span class="line">    <span class="string">'PetalWidth'</span>: [<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.1</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入数据转换成序列化后的 Example 字符串。</span></span><br><span class="line">examples = []</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> inputs.iterrows():</span><br><span class="line">    feature = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> col, value <span class="keyword">in</span> row.iteritems():</span><br><span class="line">        feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line">    example = tf.train.Example(</span><br><span class="line">        features=tf.train.Features(</span><br><span class="line">            feature=feature</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    examples.append(example.SerializeToString())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始预测</span></span><br><span class="line">predictions = predict_fn(&#123;<span class="string">'inputs'</span>: examples&#125;)</span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#     'classes': [</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2'],</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2'],</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2']</span></span><br><span class="line"><span class="comment">#     ],</span></span><br><span class="line"><span class="comment">#     'scores': [</span></span><br><span class="line"><span class="comment">#         [9.9826765e-01, 1.7323202e-03, 4.7271198e-15],</span></span><br><span class="line"><span class="comment">#         [2.1470961e-04, 9.9776912e-01, 2.0161823e-03],</span></span><br><span class="line"><span class="comment">#         [4.2676111e-06, 4.8709501e-02, 9.5128632e-01]</span></span><br><span class="line"><span class="comment">#     ]</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>我们可以对结果稍加整理：</p><table><thead><tr><th>SepalLength</th><th>SepalWidth</th><th>PetalLength</th><th>PetalWidth</th><th>ClassID</th><th>Probability</th></tr></thead><tbody><tr><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>0</td><td>0.998268</td></tr><tr><td>5.9</td><td>3.0</td><td>4.2</td><td>1.5</td><td>1</td><td>0.997769</td></tr><tr><td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td>2</td><td>0.951286</td></tr></tbody></table><p>本质上，<code>from_saved_model</code> 方法会使用 <code>saved_model.loader</code> 机制将导出的模型加载到一个 TensorFlow 会话中，读取模型的入参出参信息，生成并组装好相应的 Tensor，最后调用 <code>session.run</code> 来获取结果。对应这个过程，我编写了一段示例代码（<a href="https://github.com/jizhang/tf-serve/blob/master/iris_sess.py" target="_blank" rel="noopener"><code>iris_sess.py</code></a>），读者也可以直接参考 TensorFlow 的源码 <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/predictor/saved_model_predictor.py" target="_blank" rel="noopener"><code>saved_model_predictor.py</code></a>。此外，<a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/tools/saved_model_cli.py" target="_blank" rel="noopener"><code>saved_model_cli</code></a> 命令也使用了同样的方式。</p><h2 id="使用-TensorFlow-Serving-提供服务"><a href="#使用-TensorFlow-Serving-提供服务" class="headerlink" title="使用 TensorFlow Serving 提供服务"></a>使用 TensorFlow Serving 提供服务</h2><p>最后，我们来演示一下如何使用 TensorFlow 的姊妹项目 <a href="https://www.tensorflow.org/serving/" target="_blank" rel="noopener">TensorFlow Serving</a> 来基于 SavedModel 对外提供服务。</p><h3 id="安装并启动-TensorFlow-ModelServer"><a href="#安装并启动-TensorFlow-ModelServer" class="headerlink" title="安装并启动 TensorFlow ModelServer"></a>安装并启动 TensorFlow ModelServer</h3><p>TensorFlow 服务端代码是使用 C++ 开发的，因此最便捷的安装方式是通过软件源来获取编译好的二进制包。读者可以根据 <a href="https://www.tensorflow.org/serving/setup" target="_blank" rel="noopener">官方文档</a> 在 Ubuntu 中配置软件源和安装服务端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install tensorflow-model-server</span><br></pre></td></tr></table></figure><p>然后就可以使用以下命令启动服务端了，该命令会加载导出目录中最新的一份模型来提供服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tensorflow_model_server --port=9000 --model_base_path=/root/<span class="built_in">export</span></span><br><span class="line">2018-05-14 01:05:12.561 Loading SavedModel with tags: &#123; serve &#125;; from: /root/<span class="built_in">export</span>/1524907728</span><br><span class="line">2018-05-14 01:05:12.639 Successfully loaded servable version &#123;name: default version: 1524907728&#125;</span><br><span class="line">2018-05-14 01:05:12.641 Running ModelServer at 0.0.0.0:9000 ...</span><br></pre></td></tr></table></figure><h3 id="使用-SDK-访问远程模型"><a href="#使用-SDK-访问远程模型" class="headerlink" title="使用 SDK 访问远程模型"></a>使用 SDK 访问远程模型</h3><p>TensorFlow Serving 是基于 gRPC 和 Protocol Buffers 开发的，因此我们需要安装相应的 SDK 包来发起调用。需要注意的是，官方的 TensorFlow Serving API 目前只提供了 Python 2.7 版本的 SDK，不过社区有人贡献了支持 Python 3.x 的软件包，我们可以用以下命令安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install tensorflow-seving-api-python3==1.7.0</span><br></pre></td></tr></table></figure><p>调用过程很容易理解：我们首先创建远程连接，向服务端发送 <code>Example</code> 实例列表，并获取预测结果。完整代码可以在 <a href="https://github.com/jizhang/tf-serve/blob/master/iris_remote.py" target="_blank" rel="noopener"><code>iris_remote.py</code></a> 中找到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 gRPC 连接</span></span><br><span class="line">channel = implementations.insecure_channel(<span class="string">'127.0.0.1'</span>, <span class="number">9000</span>)</span><br><span class="line">stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取测试数据集，并转换成 Example 实例。</span></span><br><span class="line">inputs = pd.DateFrame()</span><br><span class="line">examples = [tf.tain.Example() <span class="keyword">for</span> index, row <span class="keyword">in</span> inputs.iterrows()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备 RPC 请求，指定模型名称。</span></span><br><span class="line">request = classification_pb2.ClassificationRequest()</span><br><span class="line">request.model_spec.name = <span class="string">'default'</span></span><br><span class="line">request.input.example_list.examples.extend(examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取结果</span></span><br><span class="line">response = stub.Classify(request, <span class="number">10.0</span>)</span><br><span class="line"><span class="comment"># result &#123;</span></span><br><span class="line"><span class="comment">#   classifications &#123;</span></span><br><span class="line"><span class="comment">#     classes &#123;</span></span><br><span class="line"><span class="comment">#       label: "0"</span></span><br><span class="line"><span class="comment">#       score: 0.998267650604248</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment">#   ...</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.tensorflow.org/get_started/premade_estimators" target="_blank" rel="noopener">https://www.tensorflow.org/get_started/premade_estimators</a></li><li><a href="https://www.tensorflow.org/programmers_guide/saved_model" target="_blank" rel="noopener">https://www.tensorflow.org/programmers_guide/saved_model</a></li><li><a href="https://www.tensorflow.org/serving/" target="_blank" rel="noopener">https://www.tensorflow.org/serving/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow&lt;/a&gt; 是目前最为流行的机器学习框架之一，通过它我们可以便捷地构建机器学习模型。使用 TensorFlow 模型对外提供服务有若干种方式，本文将介绍如何使用 SavedModel 机制来编写模型预测接口。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/tf-logo.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;鸢尾花深层神经网络分类器&quot;&gt;&lt;a href=&quot;#鸢尾花深层神经网络分类器&quot; class=&quot;headerlink&quot; title=&quot;鸢尾花深层神经网络分类器&quot;&gt;&lt;/a&gt;鸢尾花深层神经网络分类器&lt;/h2&gt;&lt;p&gt;首先让我们使用 TensorFlow 的深层神经网络模型来构建一个鸢尾花的分类器。完整的教程可以在 TensorFlow 的官方文档中查看（&lt;a href=&quot;https://www.tensorflow.org/get_started/premade_estimators&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Premade Estimators&lt;/a&gt;），我也提供了一份示例代码，托管在 GitHub 上（&lt;a href=&quot;https://github.com/jizhang/tf-serve/blob/master/iris_dnn.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;iris_dnn.py&lt;/code&gt;&lt;/a&gt;），读者可以克隆到本地进行测试。以下是部分代码摘要：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;feature_columns = [tf.feature_column.numeric_column(key=key)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; train_x.keys()]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;classifier = tf.estimator.DNNClassifier(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    feature_columns=feature_columns,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    hidden_units=[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    n_classes=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;classifier.train(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    input_fn=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt;: train_input_fn(train_x, train_y, batch_size=BATCH_SIZE),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    steps=STEPS)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;predictions = classifier.predict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    input_fn=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt;: eval_input_fn(predict_x, labels=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;, batch_size=BATCH_SIZE))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="machine learning" scheme="http://shzhangji.com/cnblogs/tags/machine-learning/"/>
    
      <category term="tensorflow" scheme="http://shzhangji.com/cnblogs/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>使用 Python 和 Thrift 连接 HBase</title>
    <link href="http://shzhangji.com/cnblogs/2018/04/22/connect-hbase-with-python-and-thrift/"/>
    <id>http://shzhangji.com/cnblogs/2018/04/22/connect-hbase-with-python-and-thrift/</id>
    <published>2018-04-22T12:36:08.000Z</published>
    <updated>2020-08-22T12:06:11.268Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://hbase.apache.org/" target="_blank" rel="noopener">Apache HBase</a> 是 Hadoop 生态环境中的键值存储系统（Key-value Store）。它构建在 HDFS 之上，可以对大型数据进行高速的读写操作。HBase 的开发语言是 Java，因此提供了原生的 Java 语言客户端。不过，借助于 Thrift 和其丰富的语言扩展，我们可以十分便捷地在任何地方调用 HBase 服务。文本将讲述的就是如何使用 Thrift 和 Python 来读写 HBase。</p><p><img src="/cnblogs/images/hbase.png" alt></p><h2 id="生成-Thrift-类定义"><a href="#生成-Thrift-类定义" class="headerlink" title="生成 Thrift 类定义"></a>生成 Thrift 类定义</h2><p>如果你对 <a href="https://thrift.apache.org/" target="_blank" rel="noopener">Apache Thrift</a> 并不熟悉，它提供了一套 IDL（接口描述语言），用于定义远程服务的方法签名和数据类型，并能将其转换成所需要的目标语言。举例来说，以下是用该 IDL 定义的一个数据结构：</p><figure class="highlight thrift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct TColumn &#123;</span><br><span class="line">  1: required binary family,</span><br><span class="line">  2: optional binary qualifier,</span><br><span class="line">  3: optional i64 timestamp</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>转换后的 Python 代码是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TColumn</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, family=None, qualifier=None, timestamp=None,)</span>:</span></span><br><span class="line">        self.family = family</span><br><span class="line">        self.qualifier = qualifier</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self, iprot)</span>:</span></span><br><span class="line">        iprot.readStructBegin()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            (fname, ftype, fid) = iprot.readFieldBegin()</span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, oprot)</span>:</span></span><br><span class="line">        oprot.writeStructBegin(<span class="string">'TColumn'</span>)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="HBase-Thrift-vs-Thrift2"><a href="#HBase-Thrift-vs-Thrift2" class="headerlink" title="HBase Thrift vs Thrift2"></a>HBase Thrift vs Thrift2</h3><p>HBase 提供了 <a href="https://github.com/apache/hbase/tree/master/hbase-thrift/src/main/resources/org/apache/hadoop/hbase" target="_blank" rel="noopener">两个版本</a> 的 IDL 文件，它们有以下两个不同点：</p><p>首先，<code>thrift2</code> 模仿了 HBase Java API 的数据类型和方法定义，调用方式更人性化一些。比如，构建一个 <code>Get</code> 操作的 Java 代码是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(<span class="string">"rowkey"</span>));</span><br><span class="line">get.addColumn(Bytes.toBytes(<span class="string">"cf"</span>), Bytes.toBytes(<span class="string">"col1"</span>));</span><br><span class="line">get.addColumn(Bytes.toBytes(<span class="string">"cf"</span>), Bytes.toBytes(<span class="string">"col2"</span>));</span><br></pre></td></tr></table></figure><p>在 <code>thrift2</code> 中有对应的 <code>TGet</code> 类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tget = TGet(</span><br><span class="line">    row=<span class="string">'rowkey'</span>,</span><br><span class="line">    columns=[</span><br><span class="line">        TColumn(family=<span class="string">'cf'</span>, qualifier=<span class="string">'col1'</span>),</span><br><span class="line">        TColumn(family=<span class="string">'cf'</span>, qualifier=<span class="string">'col2'</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>如果使用旧版的 <code>thrift</code>，我们就需要直接调用其众多的 <code>get</code> 方法之一了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">client.getRowWithColumns(</span><br><span class="line">    tableName=<span class="string">'tbl'</span>,</span><br><span class="line">    row=<span class="string">'rowkey'</span>,</span><br><span class="line">    columns=[<span class="string">'cf:col1'</span>, <span class="string">'cf:col2'</span>],</span><br><span class="line">    attributes=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>第二个不同点在于 <code>thrift2</code> 目前尚缺少 HBase 管理相关的接口，如 <code>createTable</code>、<code>majorCompact</code> 等。这些 API 仍在开发过程中，因此如果你需要通过 Thrift 来建表或维护 HBase，就只能使用旧版的 <code>thrift</code> 了。</p><p>决定了使用哪个版本的描述文件后，我们就可以将 <code>hbase.thrift</code> 下载到本地，通过它来生成 Python 代码。对于 Apache Thrift 本身的版本这里还要强调一点：由于我们使用的是 Python 3.x，而 Thrift 从 0.10 版本才开始支持，因此请确认自己安装了正确的版本。执行以下命令，我们就可以得到一组 Python 文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ thrift -gen py hbase.thrift</span><br><span class="line">$ find gen-py</span><br><span class="line">gen-py/hbase/__init__.py</span><br><span class="line">gen-py/hbase/constants.py</span><br><span class="line">gen-py/hbase/THBaseService.py</span><br><span class="line">gen-py/hbase/ttypes.py</span><br></pre></td></tr></table></figure><h2 id="在单机模式下运行-HBase"><a href="#在单机模式下运行-HBase" class="headerlink" title="在单机模式下运行 HBase"></a>在单机模式下运行 HBase</h2><p>如果你手边没有可供测试的 HBase 服务，可以根据官网上的快速开始指引（<a href="https://hbase.apache.org/book.html#quickstart" target="_blank" rel="noopener">链接</a>），下载 HBase 二进制包，做一下简单的配合，并执行下列命令来启动 HBase 服务及 Thrift2 Server。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/hbase-daemon.sh start thrift2</span><br><span class="line">bin/hbase shell</span><br></pre></td></tr></table></figure><p>进入 HBase 命令行后，我们可以创建一个测试表，并尝试读写数据：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; create <span class="string">"tsdata"</span>, NAME =&gt; <span class="string">"cf"</span></span><br><span class="line">&gt; put <span class="string">"tsdata"</span>, <span class="string">"sys.cpu.user:20180421:192.168.1.1"</span>, <span class="string">"cf:1015"</span>, <span class="string">"0.28"</span></span><br><span class="line">&gt; get <span class="string">"tsdata"</span>, <span class="string">"sys.cpu.user:20180421:192.168.1.1"</span></span><br><span class="line">COLUMN                                        CELL</span><br><span class="line"> <span class="symbol">cf:</span><span class="number">1015</span>                                      timestamp=<span class="number">1524277135973</span>, value=<span class="number">0</span>.<span class="number">28</span></span><br><span class="line"><span class="number">1</span> row(s) <span class="keyword">in</span> <span class="number">0</span>.<span class="number">0330</span> seconds</span><br></pre></td></tr></table></figure><h2 id="通过-Thrift2-Server-连接-HBase"><a href="#通过-Thrift2-Server-连接-HBase" class="headerlink" title="通过 Thrift2 Server 连接 HBase"></a>通过 Thrift2 Server 连接 HBase</h2><p>以下是创建 Thrift 连接的样板代码。需要注意的是，Thrift 客户端并不是线程安全的，因此无法在多个线程间共享。而且，它也没有提供类似连接池的特性。通常我们会选择每次查询都创建新的连接，当然你也可以引入自己的连接池机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"></span><br><span class="line">transport = TTransport.TBufferedTransport(TSocket.TSocket(<span class="string">'127.0.0.1'</span>, <span class="number">9090</span>))</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line">transport.open()</span><br><span class="line"><span class="comment"># 使用 client 实例进行操作</span></span><br><span class="line">transport.close()</span><br></pre></td></tr></table></figure><p>我们来尝试编写几个基本的读写操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hbase.ttypes <span class="keyword">import</span> TPut, TColumnValue, TGet</span><br><span class="line">tput = TPut(</span><br><span class="line">    row=<span class="string">'sys.cpu.user:20180421:192.168.1.1'</span>,</span><br><span class="line">    columnValues=[</span><br><span class="line">        TColumnValue(family=<span class="string">'cf'</span>, qualifier=<span class="string">'1015'</span>, value=<span class="string">'0.28'</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">client.put(<span class="string">'tsdata'</span>, tput)</span><br><span class="line"></span><br><span class="line">tget = TGet(row=<span class="string">'sys.cpu.user:20180421:192.168.1.1'</span>)</span><br><span class="line">tresult = client.get(<span class="string">'tsdata'</span>, tget)</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tresult.columnValues:</span><br><span class="line">    print(col.qualifier, <span class="string">'='</span>, col.value)</span><br></pre></td></tr></table></figure><h2 id="Thrift2-数据类型和方法一览"><a href="#Thrift2-数据类型和方法一览" class="headerlink" title="Thrift2 数据类型和方法一览"></a>Thrift2 数据类型和方法一览</h2><p>完整的方法列表可以直接查阅 <code>hbase.thrift</code> 和 <code>hbase/THBaseService.py</code> 这两个文件。下面是对常用方法的总结：</p><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><table><thead><tr><th>类名</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>TColumn</td><td>表示一个列族或单个列。</td><td>TColumn(family=’cf’, qualifier=’gender’)</td></tr><tr><td>TColumnValue</td><td>列名及其包含的值。</td><td>TColumnValue(family=’cf’, qualifier=’gender’, value=’male’)</td></tr><tr><td>TResult</td><td>查询结果（一行）。若 <code>row</code> 属性的值为 <code>None</code>，则表示查无结果。</td><td>TResult(row=’employee_001’, columnValues=[TColumnValue])</td></tr><tr><td>TGet</td><td>查询单行。</td><td>TGet(row=’employee_001’, columns=[TColumn])</td></tr><tr><td>TPut</td><td>修改一行数据.</td><td>TPut(row=’employee_001’, columnValues=[TColumnValue])</td></tr><tr><td>TDelete</td><td>删除整行或部分列。</td><td>TDelete(row=’employee_001’, columns=[TColumn])</td></tr><tr><td>TScan</td><td>扫描多行数据。</td><td>见下文</td></tr></tbody></table><h3 id="THBaseService-类方法"><a href="#THBaseService-类方法" class="headerlink" title="THBaseService 类方法"></a>THBaseService 类方法</h3><table><thead><tr><th>方法签名</th><th>描述</th></tr></thead><tbody><tr><td>get(table: str, tget: TGet) -&gt; TResult</td><td>查询单行。</td></tr><tr><td>getMultiple(table: str, tgets: List[TGet]) -&gt; List[TResult]</td><td>查询多行。</td></tr><tr><td>put(table: str, tput: TPut) -&gt; None</td><td>修改单行。</td></tr><tr><td>putMultiple(table: str, tputs: List[TPut]) -&gt; None</td><td>修改多行。</td></tr><tr><td>deleteSingle(table: str, tdelete: TDelete) -&gt; None</td><td>删除单行。</td></tr><tr><td>deleteMultiple(table: str, tdeletes: List[TDelete]) -&gt; None</td><td>删除多行。</td></tr><tr><td>openScanner(table: str, tscan: TScan) -&gt; int</td><td>打开一个扫描器，返回其唯一标识。</td></tr><tr><td>getScannerRows(scannerId: int, numRows: int) -&gt; List[TResult]</td><td>返回扫描结果。</td></tr><tr><td>closeScanner(scannerId: int) -&gt; None</td><td>关闭扫描器。</td></tr><tr><td>getScannerResults(table: str, tscan: TScan, numRows: int) -&gt; List[TResult]</td><td>直接获取扫描结果的快捷方法。</td></tr></tbody></table><h3 id="Scan-操作示例"><a href="#Scan-操作示例" class="headerlink" title="Scan 操作示例"></a>Scan 操作示例</h3><p>我在 GitHub（<a href="https://github.com/jizhang/python-hbase" target="_blank" rel="noopener">链接</a>）上放置了一些样例代码，以下是 <code>Scan</code> 操作的样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scanner_id = client.openScanner(</span><br><span class="line">    table=<span class="string">'tsdata'</span>,</span><br><span class="line">    tscan=TScan(</span><br><span class="line">        startRow=<span class="string">'sys.cpu.user:20180421'</span>,</span><br><span class="line">        stopRow=<span class="string">'sys.cpu.user:20180422'</span>,</span><br><span class="line">        columns=[TColumn(<span class="string">'cf'</span>, <span class="string">'1015'</span>)]</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    num_rows = <span class="number">10</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tresults = client.getScannerRows(scanner_id, num_rows)</span><br><span class="line">        <span class="keyword">for</span> tresult <span class="keyword">in</span> tresults:</span><br><span class="line">            print(tresult)</span><br><span class="line">        <span class="keyword">if</span> len(tresults) &lt; num_rows:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    client.closeScanner(scanner_id)</span><br></pre></td></tr></table></figure><h2 id="Thrift-Server-高可用"><a href="#Thrift-Server-高可用" class="headerlink" title="Thrift Server 高可用"></a>Thrift Server 高可用</h2><p>Thrift Server 的单点问题有几种解决方案：</p><ol><li>在客户端中配置多个 Thrift Server 地址，发送请求时随机选择一个，并做好错误重试；</li><li>搭建代理，对 TCP 连接做负载均衡；</li><li>在客户端服务器上配置独立的 Thrift Server，每个客户端直接创建本地连接。</li></ol><p>通常我们会选择第二种方案，这就需要和运维工程师一起配合搭建了。</p><p><img src="/cnblogs/images/hbase-thrift-ha.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://blog.cloudera.com/blog/2013/09/how-to-use-the-hbase-thrift-interface-part-1/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2013/09/how-to-use-the-hbase-thrift-interface-part-1/</a></li><li><a href="https://thrift.apache.org/tutorial/py" target="_blank" rel="noopener">https://thrift.apache.org/tutorial/py</a></li><li><a href="https://yq.aliyun.com/articles/88299" target="_blank" rel="noopener">https://yq.aliyun.com/articles/88299</a></li><li><a href="http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html" target="_blank" rel="noopener">http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://hbase.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache HBase&lt;/a&gt; 是 Hadoop 生态环境中的键值存储系统（Key-value Store）。它构建在 HDFS 之上，可以对大型数据进行高速的读写操作。HBase 的开发语言是 Java，因此提供了原生的 Java 语言客户端。不过，借助于 Thrift 和其丰富的语言扩展，我们可以十分便捷地在任何地方调用 HBase 服务。文本将讲述的就是如何使用 Thrift 和 Python 来读写 HBase。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/hbase.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;生成-Thrift-类定义&quot;&gt;&lt;a href=&quot;#生成-Thrift-类定义&quot; class=&quot;headerlink&quot; title=&quot;生成 Thrift 类定义&quot;&gt;&lt;/a&gt;生成 Thrift 类定义&lt;/h2&gt;&lt;p&gt;如果你对 &lt;a href=&quot;https://thrift.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Thrift&lt;/a&gt; 并不熟悉，它提供了一套 IDL（接口描述语言），用于定义远程服务的方法签名和数据类型，并能将其转换成所需要的目标语言。举例来说，以下是用该 IDL 定义的一个数据结构：&lt;/p&gt;
&lt;figure class=&quot;highlight thrift&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;struct TColumn &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  1: required binary family,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  2: optional binary qualifier,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  3: optional i64 timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;转换后的 Python 代码是：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TColumn&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, family=None, qualifier=None, timestamp=None,)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.family = family&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.qualifier = qualifier&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.timestamp = timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, iprot)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        iprot.readStructBegin()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            (fname, ftype, fid) = iprot.readFieldBegin()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;comment&quot;&gt;# ...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, oprot)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        oprot.writeStructBegin(&lt;span class=&quot;string&quot;&gt;&#39;TColumn&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# ...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="hbase" scheme="http://shzhangji.com/cnblogs/tags/hbase/"/>
    
      <category term="thrift" scheme="http://shzhangji.com/cnblogs/tags/thrift/"/>
    
  </entry>
  
  <entry>
    <title>Vuex 严格模式下的表单处理</title>
    <link href="http://shzhangji.com/cnblogs/2018/04/18/form-handling-in-vuex-strict-mode/"/>
    <id>http://shzhangji.com/cnblogs/2018/04/18/form-handling-in-vuex-strict-mode/</id>
    <published>2018-04-18T01:08:41.000Z</published>
    <updated>2020-08-22T12:06:11.268Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/cnblogs/images/vue.png" alt></p><p>在使用 Vue 进行表单处理时，我们通常会使用 <code>v-model</code> 来建立双向绑定。但是，如果将表单数据交由 Vuex 管理，这时的双向绑定就会引发问题，因为在 <strong>严格模式</strong> 下，Vuex 是不允许在 Mutation 之外的地方修改状态数据的。以下用一个简单的项目举例说明，完整代码可在 GitHub（<a href="https://github.com/jizhang/vuex-form" target="_blank" rel="noopener">链接</a>） 查看。</p><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  state: &#123;</span><br><span class="line">    namespaced: <span class="literal">true</span>,</span><br><span class="line">    table: &#123;</span><br><span class="line">      table_name: <span class="string">''</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>src/components/NonStrict.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-group</span> <span class="attr">label</span>=<span class="string">"表名："</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"table.table_name"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">b-form-group</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">import</span> &#123; mapState &#125; <span class="keyword">from</span> <span class="string">'vuex'</span></span></span><br><span class="line"></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line"><span class="javascript">    ...mapState(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'table'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>当我们在“表名”字段输入文字时，浏览器会报以下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">错误：[vuex] 禁止在 Mutation 之外修改 Vuex 状态数据。</span><br><span class="line">    at assert (vuex.esm.js?358c:97)</span><br><span class="line">    at Vue.store._vm.$watch.deep (vuex.esm.js?358c:746)</span><br><span class="line">    at Watcher.run (vue.esm.js?efeb:3233)</span><br></pre></td></tr></table></figure><p>当然，我们可以选择不开启严格模式，只是这样就无法通过工具追踪到每一次的状态变动了。下面我将列举几种解决方案，描述如何在严格模式下进行表单处理。</p><a id="more"></a><h2 id="将状态复制到组件中"><a href="#将状态复制到组件中" class="headerlink" title="将状态复制到组件中"></a>将状态复制到组件中</h2><p>第一种方案是直接将 Vuex 中的表单数据复制到本地的组件状态中，并在表单和本地状态间建立双向绑定。当用户提交表单时，再将本地数据提交到 Vuex 状态库中。</p><p><code>src/components/LocalCopy.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"table.table_name"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">import</span> _ <span class="keyword">from</span> <span class="string">'lodash'</span></span></span><br><span class="line"></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  data () &#123;</span><br><span class="line"><span class="javascript">    <span class="keyword">return</span> &#123;</span></span><br><span class="line"><span class="javascript">      table: _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line">    handleSubmit (event) &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">this</span>.$store.commit(<span class="string">'table/setTable'</span>, <span class="keyword">this</span>.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  mutations: &#123;</span><br><span class="line">    setTable (state, payload) &#123;</span><br><span class="line">      state.table = payload</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上方式有两个缺陷。其一，在提交状态更新后，若继续修改表单数据，同样会得到“禁止修改”的错误提示。这是因为 <code>setTable</code> 方法将本地状态对象直接传入了 Vuex，我们可以对该方法稍作修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">setTable (state, payload) &#123;</span><br><span class="line">  <span class="comment">// 将对象属性逐一赋值给 Vuex</span></span><br><span class="line">  _.assign(state.table, payload)</span><br><span class="line">  <span class="comment">// 或者，克隆整个对象</span></span><br><span class="line">  state.table = _.cloneDeep(payload)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二个问题在于如果其他组件也向 Vuex 提交了数据变动（如弹出的对话框中包含了一个子表单），当前表单的数据不会得到更新。这时，我们就需要用到 Vue 的监听机制了：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  data () &#123;</span><br><span class="line"><span class="javascript">    <span class="keyword">return</span> &#123;</span></span><br><span class="line"><span class="javascript">      table: _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  computed: &#123;</span><br><span class="line">    storeTable () &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">return</span> _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  watch: &#123;</span><br><span class="line">    storeTable (newValue) &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">this</span>.table = newValue</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个方法还能同时规避第一个问题，因为每当 Vuex 数据更新，本地组件都会重新克隆一份数据。</p><h2 id="响应表单更新事件并提交数据"><a href="#响应表单更新事件并提交数据" class="headerlink" title="响应表单更新事件并提交数据"></a>响应表单更新事件并提交数据</h2><p>一种类似 ReactJS 的做法是，弃用 <code>v-model</code>，转而使用 <code>:value</code> 展示数据，再通过监听 <code>@input</code> 或 <code>@change</code> 事件来提交数据变更。这样就从双向绑定转换为了单向数据流，Vuex 状态库自此成为整个应用程序的唯一数据源（Single Source of Truth）。</p><p><code>src/components/ExplicitUpdate.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">:value</span>=<span class="string">"table.table_name"</span> @<span class="attr">input</span>=<span class="string">"updateTableForm(&#123; table_name: $event &#125;)"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line"><span class="javascript">    ...mapState(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'table'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line"><span class="javascript">    ...mapMutations(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'updateTableForm'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> table &#123;</span><br><span class="line">  mutations: &#123;</span><br><span class="line">    updateTableForm (state, payload) &#123;</span><br><span class="line">      _.assign(state.table, payload)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上方法也是 <a href="https://vuex.vuejs.org/en/forms.html" target="_blank" rel="noopener">Vuex 文档</a> 所推崇的。而根据 <a href="https://vuejs.org/v2/guide/forms.html" target="_blank" rel="noopener">Vue 文档</a> 的介绍，<code>v-model</code> 本质上也是一个“监听 - 修改”流程的语法糖而已。</p><h2 id="使用-Vue-计算属性"><a href="#使用-Vue-计算属性" class="headerlink" title="使用 Vue 计算属性"></a>使用 Vue 计算属性</h2><p>Vue 的计算属性（Computed Property）可以配置双向的访问器（Getter / Setter），我们可以利用其建立起 Vuex 状态库和本地组件间的桥梁。其中一个限制在于计算属性无法支持嵌套属性（<code>table.table_name</code>），因此我们需要为这些属性设置别名。</p><p><code>src/components/ComputedProperty.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"tableName"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">b-form-select</span> <span class="attr">v-model</span>=<span class="string">"tableCategory"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line">    tableName: &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">get</span> () &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">return</span> <span class="keyword">this</span>.$store.state.table.table.table_name</span></span><br><span class="line">      &#125;,</span><br><span class="line"><span class="javascript">      <span class="keyword">set</span> (value) &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">this</span>.updateTableForm(&#123; <span class="attr">table_name</span>: value &#125;)</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    tableCategory: &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">get</span> () &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">return</span> <span class="keyword">this</span>.$store.state.table.table.category</span></span><br><span class="line">      &#125;,</span><br><span class="line"><span class="javascript">      <span class="keyword">set</span> (value) &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">this</span>.updateTableForm(&#123; <span class="attr">category</span>: value &#125;)</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line"><span class="javascript">    ...mapMutations(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'updateTableForm'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果表单字段数目过多，全部列出不免有些繁琐，我们可以创建一些工具函数来实现。首先，在 Vuex 状态库中新增一个可修改任意属性的 Mutation，它接收一个 Lodash 风格的属性路径。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mutations: &#123;</span><br><span class="line">  myUpdateField (state, payload) &#123;</span><br><span class="line">    <span class="keyword">const</span> &#123; path, value &#125; = payload</span><br><span class="line">    _.set(state, path, value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在组件中，我们将传入的“别名 - 路径”对转换成相应的 Getter / Setter 访问器。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mapFields = <span class="function">(<span class="params">namespace, fields</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> _.mapValues(fields, path =&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="keyword">get</span> () &#123;</span><br><span class="line">        <span class="keyword">return</span> _.get(<span class="keyword">this</span>.$store.state[namespace], path)</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">set</span> (value) &#123;</span><br><span class="line">        <span class="keyword">this</span>.$store.commit(<span class="string">`<span class="subst">$&#123;namespace&#125;</span>/myUpdateField`</span>, &#123; path, value &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  computed: &#123;</span><br><span class="line">    ...mapFields(<span class="string">'table'</span>, &#123;</span><br><span class="line">      tableName: <span class="string">'table.table_name'</span>,</span><br><span class="line">      tableCategory: <span class="string">'table.category'</span>,</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>开源社区中已经有人建立了一个名为 <a href="https://github.com/maoberlehner/vuex-map-fields" target="_blank" rel="noopener">vuex-map-fields</a> 的项目，其 <code>mapFields</code> 方法就实现了上述功能。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://vuex.vuejs.org/en/forms.html" target="_blank" rel="noopener">https://vuex.vuejs.org/en/forms.html</a></li><li><a href="https://ypereirareis.github.io/blog/2017/04/25/vuejs-two-way-data-binding-state-management-vuex-strict-mode/" target="_blank" rel="noopener">https://ypereirareis.github.io/blog/2017/04/25/vuejs-two-way-data-binding-state-management-vuex-strict-mode/</a></li><li><a href="https://markus.oberlehner.net/blog/form-fields-two-way-data-binding-and-vuex/" target="_blank" rel="noopener">https://markus.oberlehner.net/blog/form-fields-two-way-data-binding-and-vuex/</a></li><li><a href="https://forum.vuejs.org/t/vuex-form-best-practices/20084" target="_blank" rel="noopener">https://forum.vuejs.org/t/vuex-form-best-practices/20084</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/cnblogs/images/vue.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;在使用 Vue 进行表单处理时，我们通常会使用 &lt;code&gt;v-model&lt;/code&gt; 来建立双向绑定。但是，如果将表单数据交由 Vuex 管理，这时的双向绑定就会引发问题，因为在 &lt;strong&gt;严格模式&lt;/strong&gt; 下，Vuex 是不允许在 Mutation 之外的地方修改状态数据的。以下用一个简单的项目举例说明，完整代码可在 GitHub（&lt;a href=&quot;https://github.com/jizhang/vuex-form&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;） 查看。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;src/store/table.js&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  state: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    namespaced: &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    table: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      table_name: &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;src/components/NonStrict.vue&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;b-form-group&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;label&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;表名：&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;b-form-input&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;v-model&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;table.table_name&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;b-form-group&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; &amp;#123; mapState &amp;#125; &lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;vuex&#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  computed: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;    ...mapState(&lt;span class=&quot;string&quot;&gt;&#39;table&#39;&lt;/span&gt;, [&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;      &lt;span class=&quot;string&quot;&gt;&#39;table&#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;当我们在“表名”字段输入文字时，浏览器会报以下错误：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;错误：[vuex] 禁止在 Mutation 之外修改 Vuex 状态数据。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at assert (vuex.esm.js?358c:97)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at Vue.store._vm.$watch.deep (vuex.esm.js?358c:746)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at Watcher.run (vue.esm.js?efeb:3233)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;当然，我们可以选择不开启严格模式，只是这样就无法通过工具追踪到每一次的状态变动了。下面我将列举几种解决方案，描述如何在严格模式下进行表单处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="javascript" scheme="http://shzhangji.com/cnblogs/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/cnblogs/tags/frontend/"/>
    
      <category term="vue" scheme="http://shzhangji.com/cnblogs/tags/vue/"/>
    
      <category term="vuex" scheme="http://shzhangji.com/cnblogs/tags/vuex/"/>
    
  </entry>
  
  <entry>
    <title>RESTful API 中的错误处理</title>
    <link href="http://shzhangji.com/cnblogs/2018/04/07/error-handling-in-restful-api/"/>
    <id>http://shzhangji.com/cnblogs/2018/04/07/error-handling-in-restful-api/</id>
    <published>2018-04-07T06:49:19.000Z</published>
    <updated>2020-08-22T12:06:11.267Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/cnblogs/images/restful-api.png" alt="RESTful API"></p><p>构建 Web 服务时，我们会使用 RESTful API 来实现组件间的通信，特别是在现今前后端分离的技术背景下。REST 是一种基于 HTTP 协议的通信方式，它简单、基于文本、且在各种语言、浏览器及客户端软件中能得到很好的支持。然而，REST 目前并没有一个普遍接受的标准，因此开发者需要自行决定 API 的设计，其中一项决策就是错误处理。比如我们是否应该使用 HTTP 状态码来标识错误？如何返回表单验证的结果等等。以下这篇文章是基于日常使用中的经验总结的一套错误处理流程，供读者们参考。</p><h2 id="错误的分类"><a href="#错误的分类" class="headerlink" title="错误的分类"></a>错误的分类</h2><p>错误可以分为两种类型：全局错误和本地错误。全局错误包括：请求了一个不存在的 API、无权请求这个 API、数据库连接失败、或其他一些没有预期到的、会终止程序运行的服务端错误。这类错误应该由 Web 框架捕获，无需各个 API 处理。</p><p>本地错误则和 API 密切相关，例如表单验证、唯一性检查、或其他可预期的错误。我们需要编写特定代码来捕获这类错误，并抛出一个包含提示信息的全局异常，供 Web 框架捕获并返回给客户端。</p><p>例如，Flask 框架就提供了此类全局异常处理机制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BadRequest</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="string">"""将本地错误包装成一个异常实例供抛出"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, message, status=<span class="number">400</span>, payload=None)</span>:</span></span><br><span class="line">        self.message = message</span><br><span class="line">        self.status = status</span><br><span class="line">        self.payload = payload</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.errorhandler(BadRequest)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_bad_request</span><span class="params">(error)</span>:</span></span><br><span class="line">    <span class="string">"""捕获 BadRequest 全局异常，序列化为 JSON 并返回 HTTP 400"""</span></span><br><span class="line">    payload = dict(error.payload <span class="keyword">or</span> ())</span><br><span class="line">    payload[<span class="string">'status'</span>] = error.status</span><br><span class="line">    payload[<span class="string">'message'</span>] = error.message</span><br><span class="line">    <span class="keyword">return</span> jsonify(payload), <span class="number">400</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/person', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person_post</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""创建用户的 API，成功则返回用户 ID"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.form.get(<span class="string">'username'</span>):</span><br><span class="line">        <span class="keyword">raise</span> BadRequest(<span class="string">'用户名不能为空'</span>, <span class="number">40001</span>, &#123; <span class="string">'ext'</span>: <span class="number">1</span> &#125;)</span><br><span class="line">    <span class="keyword">return</span> jsonify(last_insert_id=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="返回的错误内容"><a href="#返回的错误内容" class="headerlink" title="返回的错误内容"></a>返回的错误内容</h2><p>上例中，如果向 <code>/person</code> API 发送一个 <code>username</code> 为空的请求，会返回以下错误结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HTTP/1.1 400 Bad Request</span><br><span class="line">Content-Type: application/json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;status&quot;: 40001,</span><br><span class="line">  &quot;message&quot;: &quot;用户名不能为空&quot;,</span><br><span class="line">  &quot;ext&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它包括以下几个部分：HTTP 状态码、自定义错误码、错误提示、以及额外信息。</p><h3 id="正确使用-HTTP-状态码"><a href="#正确使用-HTTP-状态码" class="headerlink" title="正确使用 HTTP 状态码"></a>正确使用 HTTP 状态码</h3><p>HTTP 协议中预定义了丰富的状态码，其中 <code>4xx</code> 表示客户端造成的异常，<code>5xx</code> 表示服务端产生的异常。以下是我们在 API 中经常用到的几种状态码：</p><ul><li><code>200</code> 响应结果正常；</li><li><code>400</code> 错误的请求，如用户提交了非法的数据；</li><li><code>401</code> 未授权的请求。在使用 <code>Flask-Login</code> 插件时，如果 API 的路由含有 <code>@login_required</code> 装饰器，当用户没有登录时就会返回这个错误码，而客户端通常会重定向到登录页面；</li><li><code>403</code> 禁止请求；</li><li><code>404</code> 请求的内容不存在；</li><li><code>500</code> 服务器内部错误，通常是未预期到的、不可恢复的服务端异常。</li></ul><h3 id="自定义错误码"><a href="#自定义错误码" class="headerlink" title="自定义错误码"></a>自定义错误码</h3><p>客户端接收到异常后，可以选择弹出一个全局的错误提示，告知用户请求异常；或者在发起 API 请求的方法内部进行处理，如将表单验证的错误提示展示到各个控件之后。为了实现这一点，我们需要给错误进行编码，如 <code>400</code> 表示通用的全局错误，可直接弹框提示；<code>40001</code>、<code>40002</code> 则表示这类错误需要单独做处理。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fetch().then(<span class="function"><span class="params">response</span> =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (response.status == <span class="number">400</span>) &#123; <span class="comment">// HTTP 状态码</span></span><br><span class="line">    response.json().then(<span class="function"><span class="params">responseJson</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (responseJson.status == <span class="number">400</span>) &#123; <span class="comment">// 自定义错误码</span></span><br><span class="line">        <span class="comment">// 全局错误处理</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (responseJson.status == <span class="number">40001</span>) &#123; <span class="comment">// 自定义错误码</span></span><br><span class="line">        <span class="comment">// 自定义错误处理</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="错误详情"><a href="#错误详情" class="headerlink" title="错误详情"></a>错误详情</h3><p>有时我们会将表单内所有字段的验证错误信息一并返回给客户端，这时就可以使用 <code>payload</code> 机制：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"status"</span>: <span class="number">40001</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"表单验证错误"</span></span><br><span class="line">  <span class="string">"errors"</span>: [</span><br><span class="line">    &#123; <span class="string">"name"</span>: <span class="string">"username"</span>, <span class="string">"error"</span>: <span class="string">"用户名不能为空"</span> &#125;,</span><br><span class="line">    &#123; <span class="string">"name"</span>: <span class="string">"password"</span>, <span class="string">"error"</span>: <span class="string">"密码不能少于 6 位"</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Fetch-API"><a href="#Fetch-API" class="headerlink" title="Fetch API"></a>Fetch API</h2><p>对于 AJAX 请求，<a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API" target="_blank" rel="noopener">Fetch API</a> 已经逐渐成为业界标准。我们可以将其包装成一个方法，对请求结果进行错误处理。完整的代码可以在 GitHub （<a href="https://github.com/jizhang/rest-error/blob/master/src/request.js" target="_blank" rel="noopener">链接</a>）中查看。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">request</span>(<span class="params">url, args, form</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> fetch(url, config)</span><br><span class="line">    .then(<span class="function"><span class="params">response</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (response.ok) &#123;</span><br><span class="line">        <span class="keyword">return</span> response.json()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (response.status === <span class="number">400</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> response.json()</span><br><span class="line">          .then(<span class="function"><span class="params">responseJson</span> =&gt;</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (responseJson.status === <span class="number">400</span>) &#123;</span><br><span class="line">              alert(responseJson.message) <span class="comment">// 全局错误处理</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 抛出异常，让 Promise 下游的 "catch()" 方法进行捕获</span></span><br><span class="line">            <span class="keyword">throw</span> responseJson</span><br><span class="line">          &#125;, error =&gt; &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(<span class="number">400</span>)</span><br><span class="line">          &#125;)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 处理预定义的 HTTP 错误码</span></span><br><span class="line">      <span class="keyword">switch</span> (response.status) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">401</span>:</span><br><span class="line">          <span class="keyword">break</span> <span class="comment">// 重定向至登录页面</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">          alert(<span class="string">'HTTP Status Code '</span> + response.status)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(response.status)</span><br><span class="line">    &#125;, error =&gt; &#123;</span><br><span class="line">      alert(error.message)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(<span class="number">0</span>, error.message)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，异常发生后，该函数会拒绝（reject）这个 Promise，从而由调用方进一步判断 <code>status</code> 来决定处理方式。以下是使用 MobX + ReactJS 实现的自定义错误处理流程：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MobX Store</span></span><br><span class="line">loginUser = flow(<span class="function"><span class="keyword">function</span>* <span class="title">loginUser</span>(<span class="params">form</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.loading = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// yield 语句可能会抛出异常，即拒绝当前的 Promise</span></span><br><span class="line">    <span class="keyword">this</span>.userId = <span class="keyword">yield</span> request(<span class="string">'/login'</span>, <span class="literal">null</span>, form)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.loading = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// React Component</span></span><br><span class="line">login = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">  userStore.loginUser(<span class="keyword">this</span>.state.form)</span><br><span class="line">    .catch(<span class="function"><span class="params">error</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (error.status === <span class="number">40001</span>) &#123;</span><br><span class="line">        <span class="comment">// 自定义错误处理</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://en.wikipedia.org/wiki/Representational_state_transfer" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Representational_state_transfer</a></li><li><a href="https://alidg.me/blog/2016/9/24/rest-api-error-handling" target="_blank" rel="noopener">https://alidg.me/blog/2016/9/24/rest-api-error-handling</a></li><li><a href="https://www.wptutor.io/web/js/generators-coroutines-async-javascript" target="_blank" rel="noopener">https://www.wptutor.io/web/js/generators-coroutines-async-javascript</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/cnblogs/images/restful-api.png&quot; alt=&quot;RESTful API&quot;&gt;&lt;/p&gt;
&lt;p&gt;构建 Web 服务时，我们会使用 RESTful API 来实现组件间的通信，特别是在现今前后端分离的技术背景下。REST 是一种基于 HTTP 协议的通信方式，它简单、基于文本、且在各种语言、浏览器及客户端软件中能得到很好的支持。然而，REST 目前并没有一个普遍接受的标准，因此开发者需要自行决定 API 的设计，其中一项决策就是错误处理。比如我们是否应该使用 HTTP 状态码来标识错误？如何返回表单验证的结果等等。以下这篇文章是基于日常使用中的经验总结的一套错误处理流程，供读者们参考。&lt;/p&gt;
&lt;h2 id=&quot;错误的分类&quot;&gt;&lt;a href=&quot;#错误的分类&quot; class=&quot;headerlink&quot; title=&quot;错误的分类&quot;&gt;&lt;/a&gt;错误的分类&lt;/h2&gt;&lt;p&gt;错误可以分为两种类型：全局错误和本地错误。全局错误包括：请求了一个不存在的 API、无权请求这个 API、数据库连接失败、或其他一些没有预期到的、会终止程序运行的服务端错误。这类错误应该由 Web 框架捕获，无需各个 API 处理。&lt;/p&gt;
&lt;p&gt;本地错误则和 API 密切相关，例如表单验证、唯一性检查、或其他可预期的错误。我们需要编写特定代码来捕获这类错误，并抛出一个包含提示信息的全局异常，供 Web 框架捕获并返回给客户端。&lt;/p&gt;
&lt;p&gt;例如，Flask 框架就提供了此类全局异常处理机制：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BadRequest&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Exception)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;将本地错误包装成一个异常实例供抛出&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, message, status=&lt;span class=&quot;number&quot;&gt;400&lt;/span&gt;, payload=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.message = message&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.status = status&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.payload = payload&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@app.errorhandler(BadRequest)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;handle_bad_request&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(error)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;捕获 BadRequest 全局异常，序列化为 JSON 并返回 HTTP 400&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload = dict(error.payload &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; ())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload[&lt;span class=&quot;string&quot;&gt;&#39;status&#39;&lt;/span&gt;] = error.status&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload[&lt;span class=&quot;string&quot;&gt;&#39;message&#39;&lt;/span&gt;] = error.message&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; jsonify(payload), &lt;span class=&quot;number&quot;&gt;400&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@app.route(&#39;/person&#39;, methods=[&#39;POST&#39;])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;person_post&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;创建用户的 API，成功则返回用户 ID&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; request.form.get(&lt;span class=&quot;string&quot;&gt;&#39;username&#39;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; BadRequest(&lt;span class=&quot;string&quot;&gt;&#39;用户名不能为空&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;40001&lt;/span&gt;, &amp;#123; &lt;span class=&quot;string&quot;&gt;&#39;ext&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; jsonify(last_insert_id=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="javascript" scheme="http://shzhangji.com/cnblogs/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/cnblogs/tags/frontend/"/>
    
      <category term="restful" scheme="http://shzhangji.com/cnblogs/tags/restful/"/>
    
  </entry>
  
  <entry>
    <title>Flume 源码解析：组件生命周期</title>
    <link href="http://shzhangji.com/cnblogs/2017/10/24/flume-source-code-component-lifecycle/"/>
    <id>http://shzhangji.com/cnblogs/2017/10/24/flume-source-code-component-lifecycle/</id>
    <published>2017-10-24T01:18:26.000Z</published>
    <updated>2020-08-22T12:06:11.267Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://flume.apache.org/" target="_blank" rel="noopener">Apache Flume</a> 是数据仓库体系中用于做实时 ETL 的工具。它提供了丰富的数据源和写入组件，这些组件在运行时都由 Flume 的生命周期管理机制进行监控和维护。本文将对这部分功能的源码进行解析。</p><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>Flume 的源码可以从 GitHub 上下载。它是一个 Maven 项目，我们将其导入到 IDE 中以便更好地进行源码阅读。以下是代码仓库的基本结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/flume-ng-node</span><br><span class="line">/flume-ng-code</span><br><span class="line">/flume-ng-sdk</span><br><span class="line">/flume-ng-sources/flume-kafka-source</span><br><span class="line">/flume-ng-channels/flume-kafka-channel</span><br><span class="line">/flume-ng-sinks/flume-hdfs-sink</span><br></pre></td></tr></table></figure><h2 id="程序入口"><a href="#程序入口" class="headerlink" title="程序入口"></a>程序入口</h2><p>Flume Agent 的入口 <code>main</code> 函数位于 <code>flume-ng-node</code> 模块的 <code>org.apache.flume.node.Application</code> 类中。下列代码是该函数的摘要：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    CommandLineParser parser = <span class="keyword">new</span> GnuParser();</span><br><span class="line">    <span class="keyword">if</span> (isZkConfigured) &#123;</span><br><span class="line">      <span class="keyword">if</span> (reload) &#123;</span><br><span class="line">        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;</span><br><span class="line">        components.add(zookeeperConfigurationProvider);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;</span><br><span class="line">        application.handleConfigurationEvent();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// PropertiesFileConfigurationProvider</span></span><br><span class="line">    &#125;</span><br><span class="line">    application.start();</span><br><span class="line">    Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="string">"agent-shutdown-hook"</span>) &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        appReference.stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动过程说明如下：</p><ol><li>使用 <code>commons-cli</code> 对命令行参数进行解析，提取 Agent 名称、配置信息读取方式及其路径信息；</li><li>配置信息可以通过文件或 ZooKeeper 的方式进行读取，两种方式都支持热加载，即我们不需要重启 Agent 就可以更新配置内容：<ul><li>基于文件的配置热加载是通过一个后台线程对文件进行轮询实现的；</li><li>基于 ZooKeeper 的热加载则是使用了 Curator 的 <code>NodeCache</code> 模式，底层是 ZooKeeper 原生的监听（Watch）特性。</li></ul></li><li>如果配置热更新是开启的（默认开启），配置提供方 <code>ConfigurationProvider</code> 就会将自身注册到 Agent 程序的组件列表中，并在 <code>Application#start</code> 方法调用后，由 <code>LifecycleSupervisor</code> 类进行启动和管理，加载和解析配置文件，从中读取组件列表。</li><li>如果热更新未开启，则配置提供方将在启动时立刻读取配置文件，并由 <code>LifecycleSupervisor</code> 启动和管理所有组件。</li><li>最后，<code>main</code> 会调用 <code>Runtime#addShutdownHook</code>，当 JVM 关闭时（SIGTERM 或者 Ctrl+C），<code>Application#stop</code> 会被用于关闭 Flume Agent，使各组件优雅退出。</li></ol><a id="more"></a><h2 id="配置重载"><a href="#配置重载" class="headerlink" title="配置重载"></a>配置重载</h2><p>在 <code>PollingPropertiesFileConfigurationProvider</code> 类中，当文件内容更新时，它会调用父类的 <code>AbstractConfigurationProvider#getConfiguration</code> 方法，将配置内容解析成 <code>MaterializedConfiguration</code> 实例，这个对象实例中包含了数据源（Source）、目的地（Sink）、以及管道（Channel）组件的所有信息。随后，这个轮询线程会通过 Guava 的 <code>EventBus</code> 机制通知 <code>Application</code> 类配置发生了更新，从而触发 <code>Application#handleConfigurationEvent</code> 方法，重新加载所有的组件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Application 类</span></span><br><span class="line"><span class="meta">@Subscribe</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">handleConfigurationEvent</span><span class="params">(MaterializedConfiguration conf)</span> </span>&#123;</span><br><span class="line">  stopAllComponents();</span><br><span class="line">  startAllComponents(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PollingPropertiesFileConfigurationProvider$FileWatcherRunnable 内部类</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  eventBus.post(getConfiguration());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="启动组件"><a href="#启动组件" class="headerlink" title="启动组件"></a>启动组件</h2><p>组件启动的流程位于 <code>Application#startAllComponents</code> 方法中。这个方法接收到新的组件信息后，首先将启动所有的 <code>Channel</code>，然后启动 <code>Sink</code> 和 <code>Source</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startAllComponents</span><span class="params">(MaterializedConfiguration materializedConfiguration)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.materializedConfiguration = materializedConfiguration;</span><br><span class="line">  <span class="keyword">for</span> (Entry&lt;String, Channel&gt; entry :</span><br><span class="line">      materializedConfiguration.getChannels().entrySet()) &#123;</span><br><span class="line">    supervisor.supervise(entry.getValue(),</span><br><span class="line">        <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//  等待所有管道启动完毕</span></span><br><span class="line">  <span class="keyword">for</span> (Channel ch : materializedConfiguration.getChannels().values()) &#123;</span><br><span class="line">    <span class="keyword">while</span> (ch.getLifecycleState() != LifecycleState.START</span><br><span class="line">        &amp;&amp; !supervisor.isComponentInErrorState(ch)) &#123;</span><br><span class="line">      Thread.sleep(<span class="number">500</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 相继启动目的地和数据源组件</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>LifecycleSupervisor</code> 类（代码中的 <code>supervisor</code> 变量）可用于管理实现了 <code>LifecycleAware</code> 接口的组件。该类会初始化一个 <code>MonitorRunnable</code>，每三秒轮询一次组件状态，通过 <code>LifecycleAware#start</code> 和 <code>stop</code> 方法，保证其始终处于 <code>desiredState</code> 变量所指定的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MonitorRunnable</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!lifecycleAware.getLifecycleState().equals(</span><br><span class="line">        supervisoree.status.desiredState)) &#123;</span><br><span class="line">      <span class="keyword">switch</span> (supervisoree.status.desiredState) &#123;</span><br><span class="line">        <span class="keyword">case</span> START:</span><br><span class="line">          lifecycleAware.start();</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> STOP:</span><br><span class="line">          lifecycleAware.stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="停止组件"><a href="#停止组件" class="headerlink" title="停止组件"></a>停止组件</h2><p>当 JVM 关闭时，钩子函数会调用 <code>Application#stop</code> 方法，进而调用 <code>LifecycleSupervisor#stop</code>。该方法首先停止所有的 <code>MonitorRunnable</code> 线程，将组件目标状态置为 <code>STOP</code>，并调用 <code>LifecycleAware#stop</code> 方法命其优雅终止。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LifecycleSupervisor</span> <span class="keyword">implements</span> <span class="title">LifecycleAware</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    monitorService.shutdown();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> Entry&lt;LifecycleAware, Supervisoree&gt; entry :</span><br><span class="line">        supervisedProcesses.entrySet()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (entry.getKey().getLifecycleState().equals(LifecycleState.START)) &#123;</span><br><span class="line">        entry.getValue().status.desiredState = LifecycleState.STOP;</span><br><span class="line">        entry.getKey().stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Source-与-SourceRunner"><a href="#Source-与-SourceRunner" class="headerlink" title="Source 与 SourceRunner"></a>Source 与 SourceRunner</h2><p>对于单个组件的生命周期，我们以 <code>KafkaSource</code> 为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSource</span> <span class="keyword">extends</span> <span class="title">AbstractPollableSource</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStart</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</span><br><span class="line">    consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, <span class="keyword">byte</span>[]&gt;(kafkaProps);</span><br><span class="line">    it = consumer.poll(<span class="number">1000</span>).iterator();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStop</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>KafkaSource</code> 被定义成轮询式的数据源，也就是说我们需要使用一个线程不断对其进行轮询，查看是否有数据可以供处理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PollableSourceRunner</span> <span class="keyword">extends</span> <span class="title">SourceRunner</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    source.start();</span><br><span class="line">    runner = <span class="keyword">new</span> PollingRunner();</span><br><span class="line">    runnerThread = <span class="keyword">new</span> Thread(runner);</span><br><span class="line">    runnerThread.start();</span><br><span class="line">    lifecycleState = LifecycleState.START;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    runnerThread.interrupt();</span><br><span class="line">    runnerThread.join();</span><br><span class="line">    source.stop();</span><br><span class="line">    lifecycleState = LifecycleState.STOP;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 轮询线程</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PollingRunner</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (!shouldStop.get()) &#123;</span><br><span class="line">        source.process();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>AbstractPollableSource</code> 和 <code>SourceRunner</code> 都实现了 <code>LifecycleAware</code> 接口，因此都有 <code>start</code> 和 <code>stop</code> 方法。但是，只有 <code>SourceRunner</code> 会由 <code>LifecycleSupervisor</code> 管理，<code>PollableSource</code> 则是附属于 <code>SourceRunner</code> 的一个组件。我们可以在 <code>AbstractConfigurationProvider#loadSources</code> 中看到配置关系：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadSources</span><span class="params">(Map&lt;String, SourceRunner&gt; sourceRunnerMap)</span> </span>&#123;</span><br><span class="line">  Source source = sourceFactory.create();</span><br><span class="line">  Configurables.configure(source, config);</span><br><span class="line">  sourceRunnerMap.put(comp.getComponentName(),</span><br><span class="line">      SourceRunner.forSource(source));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li><li><a href="https://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html</a></li><li><a href="https://kafka.apache.org/0100/javadoc/index.html" target="_blank" rel="noopener">https://kafka.apache.org/0100/javadoc/index.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://flume.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Flume&lt;/a&gt; 是数据仓库体系中用于做实时 ETL 的工具。它提供了丰富的数据源和写入组件，这些组件在运行时都由 Flume 的生命周期管理机制进行监控和维护。本文将对这部分功能的源码进行解析。&lt;/p&gt;
&lt;h2 id=&quot;项目结构&quot;&gt;&lt;a href=&quot;#项目结构&quot; class=&quot;headerlink&quot; title=&quot;项目结构&quot;&gt;&lt;/a&gt;项目结构&lt;/h2&gt;&lt;p&gt;Flume 的源码可以从 GitHub 上下载。它是一个 Maven 项目，我们将其导入到 IDE 中以便更好地进行源码阅读。以下是代码仓库的基本结构：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-node&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-code&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sdk&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sources/flume-kafka-source&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-channels/flume-kafka-channel&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sinks/flume-hdfs-sink&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;程序入口&quot;&gt;&lt;a href=&quot;#程序入口&quot; class=&quot;headerlink&quot; title=&quot;程序入口&quot;&gt;&lt;/a&gt;程序入口&lt;/h2&gt;&lt;p&gt;Flume Agent 的入口 &lt;code&gt;main&lt;/code&gt; 函数位于 &lt;code&gt;flume-ng-node&lt;/code&gt; 模块的 &lt;code&gt;org.apache.flume.node.Application&lt;/code&gt; 类中。下列代码是该函数的摘要：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Application&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    CommandLineParser parser = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; GnuParser();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (isZkConfigured) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (reload) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        components.add(zookeeperConfigurationProvider);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        application.handleConfigurationEvent();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;// PropertiesFileConfigurationProvider&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    application.start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Runtime.getRuntime().addShutdownHook(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(&lt;span class=&quot;string&quot;&gt;&quot;agent-shutdown-hook&quot;&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;meta&quot;&gt;@Override&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appReference.stop();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;启动过程说明如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用 &lt;code&gt;commons-cli&lt;/code&gt; 对命令行参数进行解析，提取 Agent 名称、配置信息读取方式及其路径信息；&lt;/li&gt;
&lt;li&gt;配置信息可以通过文件或 ZooKeeper 的方式进行读取，两种方式都支持热加载，即我们不需要重启 Agent 就可以更新配置内容：&lt;ul&gt;
&lt;li&gt;基于文件的配置热加载是通过一个后台线程对文件进行轮询实现的；&lt;/li&gt;
&lt;li&gt;基于 ZooKeeper 的热加载则是使用了 Curator 的 &lt;code&gt;NodeCache&lt;/code&gt; 模式，底层是 ZooKeeper 原生的监听（Watch）特性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果配置热更新是开启的（默认开启），配置提供方 &lt;code&gt;ConfigurationProvider&lt;/code&gt; 就会将自身注册到 Agent 程序的组件列表中，并在 &lt;code&gt;Application#start&lt;/code&gt; 方法调用后，由 &lt;code&gt;LifecycleSupervisor&lt;/code&gt; 类进行启动和管理，加载和解析配置文件，从中读取组件列表。&lt;/li&gt;
&lt;li&gt;如果热更新未开启，则配置提供方将在启动时立刻读取配置文件，并由 &lt;code&gt;LifecycleSupervisor&lt;/code&gt; 启动和管理所有组件。&lt;/li&gt;
&lt;li&gt;最后，&lt;code&gt;main&lt;/code&gt; 会调用 &lt;code&gt;Runtime#addShutdownHook&lt;/code&gt;，当 JVM 关闭时（SIGTERM 或者 Ctrl+C），&lt;code&gt;Application#stop&lt;/code&gt; 会被用于关闭 Flume Agent，使各组件优雅退出。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="flume" scheme="http://shzhangji.com/cnblogs/tags/flume/"/>
    
      <category term="source code" scheme="http://shzhangji.com/cnblogs/tags/source-code/"/>
    
  </entry>
  
  <entry>
    <title>Pandas 与数据整理</title>
    <link href="http://shzhangji.com/cnblogs/2017/09/30/pandas-and-tidy-data/"/>
    <id>http://shzhangji.com/cnblogs/2017/09/30/pandas-and-tidy-data/</id>
    <published>2017-09-30T06:37:56.000Z</published>
    <updated>2020-08-22T12:06:11.267Z</updated>
    
    <content type="html"><![CDATA[<p>在 <a href="https://www.jstatsoft.org/article/view/v059i10" target="_blank" rel="noopener">Tidy Data</a> 论文中，<a href="https://en.wikipedia.org/wiki/Hadley_Wickham" target="_blank" rel="noopener">Wickham 博士</a> 提出了这样一种“整洁”的数据结构：每个变量是一列，每次观测结果是一行，不同的观测类型存放在单独的表中。他认为这样的数据结构可以帮助分析师更简单高效地进行处理、建模、和可视化。他在论文中列举了 <em>五种</em> 不符合整洁数据的情况，并演示了如何通过 <a href="https://github.com/hadley/tidy-data/" target="_blank" rel="noopener">R 语言</a> 对它们进行整理。本文中，我们将使用 Python 和 Pandas 来达到同样的目的。</p><p>文中的源代码和演示数据可以在 GitHub（<a href="https://github.com/jizhang/pandas-tidy-data" target="_blank" rel="noopener">链接</a>）上找到。读者应该已经安装好 Python 开发环境，推荐各位使用 Anaconda 和 Spyder IDE。</p><h2 id="列名称是数据值，而非变量名"><a href="#列名称是数据值，而非变量名" class="headerlink" title="列名称是数据值，而非变量名"></a>列名称是数据值，而非变量名</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/pew.png" alt="宗教信仰与收入 - Pew 论坛"></p><p>表中的列“&lt;$10k”、“$10-20k”其实是“收入”变量的具体值。<em>变量</em> 是指某一特性的观测值，如身高、体重，本例中则是收入、宗教信仰。表中的数值数据构成了另一个变量——人数。要做到 <em>每个变量是一列</em> ，我们需要进行以下变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = df.set_index(<span class="string">'religion'</span>)</span><br><span class="line">df = df.stack()</span><br><span class="line">df.index = df.index.rename(<span class="string">'income'</span>, level=<span class="number">1</span>)</span><br><span class="line">df.name = <span class="string">'frequency'</span></span><br><span class="line">df = df.reset_index()</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/pew-tidy.png" alt="宗教信仰与收入 - 整洁版"></p><a id="more"></a><p>这里我们使用了 Pandas 多级索引的 <a href="https://pandas.pydata.org/pandas-docs/stable/reshaping.html" target="_blank" rel="noopener">stack / unstack</a> 特性。<code>stack()</code> 会将列名转置为新一级的索引，并将数据框（DataFrame）转换成序列（Series）。转置后，我们对行和列的名称做一些调整，再用 <code>reset_index()</code> 将数据框还原成普通的二维表。</p><p>除了使用多级索引，Pandas 还提供了另一种更为便捷的方法——<a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html" target="_blank" rel="noopener"><code>melt()</code></a>。该方法接收以下参数：</p><ul><li><code>frame</code>: 需要处理的数据框；</li><li><code>id_vars</code>: 保持原样的数据列；</li><li><code>value_vars</code>: 需要被转换成变量值的数据列；</li><li><code>var_name</code>: 转换后变量的列名；</li><li><code>value_name</code>: 数值变量的列名。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'religion'</span>], value_vars=list(df.columns)[<span class="number">1</span>:],</span><br><span class="line">             var_name=<span class="string">'income'</span>, value_name=<span class="string">'frequency'</span>)</span><br><span class="line">df = df.sort_values(by=<span class="string">'religion'</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/pew-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>这段代码会输出相同的结果，下面的示例中我们都将使用 <code>melt()</code> 方法。我们再来看另外一个案例：</p><p><img src="/cnblogs/images/tidy-data/billboard.png" alt="Billboard 2000"></p><p>在这个数据集中，每周的排名都被记录到了不同的数据列中。如果我们想要回答“Dancing Queen 这首歌在 2000年7月15日 的排名如何”，就需要结合 <code>date.entered</code> 字段做一些运算才行。下面我们来对这份数据进行整理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/billboard.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=list(df.columns)[:<span class="number">5</span>], value_vars=list(df.columns)[<span class="number">5</span>:],</span><br><span class="line">             var_name=<span class="string">'week'</span>, value_name=<span class="string">'rank'</span>)</span><br><span class="line">df[<span class="string">'week'</span>] = df[<span class="string">'week'</span>].str[<span class="number">2</span>:].astype(int)</span><br><span class="line">df[<span class="string">'date.entered'</span>] = pd.to_datetime(df[<span class="string">'date.entered'</span>]) + pd.to_timedelta((df[<span class="string">'week'</span>] - <span class="number">1</span>) * <span class="number">7</span>, <span class="string">'d'</span>)</span><br><span class="line">df = df.rename(columns=&#123;<span class="string">'date.entered'</span>: <span class="string">'date'</span>&#125;)</span><br><span class="line">df = df.sort_values(by=[<span class="string">'track'</span>, <span class="string">'date'</span>])</span><br><span class="line">df.to_csv(<span class="string">'data/billboard-intermediate.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/billboard-intermediate.png" alt="Billboard 2000 - 中间版"></p><p>上述代码中，我们还将 <code>date.entered</code> 转换成了每一周的具体日期，<code>week</code> 字段也作为单独的数据列进行存储。但是，我们会在表中看到很多重复的信息，如歌手、曲名等，我们将在第四节解决这个问题。</p><h2 id="一列包含多个变量"><a href="#一列包含多个变量" class="headerlink" title="一列包含多个变量"></a>一列包含多个变量</h2><p>人们之所以会将变量值作为列名，一方面是这样的表示方法更为紧凑、可以在一页中显示更多信息，还有一点是这种格式便于做交叉验证等数据分析工作。下面的数据集更是将性别和年龄这两个变量都放入了列名中：</p><p><img src="/cnblogs/images/tidy-data/tb.png" alt="结核病 (TB)"></p><p><code>m</code> 表示男性（Male），<code>f</code> 表示女性（Female），<code>0-14</code>、<code>15-24</code> 则表示年龄段。进行数据整理时，我们先用 Pandas 的字符串处理功能截取 <code>sex</code> 字段，再对剩余表示年龄段的子串做映射处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/tb.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'country'</span>, <span class="string">'year'</span>], value_vars=list(df.columns)[<span class="number">2</span>:],</span><br><span class="line">             var_name=<span class="string">'column'</span>, value_name=<span class="string">'cases'</span>)</span><br><span class="line">df = df[df[<span class="string">'cases'</span>] != <span class="string">'---'</span>]</span><br><span class="line">df[<span class="string">'cases'</span>] = df[<span class="string">'cases'</span>].astype(int)</span><br><span class="line">df[<span class="string">'sex'</span>] = df[<span class="string">'column'</span>].str[<span class="number">0</span>]</span><br><span class="line">df[<span class="string">'age'</span>] = df[<span class="string">'column'</span>].str[<span class="number">1</span>:].map(&#123;</span><br><span class="line">    <span class="string">'014'</span>: <span class="string">'0-14'</span>,</span><br><span class="line">    <span class="string">'1524'</span>: <span class="string">'15-24'</span>,</span><br><span class="line">    <span class="string">'2534'</span>: <span class="string">'25-34'</span>,</span><br><span class="line">    <span class="string">'3544'</span>: <span class="string">'35-44'</span>,</span><br><span class="line">    <span class="string">'4554'</span>: <span class="string">'45-54'</span>,</span><br><span class="line">    <span class="string">'5564'</span>: <span class="string">'55-64'</span>,</span><br><span class="line">    <span class="string">'65'</span>: <span class="string">'65+'</span></span><br><span class="line">&#125;)</span><br><span class="line">df = df[[<span class="string">'country'</span>, <span class="string">'year'</span>, <span class="string">'sex'</span>, <span class="string">'age'</span>, <span class="string">'cases'</span>]]</span><br><span class="line">df.to_csv(<span class="string">'data/tb-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/tb-tidy.png" alt="结核病 (TB) - 整洁版"></p><h2 id="变量存储在行和列中"><a href="#变量存储在行和列中" class="headerlink" title="变量存储在行和列中"></a>变量存储在行和列中</h2><p>下表是一个名为 MX17004 的气象站收集的温度数据。可以看到，日期被放置在列名中，我们可以用 <code>melt</code> 进行处理；<code>tmax</code> 和 <code>tmin</code> 则表示最高温度和最低温度，他们很显然是两个不同的变量，用来衡量单个观测对象的属性的，本例中的观测对象是“天”。因此，我们需要使用 <code>unstack</code> 将其拆分成两列。</p><p><img src="/cnblogs/images/tidy-data/weather.png" alt="气象站"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/weather.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'id'</span>, <span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'element'</span>],</span><br><span class="line">             value_vars=list(df.columns)[<span class="number">4</span>:],</span><br><span class="line">             var_name=<span class="string">'date'</span>, value_name=<span class="string">'value'</span>)</span><br><span class="line">df[<span class="string">'date'</span>] = df[<span class="string">'date'</span>].str[<span class="number">1</span>:].astype(<span class="string">'int'</span>)</span><br><span class="line">df[<span class="string">'date'</span>] = df[[<span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'date'</span>]].apply(</span><br><span class="line">    <span class="keyword">lambda</span> row: <span class="string">'&#123;:4d&#125;-&#123;:02d&#125;-&#123;:02d&#125;'</span>.format(*row),</span><br><span class="line">    axis=<span class="number">1</span>)</span><br><span class="line">df = df.loc[df[<span class="string">'value'</span>] != <span class="string">'---'</span>, [<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>, <span class="string">'value'</span>]]</span><br><span class="line">df = df.set_index([<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>])</span><br><span class="line">df = df.unstack()</span><br><span class="line">df.columns = list(df.columns.get_level_values(<span class="string">'element'</span>))</span><br><span class="line">df = df.reset_index()</span><br><span class="line">df.to_csv(<span class="string">'data/weather-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/weather-tidy.png" alt="气象站 - 整洁版"></p><h2 id="同一表中包含多种观测类型"><a href="#同一表中包含多种观测类型" class="headerlink" title="同一表中包含多种观测类型"></a>同一表中包含多种观测类型</h2><p>在处理 Billboard 数据集时，我们会看到冗余的曲目信息，这是因为该表实际记录的是两种不同的观测类型——歌曲曲目和周排名。整理时，我们需要先为每首歌曲生成一个唯一标识，即 <code>id</code>，然后拆分到单独的表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/billboard-intermediate.csv'</span>)</span><br><span class="line">df_track = df[[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>]].drop_duplicates()</span><br><span class="line">df_track.insert(<span class="number">0</span>, <span class="string">'id'</span>, range(<span class="number">1</span>, len(df_track) + <span class="number">1</span>))</span><br><span class="line">df = pd.merge(df, df_track, on=[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>])</span><br><span class="line">df = df[[<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'rank'</span>]]</span><br><span class="line">df_track.to_csv(<span class="string">'data/billboard-track.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/billboard-rank.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">print(df_track, <span class="string">'\n\n'</span>, df)</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/tidy-data/billboard-track.png" alt="Billboard 2000 - 歌曲"></p><p><img src="/cnblogs/images/tidy-data/billboard-rank.png" alt="Billboard 2000 - 排名"></p><h2 id="同一观测类型分布在不同表中"><a href="#同一观测类型分布在不同表中" class="headerlink" title="同一观测类型分布在不同表中"></a>同一观测类型分布在不同表中</h2><p>原始的数据集可能会以两种方式进行了拆分，一种是按照某个变量拆分，如按年拆分为2000年、2001年，按地理位置拆分为中国、英国；另一种是按不同的属性拆分，如一份数据是收集温度的传感器记录的，另一份是湿度传感器，他们记录的都是每一天的观测值。对于第一种情况，我们可以编写一个读取数据的函数，遍历目录中的文件，并将文件名作为单独的列加入数据框，最后使用 <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html" target="_blank" rel="noopener"><code>pd.concat</code></a> 进行合并；第二种情况则要求数据集中的记录有一个唯一标识，如日期、身份证号，并通过 <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html" target="_blank" rel="noopener"><code>pd.merge</code></a> 将各个数据集联系起来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://tomaugspurger.github.io/modern-5-tidy.html" target="_blank" rel="noopener">https://tomaugspurger.github.io/modern-5-tidy.html</a></li><li><a href="https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77" target="_blank" rel="noopener">https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77</a></li><li><a href="http://www.jeannicholashould.com/tidy-data-in-python.html" target="_blank" rel="noopener">http://www.jeannicholashould.com/tidy-data-in-python.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 &lt;a href=&quot;https://www.jstatsoft.org/article/view/v059i10&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Tidy Data&lt;/a&gt; 论文中，&lt;a href=&quot;https://en.wikipedia.org/wiki/Hadley_Wickham&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Wickham 博士&lt;/a&gt; 提出了这样一种“整洁”的数据结构：每个变量是一列，每次观测结果是一行，不同的观测类型存放在单独的表中。他认为这样的数据结构可以帮助分析师更简单高效地进行处理、建模、和可视化。他在论文中列举了 &lt;em&gt;五种&lt;/em&gt; 不符合整洁数据的情况，并演示了如何通过 &lt;a href=&quot;https://github.com/hadley/tidy-data/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R 语言&lt;/a&gt; 对它们进行整理。本文中，我们将使用 Python 和 Pandas 来达到同样的目的。&lt;/p&gt;
&lt;p&gt;文中的源代码和演示数据可以在 GitHub（&lt;a href=&quot;https://github.com/jizhang/pandas-tidy-data&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;）上找到。读者应该已经安装好 Python 开发环境，推荐各位使用 Anaconda 和 Spyder IDE。&lt;/p&gt;
&lt;h2 id=&quot;列名称是数据值，而非变量名&quot;&gt;&lt;a href=&quot;#列名称是数据值，而非变量名&quot; class=&quot;headerlink&quot; title=&quot;列名称是数据值，而非变量名&quot;&gt;&lt;/a&gt;列名称是数据值，而非变量名&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;data/pew.csv&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/tidy-data/pew.png&quot; alt=&quot;宗教信仰与收入 - Pew 论坛&quot;&gt;&lt;/p&gt;
&lt;p&gt;表中的列“&amp;lt;$10k”、“$10-20k”其实是“收入”变量的具体值。&lt;em&gt;变量&lt;/em&gt; 是指某一特性的观测值，如身高、体重，本例中则是收入、宗教信仰。表中的数值数据构成了另一个变量——人数。要做到 &lt;em&gt;每个变量是一列&lt;/em&gt; ，我们需要进行以下变换：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;df = df.set_index(&lt;span class=&quot;string&quot;&gt;&#39;religion&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = df.stack()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.index = df.index.rename(&lt;span class=&quot;string&quot;&gt;&#39;income&#39;&lt;/span&gt;, level=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.name = &lt;span class=&quot;string&quot;&gt;&#39;frequency&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = df.reset_index()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/tidy-data/pew-tidy.png&quot; alt=&quot;宗教信仰与收入 - 整洁版&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/cnblogs/tags/analytics/"/>
    
      <category term="pandas" scheme="http://shzhangji.com/cnblogs/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Apache Beam 快速入门（Python 版）</title>
    <link href="http://shzhangji.com/cnblogs/2017/09/13/apache-beam-quick-start-with-python/"/>
    <id>http://shzhangji.com/cnblogs/2017/09/13/apache-beam-quick-start-with-python/</id>
    <published>2017-09-13T04:39:03.000Z</published>
    <updated>2020-08-22T12:06:11.266Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">Apache Beam</a> 是一种大数据处理标准，由谷歌于 2016 年创建。它提供了一套统一的 DSL 用以处理离线和实时数据，并能在目前主流的大数据处理平台上使用，包括 Spark、Flink、以及谷歌自身的商业套件 Dataflow。Beam 的数据模型基于过去的几项研究成果：<a href="https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf" target="_blank" rel="noopener">FlumeJava</a>、<a href="https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf" target="_blank" rel="noopener">Millwheel</a>，适用场景包括 ETL、统计分析、实时计算等。目前，Beam 提供了两种语言的 SDK：Java、Python。本文将讲述如何使用 Python 编写 Beam 应用程序。</p><p><img src="/cnblogs/images/beam/arch.jpg" alt="Apache Beam Pipeline"></p><h2 id="安装-Apache-Beam"><a href="#安装-Apache-Beam" class="headerlink" title="安装 Apache Beam"></a>安装 Apache Beam</h2><p>Apache Beam Python SDK 必须使用 Python 2.7.x 版本，你可以安装 <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noopener">pyenv</a> 来管理不同版本的 Python，或者直接从<a href="https://www.python.org/downloads/source/" target="_blank" rel="noopener">源代码</a>编译安装（需要支持 SSL）。之后，你便可以在 Python 虚拟环境中安装 Beam SDK 了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ virtualenv venv --distribute</span><br><span class="line">$ source venv/bin/activate</span><br><span class="line">(venv) $ pip install apache-beam</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Wordcount-示例"><a href="#Wordcount-示例" class="headerlink" title="Wordcount 示例"></a>Wordcount 示例</h2><p>Wordcount 是大数据领域的 Hello World，我们来看如何使用 Beam 实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</span><br><span class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions</span><br><span class="line"><span class="keyword">with</span> beam.Pipeline(options=PipelineOptions()) <span class="keyword">as</span> p:</span><br><span class="line">    lines = p | <span class="string">'Create'</span> &gt;&gt; beam.Create([<span class="string">'cat dog'</span>, <span class="string">'snake cat'</span>, <span class="string">'dog'</span>])</span><br><span class="line">    counts = (</span><br><span class="line">        lines</span><br><span class="line">        | <span class="string">'Split'</span> &gt;&gt; (beam.FlatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</span><br><span class="line">                      .with_output_types(unicode))</span><br><span class="line">        | <span class="string">'PairWithOne'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        | <span class="string">'GroupAndSum'</span> &gt;&gt; beam.CombinePerKey(sum)</span><br><span class="line">    )</span><br><span class="line">    counts | <span class="string">'Print'</span> &gt;&gt; beam.ParDo(<span class="keyword">lambda</span> (w, c): print(<span class="string">'%s: %s'</span> % (w, c)))</span><br></pre></td></tr></table></figure><p>运行脚本，我们便可得到每个单词出现的次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(venv) $ python wordcount.py</span><br><span class="line">cat: 2</span><br><span class="line">snake: 1</span><br><span class="line">dog: 2</span><br></pre></td></tr></table></figure><p>Apache Beam 有三个重要的基本概念：Pipeline、PCollection、以及 Transform。</p><ul><li><strong>Pipeline</strong> （管道）用以构建数据集和处理过程的 DAG（有向无环图）。我们可以将它看成 MapReduce 中的 <code>Job</code> 或是 Storm 的 <code>Topology</code>。</li><li><strong>PCollection</strong> 是一种数据结构，我们可以对其进行各类转换操作，如解析、过滤、聚合等。它和 Spark 中的 <code>RDD</code> 概念类似。</li><li><strong>Transform</strong> （转换）则用于编写业务逻辑。通过它，我们可以将一个 PCollection 转换成另一个 PCollection。Beam 提供了许多内置的转换函数，我们将在下文讨论。</li></ul><p>在本例中，<code>Pipeline</code> 和 <code>PipelineOptions</code> 用来创建一个管道。通过 <code>with</code> 关键字，上下文管理器会自动调用 <code>Pipeline.run</code> 和 <code>wait_until_finish</code> 方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Output PCollection] = [Input PCollection] | [Label] &gt;&gt; [Transform]</span><br></pre></td></tr></table></figure><p><code>|</code> 是 Beam 引入的新操作符，用来添加一个转换。每次转换都可以定义一个唯一的标签，默认由 Beam 自动生成。转换能够串联，我们可以构建出不同形态的转换流程，它们在运行时会表示为一个 DAG。</p><p><code>beam.Create</code> 用来从内存数据创建出一个 PCollection，主要用于测试和演示。Beam 提供了多种内置的输入源（Source）和输出目标（Sink），可以接收和写入有界（Bounded）或无界（Unbounded）的数据，并且能进行自定义。</p><p><code>beam.Map</code> 是一种 <em>一对一</em> 的转换，本例中我们将一个个单词转换成形如 <code>(word, 1)</code> 的元组。<code>beam.FlatMap</code> 则是 <code>Map</code> 和 <code>Flatten</code> 的结合体，通过它，我们将包含多个单词的数组合并成一个一维的数组。</p><p><code>CombinePerKey</code> 的输入源是一系列的二元组（2-element tuple）。这个操作会将元素的第一个元素作为键进行分组，并将相同键的值（第二个元素）组成一个列表。最后，我们使用 <code>beam.ParDo</code> 输出统计结果。这个转换函数比较底层，我们会在下文详述。</p><h2 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h2><p>目前，Beam Python SDK 对输入输出的支持十分有限。下表列出了现阶段支持的数据源（<a href="https://beam.apache.org/documentation/io/built-in/" target="_blank" rel="noopener">资料来源</a>）：</p><table><thead><tr><th>语言</th><th>文件系统</th><th>消息队列</th><th>数据库</th></tr></thead><tbody><tr><td>Java</td><td>HDFS<br>TextIO<br>XML</td><td>AMQP<br>Kafka<br>JMS</td><td>Hive<br>Solr<br>JDBC</td></tr><tr><td>Python</td><td>textio<br>avroio<br>tfrecordio</td><td>-</td><td>Google Big Query<br>Google Cloud Datastore</td></tr></tbody></table><p>这段代码演示了如何使用 <code>textio</code> 对文本文件进行读写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = p | <span class="string">'Read'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'/path/to/input-*.csv'</span>)</span><br><span class="line">lines | <span class="string">'Write'</span> &gt;&gt; beam.io.WriteToText(<span class="string">'/path/to/output'</span>, file_name_suffix=<span class="string">'.csv'</span>)</span><br></pre></td></tr></table></figure><p>通过使用通配符，<code>textio</code> 可以读取多个文件。我们还可以从不同的数据源中读取文件，并用 <code>Flatten</code> 方法将多个 <code>PCollection</code> 合并成一个。输出文件默认也会是多个，因为 Beam Pipeline 是并发执行的，不同的进程会写入独立的文件。</p><h2 id="转换函数"><a href="#转换函数" class="headerlink" title="转换函数"></a>转换函数</h2><p>Beam 中提供了基础和上层的转换函数。通常我们更偏向于使用上层函数，这样就可以将精力聚焦在实现业务逻辑上。下表列出了常用的上层转换函数：</p><table><thead><tr><th>转换函数</th><th>功能含义</th></tr></thead><tbody><tr><td>Create(value)</td><td>基于内存中的集合数据生成一个 PCollection。</td></tr><tr><td>Filter(fn)</td><td>使用 <code>fn</code> 函数过滤 PCollection 中的元素。</td></tr><tr><td>Map(fn)</td><td>使用 <code>fn</code> 函数做一对一的转换处理。</td></tr><tr><td>FlatMap(fn)</td><td>功能和 <code>Map</code> 类似，但是 <code>fn</code> 需要返回一个集合，里面包含零个或多个元素，最终 <code>FlatMap</code> 会将这些集合合并成一个 PCollection。</td></tr><tr><td>Flatten()</td><td>合并多个 PCollection。</td></tr><tr><td>Partition(fn)</td><td>将一个 PCollection 切分成多个分区。<code>fn</code> 可以是 <code>PartitionFn</code> 或一个普通函数，能够接受两个参数：<code>element</code>、<code>num_partitions</code>。</td></tr><tr><td>GroupByKey()</td><td>输入源必须是使用二元组表示的键值对，该方法会按键进行分组，并返回一个 <code>(key, iter&lt;value&gt;)</code> 的序列。</td></tr><tr><td>CoGroupByKey()</td><td>对多个二元组 PCollection 按相同键进行合并，如输入的是 <code>(k, v)</code> 和 <code>(k, w)</code>，则输出 <code>(k, (iter&lt;v&gt;, iter&lt;w&gt;))</code>。</td></tr><tr><td>RemoveDuplicates()</td><td>对 PCollection 的元素进行去重。</td></tr><tr><td>CombinePerKey(fn)</td><td>功能和 <code>GroupByKey</code> 类似，但会进一步使用 <code>fn</code> 对值列表进行合并。<code>fn</code> 可以是一个 <code>CombineFn</code>，或是一个普通函数，接收序列并返回结果，如 <code>sum</code>、<code>max</code> 函数等。</td></tr><tr><td>CombineGlobally(fn)</td><td>使用 <code>fn</code> 将整个 PCollection 合并计算成单个值。</td></tr></tbody></table><h3 id="Callable-DoFn-ParDo"><a href="#Callable-DoFn-ParDo" class="headerlink" title="Callable, DoFn, ParDo"></a>Callable, DoFn, ParDo</h3><p>可以看到，多数转换函数都会接收另一个函数（Callable）做为参数。在 Python 中，<a href="https://docs.python.org/2/library/functions.html#callable" target="_blank" rel="noopener">Callable</a> 可以是一个函数、类方法、Lambda 表达式、或是任何包含 <code>__call__</code> 方法的对象实例。Beam 会将这些函数包装成一个 <code>DoFn</code> 类，所有转换函数最终都会调用最基础的 <code>ParDo</code> 函数，并将 <code>DoFn</code> 传递给它。</p><p>我们可以尝试将 <code>lambda x: x.split(&#39; &#39;)</code> 这个表达式转换成 <code>DoFn</code> 类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitFn</span><span class="params">(beam.DoFn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> element.split(<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line">lines | beam.ParDo(SplitFn())</span><br></pre></td></tr></table></figure><p><code>ParDo</code> 转换和 <code>FlatMap</code> 的功能类似，只是它的 <code>fn</code> 参数必须是一个 <code>DoFn</code>。除了使用 <code>return</code>，我们还可以用 <code>yield</code> 语句来返回结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitAndPairWithOneFn</span><span class="params">(beam.DoFn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> element.split(<span class="string">' '</span>):</span><br><span class="line">            <span class="keyword">yield</span> (word, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="合并函数"><a href="#合并函数" class="headerlink" title="合并函数"></a>合并函数</h3><p>合并函数（<code>CombineFn</code>）用来将集合数据合并计算成单个值。我们既可以对整个 PCollection 做合并（<code>CombineGlobally</code>），也可以计算每个键的合并结果（<code>CombinePerKey</code>）。Beam 会将普通函数（Callable）包装成 <code>CombineFn</code>，这些函数需要接收一个集合，并返回单个结果。需要注意的是，Beam 会将计算过程分发到多台服务器上，合并函数会被多次调用来计算中间结果，因此需要满足<a href="https://en.wikipedia.org/wiki/Commutative_property" target="_blank" rel="noopener">交换律</a>和<a href="https://en.wikipedia.org/wiki/Associative_property" target="_blank" rel="noopener">结合律</a>。<code>sum</code>、<code>min</code>、<code>max</code> 是符合这样的要求的。</p><p>Beam 提供了许多内置的合并函数，如计数、求平均值、排序等。以计数为例，下面两种写法都可以用来统计整个 PCollection 中元素的个数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines | beam.combiners.Count.Globally()</span><br><span class="line">lines | beam.CombineGlobally(beam.combiners.CountCombineFn())</span><br></pre></td></tr></table></figure><p>其他合并函数可以参考 Python SDK 的官方文档（<a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/apache_beam.transforms.html#module-apache_beam.transforms.combiners" target="_blank" rel="noopener">链接</a>）。我们也可以自行实现合并函数，只需继承 <code>CombineFn</code>，并实现四个方法。我们以内置的 <code>Mean</code> 平均值合并函数的源码为例：</p><p><a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L75" target="_blank" rel="noopener"><code>apache_beam/transforms/combiners.py</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanCombineFn</span><span class="params">(core.CombineFn)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_accumulator</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""创建一个“本地”的中间结果，记录合计值和记录数。"""</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_input</span><span class="params">(self, <span class="params">(sum_, count)</span>, element)</span>:</span></span><br><span class="line">    <span class="string">"""处理新接收到的值。"""</span></span><br><span class="line">    <span class="keyword">return</span> sum_ + element, count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge_accumulators</span><span class="params">(self, accumulators)</span>:</span></span><br><span class="line">    <span class="string">"""合并多个中间结果。"""</span></span><br><span class="line">    sums, counts = zip(*accumulators)</span><br><span class="line">    <span class="keyword">return</span> sum(sums), sum(counts)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">extract_output</span><span class="params">(self, <span class="params">(sum_, count)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""计算平均值。"""</span></span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">return</span> float(<span class="string">'NaN'</span>)</span><br><span class="line">    <span class="keyword">return</span> sum_ / float(count)</span><br></pre></td></tr></table></figure><h3 id="复合转换函数"><a href="#复合转换函数" class="headerlink" title="复合转换函数"></a>复合转换函数</h3><p>我们简单看一下上文中使用到的 <code>beam.combiners.Count.Globally</code> 的源码（<a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L101" target="_blank" rel="noopener">链接</a>），它继承了 <code>PTransform</code> 类，并在 <code>expand</code> 方法中对 PCollection 应用了转换函数。这会形成一个小型的有向无环图，并合并到最终的 DAG 中。我们称其为复合转换函数，主要用于将相关的转换逻辑整合起来，便于理解和管理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Count</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Globally</span><span class="params">(ptransform.PTransform)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(self, pcoll)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> pcoll | core.CombineGlobally(CountCombineFn())</span><br></pre></td></tr></table></figure><p>更多内置的复合转换函数如下表所示：</p><table><thead><tr><th>复合转换函数</th><th>功能含义</th></tr></thead><tbody><tr><td>Count.Globally()</td><td>计算元素总数。</td></tr><tr><td>Count.PerKey()</td><td>计算每个键的元素数。</td></tr><tr><td>Count.PerElement()</td><td>计算每个元素出现的次数，类似 Wordcount。</td></tr><tr><td>Mean.Globally()</td><td>计算所有元素的平均值。</td></tr><tr><td>Mean.PerKey()</td><td>计算每个键的元素平均值。</td></tr><tr><td>Top.Of(n, reverse)</td><td>获取 PCollection 中最大或最小的 <code>n</code> 个元素，另有 Top.Largest(n), Top.Smallest(n).</td></tr><tr><td>Top.PerKey(n, reverse)</td><td>获取每个键的值列表中最大或最小的 <code>n</code> 个元素，另有 Top.LargestPerKey(n), Top.SmallestPerKey(n)</td></tr><tr><td>Sample.FixedSizeGlobally(n)</td><td>随机获取 <code>n</code> 个元素。</td></tr><tr><td>Sample.FixedSizePerKey(n)</td><td>随机获取每个键下的 <code>n</code> 个元素。</td></tr><tr><td>ToList()</td><td>将 PCollection 合并成一个列表。</td></tr><tr><td>ToDict()</td><td>将 PCollection 合并成一个哈希表，输入数据需要是二元组集合。</td></tr></tbody></table><h2 id="时间窗口"><a href="#时间窗口" class="headerlink" title="时间窗口"></a>时间窗口</h2><p>在处理事件数据时，如访问日志、用户点击流，每条数据都会有一个 <em>事件时间</em> 属性，而通常我们会按事件时间对数据进行分组统计，这些分组即时间窗口。在 Beam 中，我们可以定义不同的时间窗口类型，能够支持有界和无界数据。由于 Python SDK 暂时只支持有界数据，我们就以一个离线访问日志文件作为输入源，统计每个时间窗口的记录条数。对于无界数据，概念和处理流程也是类似的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] &quot;GET /edit HTTP/1.1&quot; 401 12846</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] &quot;GET /rdiff HTTP/1.1&quot; 200 4523</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] &quot;GET /hsdivision HTTP/1.1&quot; 200 6291</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] &quot;GET /view HTTP/1.1&quot; 200 7352</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:20:55 -0800] &quot;GET /view HTTP/1.1&quot; 200 5253</span><br></pre></td></tr></table></figure><p><code>logmining.py</code> 的完整源码可以在 GitHub（<a href="https://github.com/jizhang/hello-beam/blob/master/logmining.py" target="_blank" rel="noopener">链接</a>）中找到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lines = p | <span class="string">'Create'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'access.log'</span>)</span><br><span class="line">windowed_counts = (</span><br><span class="line">    lines</span><br><span class="line">    | <span class="string">'Timestamp'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: beam.window.TimestampedValue(</span><br><span class="line">                              x, extract_timestamp(x)))</span><br><span class="line">    | <span class="string">'Window'</span> &gt;&gt; beam.WindowInto(beam.window.SlidingWindows(<span class="number">600</span>, <span class="number">300</span>))</span><br><span class="line">    | <span class="string">'Count'</span> &gt;&gt; (beam.CombineGlobally(beam.combiners.CountCombineFn())</span><br><span class="line">                  .without_defaults())</span><br><span class="line">)</span><br><span class="line">windowed_counts =  windowed_counts | beam.ParDo(PrintWindowFn())</span><br></pre></td></tr></table></figure><p>首先，我们需要为每一条记录附加上时间戳。自定义函数 <code>extract_timestamp</code> 用以将日志中的时间 <code>[07/Mar/2004:16:05:49 -0800]</code> 转换成 Unix 时间戳，<code>TimestampedValue</code> 则会将这个时间戳和对应记录关联起来。之后，我们定义了一个大小为 <em>10 分钟</em>，间隔为 <em>5 分钟</em> 的滑动窗口（Sliding Window）。从零点开始，第一个窗口的范围是 <code>[00:00, 00:10)</code>，第二个窗口的范围是 <code>[00:05, 00:15)</code>，以此类推。所有窗口的长度都是 <em>10 分钟</em>，相邻两个窗口之间相隔 <em>5 分钟</em>。滑动窗口和固定窗口（Fixed Window）不同，因为相同的元素可能会落入不同的窗口中参与计算。最后，我们使用一个合并函数计算每个窗口中的记录数。通过这个方法得到前五条记录的计算结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[2004-03-08T00:00:00Z, 2004-03-08T00:10:00Z) @ 2</span><br><span class="line">[2004-03-08T00:05:00Z, 2004-03-08T00:15:00Z) @ 4</span><br><span class="line">[2004-03-08T00:10:00Z, 2004-03-08T00:20:00Z) @ 2</span><br><span class="line">[2004-03-08T00:15:00Z, 2004-03-08T00:25:00Z) @ 1</span><br><span class="line">[2004-03-08T00:20:00Z, 2004-03-08T00:30:00Z) @ 1</span><br></pre></td></tr></table></figure><p>在无界数据的实时计算过程中，事件数据的接收顺序是不固定的，因此需要利用 Beam 的水位线和触发器机制来处理延迟数据（Late Data）。这个话题比较复杂，而且 Python SDK 尚未支持这些特性，感兴趣的读者可以参考 Stream <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="noopener">101</a> 和 <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102" target="_blank" rel="noopener">102</a> 这两篇文章。</p><h2 id="Pipeline-运行时"><a href="#Pipeline-运行时" class="headerlink" title="Pipeline 运行时"></a>Pipeline 运行时</h2><p>上文中提到，Apache Beam 是一个数据处理标准，只提供了 SDK 和 API，因而必须使用 Spark、Flink 这样的计算引擎来运行它。下表列出了当前支持 Beam Model 的引擎，以及他们的兼容程度：</p><p><img src="/cnblogs/images/beam/matrix.png" alt="Beam 运行时能力矩阵"></p><p><a href="https://beam.apache.org/documentation/runners/capability-matrix/" target="_blank" rel="noopener">图片来源</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://beam.apache.org/documentation/programming-guide/" target="_blank" rel="noopener">https://beam.apache.org/documentation/programming-guide/</a></li><li><a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/" target="_blank" rel="noopener">https://beam.apache.org/documentation/sdks/pydoc/2.1.0/</a></li><li><a href="https://sookocheff.com/post/dataflow/get-to-know-dataflow/" target="_blank" rel="noopener">https://sookocheff.com/post/dataflow/get-to-know-dataflow/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://beam.apache.org/get-started/beam-overview/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Beam&lt;/a&gt; 是一种大数据处理标准，由谷歌于 2016 年创建。它提供了一套统一的 DSL 用以处理离线和实时数据，并能在目前主流的大数据处理平台上使用，包括 Spark、Flink、以及谷歌自身的商业套件 Dataflow。Beam 的数据模型基于过去的几项研究成果：&lt;a href=&quot;https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FlumeJava&lt;/a&gt;、&lt;a href=&quot;https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Millwheel&lt;/a&gt;，适用场景包括 ETL、统计分析、实时计算等。目前，Beam 提供了两种语言的 SDK：Java、Python。本文将讲述如何使用 Python 编写 Beam 应用程序。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/beam/arch.jpg&quot; alt=&quot;Apache Beam Pipeline&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装-Apache-Beam&quot;&gt;&lt;a href=&quot;#安装-Apache-Beam&quot; class=&quot;headerlink&quot; title=&quot;安装 Apache Beam&quot;&gt;&lt;/a&gt;安装 Apache Beam&lt;/h2&gt;&lt;p&gt;Apache Beam Python SDK 必须使用 Python 2.7.x 版本，你可以安装 &lt;a href=&quot;https://github.com/pyenv/pyenv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pyenv&lt;/a&gt; 来管理不同版本的 Python，或者直接从&lt;a href=&quot;https://www.python.org/downloads/source/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;源代码&lt;/a&gt;编译安装（需要支持 SSL）。之后，你便可以在 Python 虚拟环境中安装 Beam SDK 了：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ virtualenv venv --distribute&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ source venv/bin/activate&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(venv) $ pip install apache-beam&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="mapreduce" scheme="http://shzhangji.com/cnblogs/tags/mapreduce/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/cnblogs/tags/stream-processing/"/>
    
      <category term="apache beam" scheme="http://shzhangji.com/cnblogs/tags/apache-beam/"/>
    
  </entry>
  
  <entry>
    <title>2017 Top 15 Python 数据科学类库；时间序列异常点检测；如何加入开源项目</title>
    <link href="http://shzhangji.com/cnblogs/2017/09/06/python-data-science-anomaly-detection-opensource/"/>
    <id>http://shzhangji.com/cnblogs/2017/09/06/python-data-science-anomaly-detection-opensource/</id>
    <published>2017-09-06T01:49:10.000Z</published>
    <updated>2020-08-22T12:06:11.266Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2017-Top-15-Python-数据科学类库"><a href="#2017-Top-15-Python-数据科学类库" class="headerlink" title="2017 Top 15 Python 数据科学类库"></a>2017 Top 15 Python 数据科学类库</h2><p><img src="/cnblogs/images/digest/google-trends.png" alt="Google Trends"></p><p>近年来，Python 在数据科学领域得到了越来越多的关注，本文整理归类了使用率最高的数据科学类库，供大家参考。</p><p>NumPy、SciPy、Pandas 是 Python 数据科学的核心类库。NumPy 提供了 N 维数组、矩阵、向量等数据结构，能够进行高性能的数学运算；SciPy 包含了线性代数、拟合优化、统计学习的通用方法；Pandas 则一般用于数据清洗、探索型分析等工作。</p><p>可视化方面，Matplotlib 是最早流行的类库，提供了丰富的图形化接口，但 API 的使用方式偏底层，需要编写较多代码；Seaborn 构建在 Matplotlib 之上，重新定义了图表样式，更适合在报告、演示文档中使用，并且它还预置了诸多探索型分析函数，可以快速地对数据进行描述性可视化；Bokeh 主打交互性，它运行在浏览器中，让使用者可以方便地调节可视化参数；Plotly 也是一款基于页面的可视化工具，但因为是商业软件，需要授权后才能使用。</p><p>SciKit-Learn 是公认的 Python 机器学习标准类库，它提供了准确、统一的接口，可以方便地使用各种机器学习算法；深度学习领域，Theano 是比较老牌的类库之一，特点是能够运行于不同的系统架构之上（CPU、GPU）；Tensorflow 则是最近较火的基础类库，使用它提供的各种算子和数据流工具，我们可以构建出多层神经网络，在集群上对大数据进行运算；Keras 则是一款较上层的工具库，底层使用 Theano 或 Tensorflow 作为引擎，可以通过快速构建实验来验证模型。</p><p>自然语言处理领域中，NLTK 提供了文本标记、分词、构建语料树等功能，用以揭示句中或句间的依赖关系；Gensim 则擅长构建向量空间模型、话题建模、挖掘大量文本中重复出现的模式，其算法都属于非监督学习，因此只需提供语料库就能得到结果。</p><p>原文：<a href="http://www.kdnuggets.com/2017/06/top-15-python-libraries-data-science.html" target="_blank" rel="noopener">http://www.kdnuggets.com/2017/06/top-15-python-libraries-data-science.html</a></p><a id="more"></a><h2 id="时间序列数据异常点检测算法"><a href="#时间序列数据异常点检测算法" class="headerlink" title="时间序列数据异常点检测算法"></a>时间序列数据异常点检测算法</h2><p><img src="/cnblogs/images/digest/anomaly-detection-cart.png" alt></p><p>异常点检测是指寻找那些偏离标准值或正常值的数据点。异常点有几种常见的类型：短期内产生的峰值，包括最大值、最小值、以及零值；长期的数据合计与上一周期的比较等。检测方法也可以归类为两种：对数据点进行分类，标记异常与否；或是对未来数据走势做预测，给出置信区间。</p><p>使用 STL 分解法将时间序列数据表示成三个要素：季节性、趋势、残差。通过分析残差的背离程度，引入一定的阈值，就可以作为预警依据了。我们可以使用绝对中位差来作为阈值，推特使用并开源了相关类库（<a href="https://github.com/twitter/AnomalyDetection" target="_blank" rel="noopener">链接</a>）。这种方法的优点是简单，对峰值异常较敏感，并能结合滑动平均来检测周期性的异常。缺点是需要进行调参，且不能检测剧烈变动的指标。</p><p>分类和回归树算法有两种使用方式：一种是准备好已标记过异常点的数据集，进行监督型的机器学习；另一种则是让 CART 算法自动寻找数据集中的模式，预测异常点的置信区间。最常用的开源库是 <a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">xgboost</a>。这一方法可以用各种特征进行学习和预测，当然计算量也会因此上升。</p><p>ARIMA 是一种较为简单的算法，通过历史值来预测下一个数据点的动向。它的特点是每接收一个新的数据点都需要重新构建一次预测模型，并且你的数据必须和时间是无关的。和该算法相似的是指数平滑法，比较有趣的实现是 <a href="https://www.otexts.org/fpp/7/5" target="_blank" rel="noopener">Holt-Winters 季节性指标</a>，用于检测阶段性长期趋势的异常。</p><p>人工神经网络也能够进行异常检测，只是这一方式还处于<a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf" target="_blank" rel="noopener">科研阶段</a>。不过我们想要提醒读者的是，尽量从简单的模型开始，针对你的具体问题进行优化，因为通用的算法并不一定是最优的。</p><p>原文：<a href="https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2" target="_blank" rel="noopener">https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2</a></p><h2 id="如何加入开源项目"><a href="#如何加入开源项目" class="headerlink" title="如何加入开源项目"></a>如何加入开源项目</h2><p><img src="/cnblogs/images/digest/opensource.jpg" alt="opensource"></p><p>已经蠢蠢欲动了吗？不如马上开始！找一个你正在使用的开源项目，Fork 源码仓库。先从修复 Bug 开始，或者为现有代码编写单元测试。过程中你需要学会读懂别人的代码，遵循代码规范。提交补丁后，你一定会受到猛烈的抨击，千万不要因此胆怯。熟悉代码之后，可以寻求一些更重要的职责，比如提出一个新的特性，或者认领新特性的开发工作。最后，你也可以开启自己的开源项目。</p><p>如何寻找自己想做贡献的开源项目？首先你可以在邮件列表、论坛、Bug 跟踪系统中找到这样的项目，问问自己是否喜欢这里的社区氛围。一开始不要在一棵树上吊死，多观察几个开源项目，找到自己对味的。从小处着手，比如重现 Bug、提交测试代码、改进文档等等。中途不要放弃！</p><p>最直接的加入方式其实是当正在使用的某个项目出现 Bug，或者你有用的不爽的地方，对它加以改进。此外，加入一个对开源项目友好的公司也是不错的选择。</p><p>原文：<a href="https://arstechnica.com/information-technology/2012/03/ask-stack-how-can-i-find-a-good-open-source-project-to-join/" target="_blank" rel="noopener">https://arstechnica.com/information-technology/2012/03/ask-stack-how-can-i-find-a-good-open-source-project-to-join/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2017-Top-15-Python-数据科学类库&quot;&gt;&lt;a href=&quot;#2017-Top-15-Python-数据科学类库&quot; class=&quot;headerlink&quot; title=&quot;2017 Top 15 Python 数据科学类库&quot;&gt;&lt;/a&gt;2017 Top 15 Python 数据科学类库&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/cnblogs/images/digest/google-trends.png&quot; alt=&quot;Google Trends&quot;&gt;&lt;/p&gt;
&lt;p&gt;近年来，Python 在数据科学领域得到了越来越多的关注，本文整理归类了使用率最高的数据科学类库，供大家参考。&lt;/p&gt;
&lt;p&gt;NumPy、SciPy、Pandas 是 Python 数据科学的核心类库。NumPy 提供了 N 维数组、矩阵、向量等数据结构，能够进行高性能的数学运算；SciPy 包含了线性代数、拟合优化、统计学习的通用方法；Pandas 则一般用于数据清洗、探索型分析等工作。&lt;/p&gt;
&lt;p&gt;可视化方面，Matplotlib 是最早流行的类库，提供了丰富的图形化接口，但 API 的使用方式偏底层，需要编写较多代码；Seaborn 构建在 Matplotlib 之上，重新定义了图表样式，更适合在报告、演示文档中使用，并且它还预置了诸多探索型分析函数，可以快速地对数据进行描述性可视化；Bokeh 主打交互性，它运行在浏览器中，让使用者可以方便地调节可视化参数；Plotly 也是一款基于页面的可视化工具，但因为是商业软件，需要授权后才能使用。&lt;/p&gt;
&lt;p&gt;SciKit-Learn 是公认的 Python 机器学习标准类库，它提供了准确、统一的接口，可以方便地使用各种机器学习算法；深度学习领域，Theano 是比较老牌的类库之一，特点是能够运行于不同的系统架构之上（CPU、GPU）；Tensorflow 则是最近较火的基础类库，使用它提供的各种算子和数据流工具，我们可以构建出多层神经网络，在集群上对大数据进行运算；Keras 则是一款较上层的工具库，底层使用 Theano 或 Tensorflow 作为引擎，可以通过快速构建实验来验证模型。&lt;/p&gt;
&lt;p&gt;自然语言处理领域中，NLTK 提供了文本标记、分词、构建语料树等功能，用以揭示句中或句间的依赖关系；Gensim 则擅长构建向量空间模型、话题建模、挖掘大量文本中重复出现的模式，其算法都属于非监督学习，因此只需提供语料库就能得到结果。&lt;/p&gt;
&lt;p&gt;原文：&lt;a href=&quot;http://www.kdnuggets.com/2017/06/top-15-python-libraries-data-science.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.kdnuggets.com/2017/06/top-15-python-libraries-data-science.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Digest" scheme="http://shzhangji.com/cnblogs/categories/Digest/"/>
    
    
      <category term="analytics" scheme="http://shzhangji.com/cnblogs/tags/analytics/"/>
    
      <category term="data science" scheme="http://shzhangji.com/cnblogs/tags/data-science/"/>
    
      <category term="opensource" scheme="http://shzhangji.com/cnblogs/tags/opensource/"/>
    
  </entry>
  
  <entry>
    <title>Hive 窗口与分析型函数</title>
    <link href="http://shzhangji.com/cnblogs/2017/09/05/hive-window-and-analytical-functions/"/>
    <id>http://shzhangji.com/cnblogs/2017/09/05/hive-window-and-analytical-functions/</id>
    <published>2017-09-05T04:17:10.000Z</published>
    <updated>2020-08-22T12:06:11.266Z</updated>
    
    <content type="html"><![CDATA[<p>SQL 结构化查询语言是数据分析领域的重要工具之一。它提供了数据筛选、转换、聚合等操作，并能借助 Hive 和 Hadoop 进行大数据量的处理。但是，传统的 SQL 语句并不能支持诸如分组排名、滑动平均值等计算，原因是 <code>GROUP BY</code> 语句只能为每个分组的数据返回一行结果，而非每条数据一行。幸运的是，新版的 SQL 标准引入了窗口查询功能，使用 <code>WINDOW</code> 语句我们可以基于分区和窗口为每条数据都生成一行结果记录，这一标准也已得到了 Hive 的支持。</p><p><img src="/cnblogs/images/hive-window/window-stock.png" alt="滑动平均值"></p><p>举例来说，我们想要计算表中每只股票的两日滑动平均值，可以编写以下查询语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="string">`date`</span>, <span class="string">`stock`</span>, <span class="string">`close`</span></span><br><span class="line">  ,<span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> <span class="string">`w`</span> <span class="keyword">AS</span> <span class="string">`mavg`</span></span><br><span class="line"><span class="keyword">FROM</span> <span class="string">`t_stock`</span></span><br><span class="line"><span class="keyword">WINDOW</span> <span class="string">`w`</span> <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`date`</span></span><br><span class="line">               <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br></pre></td></tr></table></figure><p><code>OVER</code>、<code>WINDOW</code>、以及 <code>ROWS BETWEEN AND</code> 都是新增的窗口查询关键字。在这个查询中，<code>PARTITION BY</code> 和 <code>ORDER BY</code> 的工作方式与 <code>GROUP BY</code>、<code>ORDER BY</code> 相似，区别在于它们不会将多行记录聚合成一条结果，而是将它们拆分到互不重叠的分区中进行后续处理。其后的 <code>ROWS BETWEEN AND</code> 语句用于构建一个 <em>窗口帧</em>。此例中，每一个窗口帧都包含了当前记录和上一条记录。下文会对窗口帧做进一步描述。最后，<code>AVG</code> 是一个窗口函数，用于计算每个窗口帧的结果。窗口帧的定义（<code>WINDOW</code> 语句）还可以直接附加到窗口函数之后：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span>) <span class="keyword">AS</span> <span class="string">`mavg`</span> <span class="keyword">FROM</span> <span class="string">`t_stock`</span>;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="窗口查询的基本概念"><a href="#窗口查询的基本概念" class="headerlink" title="窗口查询的基本概念"></a>窗口查询的基本概念</h2><p><img src="/cnblogs/images/hive-window/concepts.png" alt="基本概念"></p><p><a href="https://en.wikibooks.org/wiki/Structured_Query_Language/Window_functions" target="_blank" rel="noopener">图片来源</a></p><p>SQL 窗口查询引入了三个新的概念：窗口分区、窗口帧、以及窗口函数。</p><p><code>PARTITION</code> 语句会按照一个或多个指定字段，将查询结果集拆分到不同的 <strong>窗口分区</strong> 中，并可按照一定规则排序。如果没有 <code>PARTITION BY</code>，则整个结果集将作为单个窗口分区；如果没有 <code>ORDER BY</code>，我们则无法定义窗口帧，进而整个分区将作为单个窗口帧进行处理。</p><p><strong>窗口帧</strong> 用于从分区中选择指定的多条记录，供窗口函数处理。Hive 提供了两种定义窗口帧的形式：<code>ROWS</code> 和 <code>RANGE</code>。两种类型都需要配置上界和下界。例如，<code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> 表示选择分区起始记录到当前记录的所有行；<code>SUM(close) RANGE BETWEEN 100 PRECEDING AND 200 FOLLOWING</code> 则通过 <em>字段差值</em> 来进行选择。如当前行的 <code>close</code> 字段值是 <code>200</code>，那么这个窗口帧的定义就会选择分区中 <code>close</code> 字段值落在 <code>100</code> 至 <code>400</code> 区间的记录。以下是所有可能的窗口帧定义组合。如果没有定义窗口帧，则默认为 <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><p><strong>窗口函数</strong> 会基于当前窗口帧的记录计算结果。Hive 提供了以下窗口函数：</p><ul><li><code>FIRST_VALUE(col)</code>, <code>LAST_VALUE(col)</code> 可以返回窗口帧中第一条或最后一条记录的指定字段值；</li><li><code>LEAD(col, n)</code>, <code>LAG(col, n)</code> 返回当前记录的上 <code>n</code> 条或下 <code>n</code> 条记录的字段值；</li><li><code>RANK()</code>, <code>ROW_NUMBER()</code> 会为帧内的每一行返回一个序数，区别在于存在字段值相等的记录时，<code>RANK()</code> 会返回相同的序数；</li><li><code>COUNT()</code>, <code>SUM(col)</code>, <code>MIN(col)</code> 和一般的聚合操作相同。</li></ul><h2 id="Hive-窗口查询示例"><a href="#Hive-窗口查询示例" class="headerlink" title="Hive 窗口查询示例"></a>Hive 窗口查询示例</h2><h3 id="Top-K"><a href="#Top-K" class="headerlink" title="Top K"></a>Top K</h3><p>首先，我们在 Hive 中创建一些有关员工收入的模拟数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t_employee (<span class="keyword">id</span> <span class="built_in">INT</span>, emp_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>), dep_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">salary <span class="built_in">DECIMAL</span>(<span class="number">7</span>, <span class="number">2</span>), age <span class="built_in">DECIMAL</span>(<span class="number">3</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t_employee <span class="keyword">VALUES</span></span><br><span class="line">( <span class="number">1</span>,  <span class="string">'Matthew'</span>, <span class="string">'Management'</span>,  <span class="number">4500</span>, <span class="number">55</span>),</span><br><span class="line">( <span class="number">2</span>,  <span class="string">'Olivia'</span>,  <span class="string">'Management'</span>,  <span class="number">4400</span>, <span class="number">61</span>),</span><br><span class="line">( <span class="number">3</span>,  <span class="string">'Grace'</span>,   <span class="string">'Management'</span>,  <span class="number">4000</span>, <span class="number">42</span>),</span><br><span class="line">( <span class="number">4</span>,  <span class="string">'Jim'</span>,     <span class="string">'Production'</span>,  <span class="number">3700</span>, <span class="number">35</span>),</span><br><span class="line">( <span class="number">5</span>,  <span class="string">'Alice'</span>,   <span class="string">'Production'</span>,  <span class="number">3500</span>, <span class="number">24</span>),</span><br><span class="line">( <span class="number">6</span>,  <span class="string">'Michael'</span>, <span class="string">'Production'</span>,  <span class="number">3600</span>, <span class="number">28</span>),</span><br><span class="line">( <span class="number">7</span>,  <span class="string">'Tom'</span>,     <span class="string">'Production'</span>,  <span class="number">3800</span>, <span class="number">35</span>),</span><br><span class="line">( <span class="number">8</span>,  <span class="string">'Kevin'</span>,   <span class="string">'Production'</span>,  <span class="number">4000</span>, <span class="number">52</span>),</span><br><span class="line">( <span class="number">9</span>,  <span class="string">'Elvis'</span>,   <span class="string">'Service'</span>,     <span class="number">4100</span>, <span class="number">40</span>),</span><br><span class="line">(<span class="number">10</span>,  <span class="string">'Sophia'</span>,  <span class="string">'Sales'</span>,       <span class="number">4300</span>, <span class="number">36</span>),</span><br><span class="line">(<span class="number">11</span>,  <span class="string">'Samantha'</span>,<span class="string">'Sales'</span>,       <span class="number">4100</span>, <span class="number">38</span>);</span><br></pre></td></tr></table></figure><p>我们可以使用 <code>RANK()</code> 函数计算每个部门中谁的收入最高：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dep_name, emp_name, salary</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    dep_name, emp_name, salary</span><br><span class="line">    ,<span class="keyword">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> dep_name <span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">DESC</span>) <span class="keyword">AS</span> rnk</span><br><span class="line">  <span class="keyword">FROM</span> t_employee</span><br><span class="line">) a</span><br><span class="line"><span class="keyword">WHERE</span> rnk = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>通常情况下，<code>RANK()</code> 在遇到相同值时会返回同一个排名，并 <em>跳过</em> 下一个排名序数。如果想保证排名连续，可以改用 <code>DENSE_RANK()</code> 这个函数。</p><h3 id="累积分布"><a href="#累积分布" class="headerlink" title="累积分布"></a>累积分布</h3><p>我们可以计算整个公司员工薪水的累积分布。如，<code>4000</code> 元的累计分布百分比是 <code>0.55</code>，表示有 55% 的员工薪资低于或等于 <code>4000</code> 元。计算时，我们先统计不同薪资的频数，再用窗口查询做一次累计求和操作：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  salary</span><br><span class="line">  ,<span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary)</span><br><span class="line">  / <span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span></span><br><span class="line">                   <span class="keyword">AND</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span>)</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> salary, <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line">  <span class="keyword">FROM</span> t_employee</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> salary</span><br><span class="line">) a;</span><br></pre></td></tr></table></figure><p>我们还可以使用 Hive 提供的 <code>CUME_DIST()</code> 来完成相同的计算。<code>PERCENT_RANK()</code> 函数则可以百分比的形式展现薪资所在排名。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  salary</span><br><span class="line">  ,<span class="keyword">CUME_DIST</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_cum</span><br><span class="line">  ,<span class="keyword">PERCENT_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_rank</span><br><span class="line"><span class="keyword">FROM</span> t_employee;</span><br></pre></td></tr></table></figure><p><img src="/cnblogs/images/hive-window/employee-pct.png" alt="累积分布"></p><h3 id="点击流会话"><a href="#点击流会话" class="headerlink" title="点击流会话"></a>点击流会话</h3><p>我们可以根据点击流的时间间隔来将它们拆分成不同的会话，如超过 30 分钟认为是一次新的会话。我们还将为每个会话赋上自增 ID：</p><p><img src="/cnblogs/images/hive-window/clickstream.png" alt="点击流"></p><p>首先，在子查询 <code>b</code> 中，我们借助 <code>LAG(col)</code> 函数计算出当前行和上一行的时间差，如果大于 30 分钟则标记为新回话的开始。之后，我们对 <code>new_session</code> 字段做累计求和，从而得到一个递增的 ID 序列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  ipaddress, clicktime</span><br><span class="line">  ,<span class="keyword">SUM</span>(<span class="keyword">IF</span>(new_session, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">OVER</span> x + <span class="number">1</span> <span class="keyword">AS</span> sessionid</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    ipaddress, clicktime, ts</span><br><span class="line">    ,ts - LAG(ts) <span class="keyword">OVER</span> w &gt; <span class="number">1800</span> <span class="keyword">AS</span> new_session</span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *, <span class="keyword">UNIX_TIMESTAMP</span>(clicktime) <span class="keyword">AS</span> ts</span><br><span class="line">    <span class="keyword">FROM</span> t_clickstream</span><br><span class="line">  ) a</span><br><span class="line">  <span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts)</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">WINDOW</span> x <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts);</span><br></pre></td></tr></table></figure><h2 id="窗口查询实现细节"><a href="#窗口查询实现细节" class="headerlink" title="窗口查询实现细节"></a>窗口查询实现细节</h2><p>简单来说，窗口查询有两个步骤：将记录分割成多个分区，然后在各个分区上调用窗口函数。分区过程对于了解 MapReduce 的用户应该很容易理解，Hadoop 会负责对记录进行打散和排序。但是，传统的 UDAF 函数只能为每个分区返回一条记录，而我们需要的是不仅输入数据是一张表，输出数据也是一张表（table-in, table-out），因此 Hive 社区引入了分区表函数（PTF）。</p><p>PTF 顾名思义是运行于分区之上、能够处理分区中的记录并输出多行结果的函数。下方的时序图列出了这个过程中重要的一些类。<code>PTFOperator</code> 会读取已经排好序的数据，创建相应的“输入分区”；<code>WindowTableFunction</code> 则负责管理窗口帧、调用窗口函数（UDAF）、并将结果写入“输出分区”。</p><p><img src="/cnblogs/images/hive-window/window-sequence.png" alt="PTF 时序图"></p><p>HIVE-896（<a href="https://issues.apache.org/jira/browse/HIVE-896" target="_blank" rel="noopener">链接</a>）包含了将分析型函数引入 Hive 的讨论过程；这份演示文档（<a href="https://www.slideshare.net/Hadoop_Summit/analytical-queries-with-hive" target="_blank" rel="noopener">链接</a>）则介绍了当时的主要研发团队是如何设计和实现 PTF 的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li><li><a href="https://github.com/hbutani/SQLWindowing" target="_blank" rel="noopener">https://github.com/hbutani/SQLWindowing</a></li><li><a href="https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions" target="_blank" rel="noopener">https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions</a></li><li><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL 结构化查询语言是数据分析领域的重要工具之一。它提供了数据筛选、转换、聚合等操作，并能借助 Hive 和 Hadoop 进行大数据量的处理。但是，传统的 SQL 语句并不能支持诸如分组排名、滑动平均值等计算，原因是 &lt;code&gt;GROUP BY&lt;/code&gt; 语句只能为每个分组的数据返回一行结果，而非每条数据一行。幸运的是，新版的 SQL 标准引入了窗口查询功能，使用 &lt;code&gt;WINDOW&lt;/code&gt; 语句我们可以基于分区和窗口为每条数据都生成一行结果记录，这一标准也已得到了 Hive 的支持。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/hive-window/window-stock.png&quot; alt=&quot;滑动平均值&quot;&gt;&lt;/p&gt;
&lt;p&gt;举例来说，我们想要计算表中每只股票的两日滑动平均值，可以编写以下查询语句：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ,&lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;WINDOW&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               &lt;span class=&quot;keyword&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;PRECEDING&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ROW&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;OVER&lt;/code&gt;、&lt;code&gt;WINDOW&lt;/code&gt;、以及 &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; 都是新增的窗口查询关键字。在这个查询中，&lt;code&gt;PARTITION BY&lt;/code&gt; 和 &lt;code&gt;ORDER BY&lt;/code&gt; 的工作方式与 &lt;code&gt;GROUP BY&lt;/code&gt;、&lt;code&gt;ORDER BY&lt;/code&gt; 相似，区别在于它们不会将多行记录聚合成一条结果，而是将它们拆分到互不重叠的分区中进行后续处理。其后的 &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; 语句用于构建一个 &lt;em&gt;窗口帧&lt;/em&gt;。此例中，每一个窗口帧都包含了当前记录和上一条记录。下文会对窗口帧做进一步描述。最后，&lt;code&gt;AVG&lt;/code&gt; 是一个窗口函数，用于计算每个窗口帧的结果。窗口帧的定义（&lt;code&gt;WINDOW&lt;/code&gt; 语句）还可以直接附加到窗口函数之后：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="hive" scheme="http://shzhangji.com/cnblogs/tags/hive/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/cnblogs/tags/analytics/"/>
    
      <category term="sql" scheme="http://shzhangji.com/cnblogs/tags/sql/"/>
    
  </entry>
  
  <entry>
    <title>实时计算工具库 stream-lib 使用指南</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/</id>
    <published>2017-08-27T05:47:16.000Z</published>
    <updated>2020-08-22T12:06:11.265Z</updated>
    
    <content type="html"><![CDATA[<p>进行大数据处理时，计算唯一值、95% 分位数等操作非常占用空间和时间。但有时我们只是想对数据集有一个概略的了解，数值的准确性并不那么重要。实时监控系统中也是如此，可以容忍一定的错误率。目前已经有许多算法可以通过牺牲准确性来减少计算所需的空间和时间，这些算法大多支持数据结构之间的合并，因此可以方便地用在实时计算中。<a href="https://github.com/addthis/stream-lib" target="_blank" rel="noopener"><code>stream-lib</code></a> 就是一个集成了很多此类算法的实时计算工具库，是对现有研究成果的 Java 实现。本文就将介绍这一工具库的使用方法。</p><h2 id="唯一值计算-HyperLogLog"><a href="#唯一值计算-HyperLogLog" class="headerlink" title="唯一值计算 HyperLogLog"></a>唯一值计算 <code>HyperLogLog</code></h2><p>独立访客（UV）是网站的重要指标之一。我们通常会为每一个用户生成一个 UUID，并在 HTTP Cookie 中记录和跟踪，或直接使用 IP 地址做近似计算。我们可以使用一个 <code>HashSet</code> 来计算 UV 的准确值，但无疑会占用大量的空间。<code>HyperLogLog</code> 则是一种近似算法，用于解决此类唯一值计算的问题。该算法<a href="https://en.wikipedia.org/wiki/HyperLogLog" target="_blank" rel="noopener">在对超过 10^9 个唯一值进行计算时可以做到 2% 的标准差，并只占用 1.5 kB 内存</a>。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.clearspring.analytics<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>stream<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ICardinality card = <span class="keyword">new</span> HyperLogLog(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span> &#125;) &#123;</span><br><span class="line">    card.offer(i);</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(card.cardinality()); <span class="comment">// 4</span></span><br></pre></td></tr></table></figure><a id="more"></a><p><code>HyperLogLog</code> 会计算每一个成员二进制值首位有多少个零，如果零的最大个数是 <code>n</code>，则唯一值数量就是 <code>2^n</code>。算法中有两个关键点，首先，成员的值必须是服从正态分布的，这一点可以通过哈希函数实现。<code>stream-lib</code> 使用的是 <a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="noopener">MurmurHash</a>，它简单、快速、且符合分布要求，应用于多种基于哈希查询的算法。其次，为了降低计算结果的方差，集合成员会先被拆分成多个子集合，最后的唯一值数量是各个子集合结果的调和平均数。上文代码中，我们传递给 <code>HyperLogLog</code> 构造函数的整型参数就表示会采用多少个二进制位来进行分桶。最后，准确性可以通过这个公式计算：<code>1.04/sqrt(2^log2m)</code>。</p><p><code>HyperLogLog</code> 是对 <code>LogLog</code> 算法的扩展，而 <code>HyperLogLogPlus</code> 则包含了更多优化策略。比如，它使用了 64 位的哈希函数，以减少哈希碰撞；对于唯一值数较小的集合，会引入纠偏机制；此外，它还对子集合的存储方式做了改进，能够从稀疏型的数据结构逐渐扩展为密集型。这几种算法都已包含在 <code>stream-lib</code> 中。</p><h2 id="集合成员测试-BloomFilter"><a href="#集合成员测试-BloomFilter" class="headerlink" title="集合成员测试 BloomFilter"></a>集合成员测试 <code>BloomFilter</code></h2><p><img src="/cnblogs/images/stream-lib/bloom-filter.jpg" alt="Bloom Filter"></p><p><code>BloomFilter</code> 用于检测一个元素是否包含在集合中，是一种广泛应用的数据结构。它的特点是有一定几率误报（False Positive Probability, FPP），但绝不会漏报（False Negative）。举例来说，Chrome 在检测恶意 URL 时，会在本地维护一个布隆过滤器。当用户输入一个 URL 时，如果布隆过滤器说它不在恶意网址库里，则它一定不在；如果返回结果为真，则 Chrome 会进一步请求服务器以确认是否的确是恶意网址，并提示给用户。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Filter filter = <span class="keyword">new</span> BloomFilter(<span class="number">100</span>, <span class="number">0.01</span>);</span><br><span class="line">filter.add(<span class="string">"google.com"</span>);</span><br><span class="line">filter.add(<span class="string">"twitter.com"</span>);</span><br><span class="line">filter.add(<span class="string">"facebook.com"</span>);</span><br><span class="line">System.out.println(filter.isPresent(<span class="string">"bing.com"</span>)); <span class="comment">// false</span></span><br></pre></td></tr></table></figure><p>布隆过滤器的构造过程比较简单：</p><ul><li>创建一个包含 <code>n</code> 个元素的位数组，Java 中可以直接使用 <a href="https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html" target="_blank" rel="noopener"><code>BitSet</code></a>；</li><li>使用 <code>k</code> 个哈希函数对新元素进行处理，结果更新到数组的对应位置中；</li><li>当需要测试一个元素是否在集合中时，同样进行 <code>k</code> 次哈希：<ul><li>若哈希结果的每一位都命中了，那这个元素就有可能会在集合中（False Positive）；</li><li>如果不是所有的比特位都命中，则该元素一定不在集合中。</li></ul></li></ul><p>同样，这些哈希函数必须是服从正态分布的，且要做到两两之间相互独立。Murmur 哈希算法能够满足这一要求。FPP 的计算公式为 <code>(1-e^(-kn/m))^k</code>，这个页面（<a href="https://llimllib.github.io/bloomfilter-tutorial/" target="_blank" rel="noopener">链接</a>）提供了在线的布隆过滤器可视化过程。这一算法的其它应用场景有：邮件服务器中用来判别垃圾发件人；Cassandra、HBase 会用它来过滤不存在的记录行；Squid 则基于布隆过滤器实现了<a href="https://wiki.squid-cache.org/SquidFaq/CacheDigests" target="_blank" rel="noopener">缓存摘要</a>。</p><h2 id="Top-K-排名-CountMinSketch"><a href="#Top-K-排名-CountMinSketch" class="headerlink" title="Top K 排名 CountMinSketch"></a>Top K 排名 <code>CountMinSketch</code></h2><p><img src="/cnblogs/images/stream-lib/count-min-sketch.png" alt="Count Min Sketch"></p><p><a href="https://stackoverflow.com/a/35356116/1030720" target="_blank" rel="noopener">图片来源</a></p><p><a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch" target="_blank" rel="noopener"><code>CountMinSketch</code></a> 是一种“速写”算法，能够使用较小的空间勾勒出数据集内各类事件的频次。比如，我们可以统计出当前最热门的推特内容，或是计算网站访问量最大的页面。当然，这一算法同样会牺牲一定的准确性。</p><p>下面这段代码演示的是如何使用 <code>stream-lib</code> 来统计数据量最多的记录：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; animals;</span><br><span class="line">IFrequency freq = <span class="keyword">new</span> CountMinSketch(<span class="number">10</span>, <span class="number">5</span>, <span class="number">0</span>);</span><br><span class="line">Map&lt;String, Long&gt; top = Collections.emptyMap();</span><br><span class="line"><span class="keyword">for</span> (String animal : animals) &#123;</span><br><span class="line">    freq.add(animal, <span class="number">1</span>);</span><br><span class="line">    top = Stream.concat(top.keySet().stream(), Stream.of(animal)).distinct()</span><br><span class="line">              .map(a -&gt; <span class="keyword">new</span> SimpleEntry&lt;String, Long&gt;(a, freq.estimateCount(a)))</span><br><span class="line">              .sorted(Comparator.comparing(SimpleEntry&lt;String, Long&gt;::getValue).reversed())</span><br><span class="line">              .limit(<span class="number">3</span>)</span><br><span class="line">              .collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(top); <span class="comment">// &#123;rabbit=25, bird=45, spider=35&#125;</span></span><br></pre></td></tr></table></figure><p><code>CountMinSketch#estimateCount</code> 方法又称为 <em>点查询</em> ，用来读取“速写”中某一事件的频次。由于数据结构中无法记录具体的值，我们需要在另行编写代码来实现。</p><p><code>CountMinSketch</code> 的数据结构和布隆过滤器类似，只不过它会使用 <code>d</code> 个 <code>w</code> 位的数组，从而组成一个 <code>d x w</code> 的矩阵。加入新值时，该算法会对其应用 <code>d</code> 个哈希函数，并更新到矩阵的相应位置。这些哈希函数只需做到<a href="https://en.wikipedia.org/wiki/Pairwise_independence" target="_blank" rel="noopener">两两独立</a>即可，因此 <code>stream-lib</code> 使用了一种简单而快速的算法：<code>(a*x+b) mod p</code>。在进行 <em>点查询</em> 时，同样计算该值的哈希结果，找到矩阵中最小的值，即是它的频次。</p><p>这一算法的误差是 <code>ε = e / w</code>，误差概率是 <code>δ = 1 / e ^ d</code>。因此，我们可以通过增加 <code>w</code> 或 <code>d</code> 来提升计算精度。算法论文可以查看这个<a href="https://web.archive.org/web/20060907232042/http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf" target="_blank" rel="noopener">链接</a>。</p><h2 id="分位数计算-T-Digest"><a href="#分位数计算-T-Digest" class="headerlink" title="分位数计算  T-Digest"></a>分位数计算  <code>T-Digest</code></h2><p><img src="/cnblogs/images/stream-lib/t-digest.png" alt="T-Digest"></p><p><a href="https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest" target="_blank" rel="noopener">图片来源</a></p><p>中位数、95% 分位数，这类计算在描述性统计中很常见。相较于平均数，中位数不会受到异常值的影响，但它的计算过程比较复杂，需要保留所有具体值，排序后取得中间位置的数作为结果。<code>T-Digest</code> 算法则通过一定计算，将数据集的分布情况粗略地记录下来，从而估计出指定的分位数值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Random rand = <span class="keyword">new</span> Random();</span><br><span class="line">List&lt;Double&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">TDigest digest = <span class="keyword">new</span> TDigest(<span class="number">100</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; ++i) &#123;</span><br><span class="line">    <span class="keyword">double</span> d = rand.nextDouble();</span><br><span class="line">    data.add(d);</span><br><span class="line">    digest.add(d);</span><br><span class="line">&#125;</span><br><span class="line">Collections.sort(data);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">double</span> q : <span class="keyword">new</span> <span class="keyword">double</span>[] &#123; <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span> &#125;) &#123;</span><br><span class="line">    System.out.println(String.format(<span class="string">"quantile=%.1f digest=%.4f exact=%.4f"</span>,</span><br><span class="line">            q, digest.quantile(q), data.get((<span class="keyword">int</span>) (data.size() * q))));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// quantile=0.1 digest=0.0998 exact=0.1003</span></span><br><span class="line"><span class="comment">// quantile=0.5 digest=0.5009 exact=0.5000</span></span><br><span class="line"><span class="comment">// quantile=0.9 digest=0.8994 exact=0.8998</span></span><br></pre></td></tr></table></figure><p><code>T-Digest</code> 的论文可以在这个<a href="https://raw.githubusercontent.com/tdunning/t-digest/master/docs/t-digest-paper/histo.pdf" target="_blank" rel="noopener">链接</a>中找到。简单来说，该算法使用了类似一维 k-means 聚类的过程，将真实的分布情况用若干中心点（Centroid）来描述。此外，不同的 <code>T-Digest</code> 实例之间可以进行合并，得到一个体积略大、但准确性更高的实例，这一点非常适用于并行计算。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们可以看到，本文提到的大部分算法都是通过牺牲准确性来提升时间与空间的利用效率的。通过对数据集进行“速写”，抓住其中的“特征”，我们就能给出不错的估计结果。<code>stream-lib</code> 以及其它开源项目能够让我们非常便捷地将这类算法应用到实际问题中。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5" target="_blank" rel="noopener">https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5</a></li><li><a href="http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/" target="_blank" rel="noopener">http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/</a></li><li><a href="https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about" target="_blank" rel="noopener">https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;进行大数据处理时，计算唯一值、95% 分位数等操作非常占用空间和时间。但有时我们只是想对数据集有一个概略的了解，数值的准确性并不那么重要。实时监控系统中也是如此，可以容忍一定的错误率。目前已经有许多算法可以通过牺牲准确性来减少计算所需的空间和时间，这些算法大多支持数据结构之间的合并，因此可以方便地用在实时计算中。&lt;a href=&quot;https://github.com/addthis/stream-lib&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;stream-lib&lt;/code&gt;&lt;/a&gt; 就是一个集成了很多此类算法的实时计算工具库，是对现有研究成果的 Java 实现。本文就将介绍这一工具库的使用方法。&lt;/p&gt;
&lt;h2 id=&quot;唯一值计算-HyperLogLog&quot;&gt;&lt;a href=&quot;#唯一值计算-HyperLogLog&quot; class=&quot;headerlink&quot; title=&quot;唯一值计算 HyperLogLog&quot;&gt;&lt;/a&gt;唯一值计算 &lt;code&gt;HyperLogLog&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;独立访客（UV）是网站的重要指标之一。我们通常会为每一个用户生成一个 UUID，并在 HTTP Cookie 中记录和跟踪，或直接使用 IP 地址做近似计算。我们可以使用一个 &lt;code&gt;HashSet&lt;/code&gt; 来计算 UV 的准确值，但无疑会占用大量的空间。&lt;code&gt;HyperLogLog&lt;/code&gt; 则是一种近似算法，用于解决此类唯一值计算的问题。该算法&lt;a href=&quot;https://en.wikipedia.org/wiki/HyperLogLog&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;在对超过 10^9 个唯一值进行计算时可以做到 2% 的标准差，并只占用 1.5 kB 内存&lt;/a&gt;。&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;com.clearspring.analytics&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;stream&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;2.9.5&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ICardinality card = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; HyperLogLog(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i : &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;[] &amp;#123; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt; &amp;#125;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    card.offer(i);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;System.out.println(card.cardinality()); &lt;span class=&quot;comment&quot;&gt;// 4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/cnblogs/tags/stream-processing/"/>
    
      <category term="algorithm" scheme="http://shzhangji.com/cnblogs/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>使用 Binlog 和 Canal 从 MySQL 抽取数据</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/13/extract-data-from-mysql-with-binlog-and-canal/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/13/extract-data-from-mysql-with-binlog-and-canal/</id>
    <published>2017-08-13T02:06:58.000Z</published>
    <updated>2020-08-22T12:06:11.265Z</updated>
    
    <content type="html"><![CDATA[<p>数据抽取是 ETL 流程的第一步。我们会将数据从 RDBMS 或日志服务器等外部系统抽取至数据仓库，进行清洗、转换、聚合等操作。在现代网站技术栈中，MySQL 是最常见的数据库管理系统，我们会从多个不同的 MySQL 实例中抽取数据，存入一个中心节点，或直接进入 Hive。市面上已有多种成熟的、基于 SQL 查询的抽取软件，如著名的开源项目 <a href="http://sqoop.apache.org/" target="_blank" rel="noopener">Apache Sqoop</a>，然而这些工具并不支持实时的数据抽取。MySQL Binlog 则是一种实时的数据流，用于主从节点之间的数据复制，我们可以利用它来进行数据抽取。借助阿里巴巴开源的 <a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">Canal</a> 项目，我们能够非常便捷地将 MySQL 中的数据抽取到任意目标存储中。</p><p><img src="/cnblogs/images/canal.png" alt="Canal"></p><h2 id="Canal-的组成部分"><a href="#Canal-的组成部分" class="headerlink" title="Canal 的组成部分"></a>Canal 的组成部分</h2><p>简单来说，Canal 会将自己伪装成 MySQL 从节点（Slave），并从主节点（Master）获取 Binlog，解析和贮存后供下游消费端使用。Canal 包含两个组成部分：服务端和客户端。服务端负责连接至不同的 MySQL 实例，并为每个实例维护一个事件消息队列；客户端则可以订阅这些队列中的数据变更事件，处理并存储到数据仓库中。下面我们来看如何快速搭建起一个 Canal 服务。</p><a id="more"></a><h3 id="配置-MySQL-主节点"><a href="#配置-MySQL-主节点" class="headerlink" title="配置 MySQL 主节点"></a>配置 MySQL 主节点</h3><p>MySQL 默认没有开启 Binlog，因此我们需要对 <code>my.cnf</code> 文件做以下修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server-id = 1</span><br><span class="line">log_bin = /path/to/mysql-bin.log</span><br><span class="line">binlog_format = ROW</span><br></pre></td></tr></table></figure><p>注意 <code>binlog_format</code> 必须设置为 <code>ROW</code>, 因为在 <code>STATEMENT</code> 或 <code>MIXED</code> 模式下, Binlog 只会记录和传输 SQL 语句（以减少日志大小），而不包含具体数据，我们也就无法保存了。</p><p>从节点通过一个专门的账号连接主节点，这个账号需要拥有全局的 <code>REPLICATION</code> 权限。我们可以使用 <code>GRANT</code> 命令创建这样的账号：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span></span><br><span class="line"><span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</span><br></pre></td></tr></table></figure><h3 id="启动-Canal-服务端"><a href="#启动-Canal-服务端" class="headerlink" title="启动 Canal 服务端"></a>启动 Canal 服务端</h3><p>从 GitHub 项目发布页中下载 Canal 服务端代码（<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">链接</a>），配置文件在 <code>conf</code> 文件夹下，有以下目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">canal.deployer/conf/canal.properties</span><br><span class="line">canal.deployer/conf/instanceA/instance.properties</span><br><span class="line">canal.deployer/conf/instanceB/instance.properties</span><br></pre></td></tr></table></figure><p><code>conf/canal.properties</code> 是主配置文件，如其中的 <code>canal.port</code> 用以指定服务端监听的端口。<code>instanceA/instance.properties</code> 则是各个实例的配置文件，主要的配置项有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># slaveId 不能与 my.cnf 中的 server-id 项重复</span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line">canal.instance.master.address = 127.0.0.1:3306</span><br><span class="line">canal.instance.dbUsername = canal</span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line">canal.instance.connectionCharset = UTF-8</span><br><span class="line"># 订阅实例中所有的数据库和表</span><br><span class="line">canal.instance.filter.regex = .*\\..*</span><br></pre></td></tr></table></figure><p>执行 <code>sh bin/startup.sh</code> 命令开启服务端，在日志文件 <code>logs/example/example.log</code> 中可以看到以下输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Loading properties file from class path resource [canal.properties]</span><br><span class="line">Loading properties file from class path resource [example/instance.properties]</span><br><span class="line">start CannalInstance for 1-example</span><br><span class="line">[destination = example , address = /127.0.0.1:3306 , EventParser] prepare to find start position just show master status</span><br></pre></td></tr></table></figure><h3 id="编写-Canal-客户端"><a href="#编写-Canal-客户端" class="headerlink" title="编写 Canal 客户端"></a>编写 Canal 客户端</h3><p>从服务端消费变更消息时，我们需要创建一个 Canal 客户端，指定需要订阅的数据库和表，并开启轮询。</p><p>首先，在项目中添加 <code>com.alibaba.otter:canal.client</code> 依赖项，构建 <code>CanalConnector</code> 实例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">CanalConnector connector = CanalConnectors.newSingleConnector(</span><br><span class="line">        <span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">11111</span>), <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line">connector.connect();</span><br><span class="line">connector.subscribe(<span class="string">".*\\..*"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    Message message = connector.getWithoutAck(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">long</span> batchId = message.getId();</span><br><span class="line">    <span class="keyword">if</span> (batchId == -<span class="number">1</span> || message.getEntries().isEmpty()) &#123;</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        printEntries(message.getEntries());</span><br><span class="line">        connector.ack(batchId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码和连接消息系统很相似。变更事件会批量发送过来，待处理完毕后我们可以 ACK 这一批次，从而避免消息丢失。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// printEntries</span></span><br><span class="line">RowChange rowChange = RowChange.parseFrom(entry.getStoreValue());</span><br><span class="line"><span class="keyword">for</span> (RowData rowData : rowChange.getRowDatasList()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (rowChange.getEventType() == EventType.INSERT) &#123;</span><br><span class="line">      printColumns(rowData.getAfterCollumnList());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每一个 <code>Entry</code> 代表一组具有相同变更类型的数据列表，如 INSERT 类型、UPDATE、DELETE 等。每一行数据我们都可以获取到各个字段的信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// printColumns</span></span><br><span class="line">String line = columns.stream()</span><br><span class="line">        .map(column -&gt; column.getName() + <span class="string">"="</span> + column.getValue())</span><br><span class="line">        .collect(Collectors.joining(<span class="string">","</span>));</span><br><span class="line">System.out.println(line);</span><br></pre></td></tr></table></figure><p>完整代码可以在 GitHub 中找到（<a href="https://github.com/jizhang/java-sandbox/blob/blog-canal/src/main/java/com/shzhangji/javasandbox/canal/SimpleClient.java" target="_blank" rel="noopener">链接</a>）。</p><h2 id="加载至数据仓库"><a href="#加载至数据仓库" class="headerlink" title="加载至数据仓库"></a>加载至数据仓库</h2><h3 id="关系型数据库与批量更新"><a href="#关系型数据库与批量更新" class="headerlink" title="关系型数据库与批量更新"></a>关系型数据库与批量更新</h3><p>若数据仓库是基于关系型数据库的，我们可以直接使用 <code>REPLACE</code> 语句将数据变更写入目标表。其中需要注意的是写入性能，在更新较频繁的场景下，我们通常会缓存一段时间的数据，并批量更新至数据库，如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">REPLACE</span> <span class="keyword">INTO</span> <span class="string">`user`</span> (<span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span>) <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">30</span>, <span class="string">'2017-08-12 16:00:00'</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'Mary'</span>, <span class="number">28</span>, <span class="string">'2017-08-12 17:00:00'</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">'Tom'</span>, <span class="number">36</span>, <span class="string">'2017-08-12 18:00:00'</span>);</span><br></pre></td></tr></table></figure><p>另一种方式是将数据变更写入按分隔符分割的文本文件，并用 <code>LOAD DATA</code> 语句载入数据库。这些文件也可以用在需要写入 Hive 的场景中。不管使用哪一种方法，请一定注意要对字符串类型的字段进行转义，避免导入时出错。</p><h3 id="基于-Hive-的数据仓库"><a href="#基于-Hive-的数据仓库" class="headerlink" title="基于 Hive 的数据仓库"></a>基于 Hive 的数据仓库</h3><p>Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase。</p><p>数据可以归类成基础数据和增量数据。如昨日的 <code>user</code> 表是基础数据，今日变更的行是增量数据。通过 <code>FULL OUTER JOIN</code>，我们可以将基础和增量数据合并成一张最新的数据表，并作为明天的基础数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="keyword">COALESCE</span>(b.<span class="string">`id`</span>, a.<span class="string">`id`</span>) <span class="keyword">AS</span> <span class="string">`id`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`name`</span>, a.<span class="string">`name`</span>) <span class="keyword">AS</span> <span class="string">`name`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`age`</span>, a.<span class="string">`age`</span>) <span class="keyword">AS</span> <span class="string">`age`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`updated`</span>, a.<span class="string">`updated`</span>) <span class="keyword">AS</span> <span class="string">`updated`</span></span><br><span class="line"><span class="keyword">FROM</span> dw_stage.<span class="string">`user`</span> a</span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line">  <span class="comment">-- 增量数据会包含重复数据，因此需要选择最新的那一条</span></span><br><span class="line">  <span class="keyword">SELECT</span> <span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span></span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *, ROW_NUMBER() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`id`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`updated`</span> <span class="keyword">DESC</span>) <span class="keyword">AS</span> <span class="string">`n`</span></span><br><span class="line">    <span class="keyword">FROM</span> dw_stage.<span class="string">`user_delta`</span></span><br><span class="line">  ) b</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="string">`n`</span> = <span class="number">1</span></span><br><span class="line">) b</span><br><span class="line"><span class="keyword">ON</span> a.<span class="string">`id`</span> = b.<span class="string">`id`</span>;</span><br></pre></td></tr></table></figure><p>Hive 0.13 引入了事务和 ACID 表，0.14 开始支持 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 语句，Hive 2.0.0 则又新增了 <a href="https://cwiki.apache.org/confluence/display/Hive/HCatalog+Streaming+Mutation+API" target="_blank" rel="noopener">Streaming Mutation API</a>，用以通过编程的方式批量更新 Hive 表中的记录。目前，ACID 表必须使用 ORC 文件格式进行存储，且须按主键进行分桶（Bucket）。Hive 会将变更记录保存在增量文件中，当 <code>OrcInputFormat</code> 读取数据时会自动定位到最新的那条记录。官方案例可以在这个<a href="https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java" target="_blank" rel="noopener">链接</a>中查看。</p><p>最后，我们可以使用 HBase 来实现表数据的更新，它是一种 KV 存储系统，同样基于 HDFS。HBase 的数据可以直接为 MapReduce 脚本使用，且 Hive 中可以创建外部映射表指向 HBase。更多信息请查看<a href="http://hbase.apache.org/" target="_blank" rel="noopener">官方网站</a>。</p><h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>数据抽取通常是按需进行的，在新增一张表时，数据源中可能已经有大量原始记录了。常见的做法是手工将这批数据全量导入至目标表中，但我们也可以复用 Canal 这套机制来实现历史数据的抽取。</p><p>首先，我们在数据源库中创建一张辅助表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`retl_buffer`</span> (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span></span><br><span class="line">  ,table_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">  ,pk_value <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>当需要全量抽取 <code>user</code> 表时，我们执行以下语句，将所有 <code>user.id</code> 写入辅助表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`retl_buffer`</span> (<span class="string">`table_name`</span>, <span class="string">`pk_value`</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="string">'user'</span>, <span class="string">`id`</span> <span class="keyword">FROM</span> <span class="string">`user`</span>;</span><br></pre></td></tr></table></figure><p>Canal 客户端在处理到 <code>retl_buffer</code> 表的数据变更时，可以从中解析出表名和主键的值，直接反查数据源，将数据写入目标表：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="string">"retl_buffer"</span>.equals(entry.getHeader().getTableName())) &#123;</span><br><span class="line">    String tableName = rowData.getAfterColumns(<span class="number">1</span>).getValue();</span><br><span class="line">    String pkValue = rowData.getAfterColumns(<span class="number">2</span>).getValue();</span><br><span class="line">    System.out.println(<span class="string">"SELECT * FROM "</span> + tableName + <span class="string">" WHERE id = "</span> + pkValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这一方法在阿里巴巴的另一个开源软件 <a href="https://github.com/alibaba/otter/wiki/Manager%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%87%AA-%E7%94%B1-%E9%97%A8" target="_blank" rel="noopener">Otter</a> 中使用。</p><h2 id="Canal-高可用"><a href="#Canal-高可用" class="headerlink" title="Canal 高可用"></a>Canal 高可用</h2><ul><li>Canal 服务端中的实例可以配置一个备用 MySQL，从而能够在双 Master 场景下自动选择正在工作的数据源。注意两台主库都需要打开 <code>log_slave_updates</code> 选项。Canal 会使用自己的心跳机制（定期更新辅助表的记录）来检测主库的存活。</li><li>Canal 自身也有 HA 配置，配合 Zookeeper，我们可以开启多个 Canal 服务端，当某台服务器宕机时，客户端可以从 ZK 中获取新的服务端地址，继续进行消费。更多信息可以参考 <a href="https://github.com/alibaba/canal/wiki/AdminGuide" target="_blank" rel="noopener">Canal AdminGuide</a>。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/alibaba/canal/wiki" target="_blank" rel="noopener">https://github.com/alibaba/canal/wiki</a></li><li><a href="https://github.com/alibaba/otter/wiki" target="_blank" rel="noopener">https://github.com/alibaba/otter/wiki</a></li><li><a href="https://www.phdata.io/4-strategies-for-updating-hive-tables/" target="_blank" rel="noopener">https://www.phdata.io/4-strategies-for-updating-hive-tables/</a></li><li><a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/" target="_blank" rel="noopener">https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据抽取是 ETL 流程的第一步。我们会将数据从 RDBMS 或日志服务器等外部系统抽取至数据仓库，进行清洗、转换、聚合等操作。在现代网站技术栈中，MySQL 是最常见的数据库管理系统，我们会从多个不同的 MySQL 实例中抽取数据，存入一个中心节点，或直接进入 Hive。市面上已有多种成熟的、基于 SQL 查询的抽取软件，如著名的开源项目 &lt;a href=&quot;http://sqoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Sqoop&lt;/a&gt;，然而这些工具并不支持实时的数据抽取。MySQL Binlog 则是一种实时的数据流，用于主从节点之间的数据复制，我们可以利用它来进行数据抽取。借助阿里巴巴开源的 &lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Canal&lt;/a&gt; 项目，我们能够非常便捷地将 MySQL 中的数据抽取到任意目标存储中。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/canal.png&quot; alt=&quot;Canal&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Canal-的组成部分&quot;&gt;&lt;a href=&quot;#Canal-的组成部分&quot; class=&quot;headerlink&quot; title=&quot;Canal 的组成部分&quot;&gt;&lt;/a&gt;Canal 的组成部分&lt;/h2&gt;&lt;p&gt;简单来说，Canal 会将自己伪装成 MySQL 从节点（Slave），并从主节点（Master）获取 Binlog，解析和贮存后供下游消费端使用。Canal 包含两个组成部分：服务端和客户端。服务端负责连接至不同的 MySQL 实例，并为每个实例维护一个事件消息队列；客户端则可以订阅这些队列中的数据变更事件，处理并存储到数据仓库中。下面我们来看如何快速搭建起一个 Canal 服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="mysql" scheme="http://shzhangji.com/cnblogs/tags/mysql/"/>
    
      <category term="etl" scheme="http://shzhangji.com/cnblogs/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="canal" scheme="http://shzhangji.com/cnblogs/tags/canal/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flume 如何解析消息中的事件时间</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/06/how-to-extract-event-time-in-apache-flume/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/06/how-to-extract-event-time-in-apache-flume/</id>
    <published>2017-08-06T01:09:06.000Z</published>
    <updated>2020-08-22T12:06:11.264Z</updated>
    
    <content type="html"><![CDATA[<p>数据开发工作中，从上游消息队列抽取数据是一项常规的 ETL 流程。在基于 Hadoop 构建的数据仓库体系中，我们通常会使用 Flume 将事件日志从 Kafka 抽取到 HDFS，然后针对其开发 MapReduce 脚本，或直接创建以时间分区的 Hive 外部表。这项流程中的关键一环是提取日志中的事件时间，因为实时数据通常会包含延迟，且在系统临时宕机的情况下，我们需要追回遗漏的数据，因而使用的时间戳必须是事件产生的时间。Flume 提供的诸多工具能帮助我们非常便捷地实现这一点。</p><p><img src="/cnblogs/images/flume.png" alt="Apache Flume"></p><h2 id="HDFS-Sink-和时间戳头信息"><a href="#HDFS-Sink-和时间戳头信息" class="headerlink" title="HDFS Sink 和时间戳头信息"></a>HDFS Sink 和时间戳头信息</h2><p>以下是一个基本的 HDFS Sink 配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks = k1</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d</span><br></pre></td></tr></table></figure><p><code>%Y%m%d</code> 是该 Sink 支持的时间占位符，它会使用头信息中 <code>timestamp</code> 的值来替换这些占位符。HDFS Sink 还提供了 <code>hdfs.useLocalTimeStamp</code> 选项，直接使用当前系统时间来替换时间占位符，但这并不是我们想要达到的目的。</p><p>我们还可以使用 Hive Sink 直接将事件日志导入成 Hive 表，它能直接和 Hive 元数据库通信，自动创建表分区，并支持分隔符分隔和 JSON 两种序列化形式。当然，它同样需要一个 <code>timestamp</code> 头信息。不过，我们没有选择 Hive Sink，主要出于以下原因：</p><ul><li>它不支持正则表达式，因此我们无法从类似访问日志这样的数据格式中提取字段列表；</li><li>它所提取的字段列表是根据 Hive 表信息产生的。假设上游数据源在 JSON 日志中加入了新的键值，直至我们主动更新 Hive 元信息，这些新增字段将被直接丢弃。对于数据仓库来说，完整保存原始数据是很有必要的。</li></ul><a id="more"></a><h2 id="正则表达式拦截器"><a href="#正则表达式拦截器" class="headerlink" title="正则表达式拦截器"></a>正则表达式拦截器</h2><p>Flume 提供了拦截器机制，我们可以在 Source 之后接上一系列操作，对数据进行基础的转换。例如，<code>TimestampInterceptor</code> 拦截器可以在消息头信息中增加当前时间。在本节中，我将演示如何借助拦截器来提取访问日志型和 JSON 型消息中的事件时间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.123 [2017-06-27 09:08:00] GET /</span><br><span class="line">0.234 [2017-06-27 09:08:01] GET /</span><br></pre></td></tr></table></figure><p><a href="http://flume.apache.org/FlumeUserGuide.html#regex-extractor-interceptor" target="_blank" rel="noopener"><code>RegexExtractorInterceptor</code></a> 可以基于正则表达式来提取字符串，配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex = \\[(.*?)\\]</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers = s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.pattern = yyyy-MM-dd HH:mm:ss</span><br></pre></td></tr></table></figure><p>它会搜索字符串中满足 <code>\[(.*?)\]</code> 模式的子串，将第一个子模式即 <code>s1</code> 作为日期字符串进行解析，并将其转化成毫秒级时间戳，存入头信息 <code>timestamp</code>。</p><h3 id="搜索与替换拦截器"><a href="#搜索与替换拦截器" class="headerlink" title="搜索与替换拦截器"></a>搜索与替换拦截器</h3><p>对于 JSON 数据:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525680.023</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525681.349</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</span><br></pre></td></tr></table></figure><p>我们同样可以用正则拦截器将 <code>actionTime</code> 提取出来，但要注意的是该字段的单位是秒，而 HDFS Sink 要求的是毫秒，这就需要我们在提取之间对其进行转换，如直接将 <code>.</code> 去掉。<code>SearchAndReplaceInterceptor</code> 可以做到这一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type = search_replace</span><br><span class="line">a1.sources.r1.interceptors.i1.searchPattern = \&quot;actionTime\&quot;:(\\d+)\\.(\\d+)</span><br><span class="line">a1.sources.r1.interceptors.i1.replaceString = \&quot;actionTime\&quot;:$1$2</span><br><span class="line">a1.sources.r1.interceptors.i2.type = regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i2.regex = \&quot;actionTime\&quot;:(\\d+)</span><br><span class="line">a1.sources.r1.interceptors.i2.serializers = s1</span><br><span class="line">a1.sources.r1.interceptors.i2.serializers.s1.name = timestamp</span><br></pre></td></tr></table></figure><p>这里我们串联了两个拦截器。首先使用正则替换将 <code>1498525680.023</code> 转换成 <code>1498525680023</code>，再用正则提取出 <code>actionTime</code> 并存入头信息。</p><h3 id="自定义拦截器"><a href="#自定义拦截器" class="headerlink" title="自定义拦截器"></a>自定义拦截器</h3><p>我们还可以编写自定义的拦截器，从而一次性完成提取、转换和更新操作。我们只需实现 <code>org.apache.flume.interceptor.Interceptor</code> 接口的 <code>intercept</code> 方法即可，源代码可以在 GitHub（<a href="https://github.com/jizhang/java-sandbox/blob/blog-flume/src/main/java/com/shzhangji/javasandbox/flume/ActionTimeInterceptor.java" target="_blank" rel="noopener">链接</a>）中找到，记得需要添加 <code>flume-ng-core</code> 依赖：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ActionTimeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            JsonNode node = mapper.readTree(<span class="keyword">new</span> ByteArrayInputStream(event.getBody()));</span><br><span class="line">            <span class="keyword">long</span> timestamp = (<span class="keyword">long</span>) (node.get(<span class="string">"actionTime"</span>).getDoubleValue() * <span class="number">1000</span>);</span><br><span class="line">            event.getHeaders().put(<span class="string">"timestamp"</span>, Long.toString(timestamp));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// no-op</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="使用-Kafka-Channel-直接导入数据"><a href="#使用-Kafka-Channel-直接导入数据" class="headerlink" title="使用 Kafka Channel 直接导入数据"></a>使用 Kafka Channel 直接导入数据</h2><p>当上游消息系统是 Kafka，并且你能够完全控制消息的数据格式，那就可以省去 Source 一环，直接用 Kafka Channel 将数据导入至 HDFS。其中的关键在于要使用 <code>AvroFlumeEvent</code> 格式来存储消息，这样 <a href="http://flume.apache.org/FlumeUserGuide.html#kafka-channel" target="_blank" rel="noopener">Kafka Channel</a> 才能从消息体中解析出 <code>timestamp</code> 头信息。如果消息内容是纯文本，那下游的 HDFS Sink 就会报时间戳找不到的错误了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构建 AvroFlumeEvent 消息，该类可以在 flume-ng-sdk 依赖中找到</span></span><br><span class="line">Map&lt;CharSequence, CharSequence&gt; headers = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">headers.put(<span class="string">"timestamp"</span>, <span class="string">"1498525680023"</span>);</span><br><span class="line">String body = <span class="string">"some message"</span>;</span><br><span class="line">AvroFlumeEvent event = <span class="keyword">new</span> AvroFlumeEvent(headers, ByteBuffer.wrap(body.getBytes()));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 Avro 编码器对消息进行序列化</span></span><br><span class="line">ByteArrayOutputStream out = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(out, <span class="keyword">null</span>);</span><br><span class="line">SpecificDatumWriter&lt;AvroFlumeEvent&gt; writer = <span class="keyword">new</span> SpecificDatumWriter&lt;&gt;(AvroFlumeEvent.class);</span><br><span class="line">writer.write(event, encoder);</span><br><span class="line">encoder.flush();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送字节码至 Kafka</span></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, <span class="keyword">byte</span>[]&gt;(<span class="string">"alog"</span>, out.toByteArray()));</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></li><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据开发工作中，从上游消息队列抽取数据是一项常规的 ETL 流程。在基于 Hadoop 构建的数据仓库体系中，我们通常会使用 Flume 将事件日志从 Kafka 抽取到 HDFS，然后针对其开发 MapReduce 脚本，或直接创建以时间分区的 Hive 外部表。这项流程中的关键一环是提取日志中的事件时间，因为实时数据通常会包含延迟，且在系统临时宕机的情况下，我们需要追回遗漏的数据，因而使用的时间戳必须是事件产生的时间。Flume 提供的诸多工具能帮助我们非常便捷地实现这一点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/flume.png&quot; alt=&quot;Apache Flume&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;HDFS-Sink-和时间戳头信息&quot;&gt;&lt;a href=&quot;#HDFS-Sink-和时间戳头信息&quot; class=&quot;headerlink&quot; title=&quot;HDFS Sink 和时间戳头信息&quot;&gt;&lt;/a&gt;HDFS Sink 和时间戳头信息&lt;/h2&gt;&lt;p&gt;以下是一个基本的 HDFS Sink 配置：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks = k1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks.k1.type = hdfs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;%Y%m%d&lt;/code&gt; 是该 Sink 支持的时间占位符，它会使用头信息中 &lt;code&gt;timestamp&lt;/code&gt; 的值来替换这些占位符。HDFS Sink 还提供了 &lt;code&gt;hdfs.useLocalTimeStamp&lt;/code&gt; 选项，直接使用当前系统时间来替换时间占位符，但这并不是我们想要达到的目的。&lt;/p&gt;
&lt;p&gt;我们还可以使用 Hive Sink 直接将事件日志导入成 Hive 表，它能直接和 Hive 元数据库通信，自动创建表分区，并支持分隔符分隔和 JSON 两种序列化形式。当然，它同样需要一个 &lt;code&gt;timestamp&lt;/code&gt; 头信息。不过，我们没有选择 Hive Sink，主要出于以下原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它不支持正则表达式，因此我们无法从类似访问日志这样的数据格式中提取字段列表；&lt;/li&gt;
&lt;li&gt;它所提取的字段列表是根据 Hive 表信息产生的。假设上游数据源在 JSON 日志中加入了新的键值，直至我们主动更新 Hive 元信息，这些新增字段将被直接丢弃。对于数据仓库来说，完整保存原始数据是很有必要的。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/cnblogs/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="flume" scheme="http://shzhangji.com/cnblogs/tags/flume/"/>
    
  </entry>
  
</feed>
