<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张吉的博客</title>
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/cnblogs/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/cnblogs/"/>
  <updated>2017-08-13T02:10:57.720Z</updated>
  <id>http://shzhangji.com/cnblogs/</id>
  
  <author>
    <name>张吉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 Binlog 和 Canal 从 MySQL 抽取数据</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/13/extract-data-from-mysql-with-binlog-and-canal/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/13/extract-data-from-mysql-with-binlog-and-canal/</id>
    <published>2017-08-13T02:06:58.000Z</published>
    <updated>2017-08-13T02:10:57.720Z</updated>
    
    <content type="html"><![CDATA[<p>数据抽取是 ETL 流程的第一步。我们会将数据从 RDBMS 或日志服务器等外部系统抽取至数据仓库，进行清洗、转换、聚合等操作。在现代网站技术栈中，MySQL 是最常见的数据库管理系统，我们会从多个不同的 MySQL 实例中抽取数据，存入一个中心节点，或直接进入 Hive。市面上已有多种成熟的、基于 SQL 查询的抽取软件，如著名的开源项目 <a href="http://sqoop.apache.org/" target="_blank" rel="external">Apache Sqoop</a>，然而这些工具并不支持实时的数据抽取。MySQL Binlog 则是一种实时的数据流，用于主从节点之间的数据复制，我们可以利用它来进行数据抽取。借助阿里巴巴开源的 <a href="https://github.com/alibaba/canal" target="_blank" rel="external">Canal</a> 项目，我们能够非常便捷地将 MySQL 中的数据抽取到任意目标存储中。</p>
<p><img src="/cnblogs/images/canal.png" alt="Canal"></p>
<h2 id="Canal-的组成部分"><a href="#Canal-的组成部分" class="headerlink" title="Canal 的组成部分"></a>Canal 的组成部分</h2><p>简单来说，Canal 会将自己伪装成 MySQL 从节点（Slave），并从主节点（Master）获取 Binlog，解析和贮存后供下游消费端使用。Canal 包含两个组成部分：服务端和客户端。服务端负责连接至不同的 MySQL 实例，并为每个实例维护一个事件消息队列；客户端则可以订阅这些队列中的数据变更事件，处理并存储到数据仓库中。下面我们来看如何快速搭建起一个 Canal 服务。</p>
<a id="more"></a>
<h3 id="配置-MySQL-主节点"><a href="#配置-MySQL-主节点" class="headerlink" title="配置 MySQL 主节点"></a>配置 MySQL 主节点</h3><p>MySQL 默认没有开启 Binlog，因此我们需要对 <code>my.cnf</code> 文件做以下修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">server-id = 1</div><div class="line">log_bin = /path/to/mysql-bin.log</div><div class="line">binlog_format = ROW</div></pre></td></tr></table></figure>
<p>注意 <code>binlog_format</code> 必须设置为 <code>ROW</code>, 因为在 <code>STATEMENT</code> 或 <code>MIXED</code> 模式下, Binlog 只会记录和传输 SQL 语句（以减少日志大小），而不包含具体数据，我们也就无法保存了。</p>
<p>从节点通过一个专门的账号连接主节点，这个账号需要拥有全局的 <code>REPLICATION</code> 权限。我们可以使用 <code>GRANT</code> 命令创建这样的账号：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span></div><div class="line"><span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</div></pre></td></tr></table></figure>
<h3 id="启动-Canal-服务端"><a href="#启动-Canal-服务端" class="headerlink" title="启动 Canal 服务端"></a>启动 Canal 服务端</h3><p>从 GitHub 项目发布页中下载 Canal 服务端代码（<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="external">链接</a>），配置文件在 <code>conf</code> 文件夹下，有以下目录结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">canal.deployer/conf/canal.properties</div><div class="line">canal.deployer/conf/instanceA/instance.properties</div><div class="line">canal.deployer/conf/instanceB/instance.properties</div></pre></td></tr></table></figure>
<p><code>conf/canal.properties</code> 是主配置文件，如其中的 <code>canal.port</code> 用以指定服务端监听的端口。<code>instanceA/instance.properties</code> 则是各个实例的配置文件，主要的配置项有：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># slaveId 不能与 my.cnf 中的 server-id 项重复</div><div class="line">canal.instance.mysql.slaveId = 1234</div><div class="line">canal.instance.master.address = 127.0.0.1:3306</div><div class="line">canal.instance.dbUsername = canal</div><div class="line">canal.instance.dbPassword = canal</div><div class="line">canal.instance.connectionCharset = UTF-8</div><div class="line"># 订阅实例中所有的数据库和表</div><div class="line">canal.instance.filter.regex = .*\\..*</div></pre></td></tr></table></figure>
<p>执行 <code>sh bin/startup.sh</code> 命令开启服务端，在日志文件 <code>logs/example/example.log</code> 中可以看到以下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Loading properties file from class path resource [canal.properties]</div><div class="line">Loading properties file from class path resource [example/instance.properties]</div><div class="line">start CannalInstance for 1-example</div><div class="line">[destination = example , address = /127.0.0.1:3306 , EventParser] prepare to find start position just show master status</div></pre></td></tr></table></figure>
<h3 id="编写-Canal-客户端"><a href="#编写-Canal-客户端" class="headerlink" title="编写 Canal 客户端"></a>编写 Canal 客户端</h3><p>从服务端消费变更消息时，我们需要创建一个 Canal 客户端，指定需要订阅的数据库和表，并开启轮询。</p>
<p>首先，在项目中添加 <code>com.alibaba.otter:canal.client</code> 依赖项，构建 <code>CanalConnector</code> 实例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">CanalConnector connector = CanalConnectors.newSingleConnector(</div><div class="line">        <span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">11111</span>), <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</div><div class="line"></div><div class="line">connector.connect();</div><div class="line">connector.subscribe(<span class="string">".*\\..*"</span>);</div><div class="line"></div><div class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">    Message message = connector.getWithoutAck(<span class="number">100</span>);</div><div class="line">    <span class="keyword">long</span> batchId = message.getId();</div><div class="line">    <span class="keyword">if</span> (batchId == -<span class="number">1</span> || message.getEntries().isEmpty()) &#123;</div><div class="line">        Thread.sleep(<span class="number">3000</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        printEntries(message.getEntries());</div><div class="line">        connector.ack(batchId);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这段代码和连接消息系统很相似。变更事件会批量发送过来，待处理完毕后我们可以 ACK 这一批次，从而避免消息丢失。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// printEntries</span></div><div class="line">RowChange rowChange = RowChange.parseFrom(entry.getStoreValue());</div><div class="line"><span class="keyword">for</span> (RowData rowData : rowChange.getRowDatasList()) &#123;</div><div class="line">    <span class="keyword">if</span> (rowChange.getEventType() == EventType.INSERT) &#123;</div><div class="line">      printColumns(rowData.getAfterCollumnList());</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>每一个 <code>Entry</code> 代表一组具有相同变更类型的数据列表，如 INSERT 类型、UPDATE、DELETE 等。每一行数据我们都可以获取到各个字段的信息：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// printColumns</span></div><div class="line">String line = columns.stream()</div><div class="line">        .map(column -&gt; column.getName() + <span class="string">"="</span> + column.getValue())</div><div class="line">        .collect(Collectors.joining(<span class="string">","</span>));</div><div class="line">System.out.println(line);</div></pre></td></tr></table></figure>
<p>完整代码可以在 GitHub 中找到（<a href="https://github.com/jizhang/java-sandbox/blob/blog-canal/src/main/java/com/shzhangji/javasandbox/canal/SimpleClient.java" target="_blank" rel="external">链接</a>）。</p>
<h2 id="加载至数据仓库"><a href="#加载至数据仓库" class="headerlink" title="加载至数据仓库"></a>加载至数据仓库</h2><h3 id="关系型数据库与批量更新"><a href="#关系型数据库与批量更新" class="headerlink" title="关系型数据库与批量更新"></a>关系型数据库与批量更新</h3><p>若数据仓库是基于关系型数据库的，我们可以直接使用 <code>REPLACE</code> 语句将数据变更写入目标表。其中需要注意的是写入性能，在更新较频繁的场景下，我们通常会缓存一段时间的数据，并批量更新至数据库，如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">REPLACE</span> <span class="keyword">INTO</span> <span class="string">`user`</span> (<span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span>) <span class="keyword">VALUES</span></div><div class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">30</span>, <span class="string">'2017-08-12 16:00:00'</span>),</div><div class="line">(<span class="number">2</span>, <span class="string">'Mary'</span>, <span class="number">28</span>, <span class="string">'2017-08-12 17:00:00'</span>),</div><div class="line">(<span class="number">3</span>, <span class="string">'Tom'</span>, <span class="number">36</span>, <span class="string">'2017-08-12 18:00:00'</span>);</div></pre></td></tr></table></figure>
<p>另一种方式是将数据变更写入按分隔符分割的文本文件，并用 <code>LOAD DATA</code> 语句载入数据库。这些文件也可以用在需要写入 Hive 的场景中。不管使用哪一种方法，请一定注意要对字符串类型的字段进行转义，避免导入时出错。</p>
<h3 id="基于-Hive-的数据仓库"><a href="#基于-Hive-的数据仓库" class="headerlink" title="基于 Hive 的数据仓库"></a>基于 Hive 的数据仓库</h3><p>Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase。</p>
<p>数据可以归类成基础数据和增量数据。如昨日的 <code>user</code> 表是基础数据，今日变更的行是增量数据。通过 <code>FULL OUTER JOIN</code>，我们可以将基础和增量数据合并成一张最新的数据表，并作为明天的基础数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  <span class="keyword">COALESCE</span>(b.<span class="string">`id`</span>, a.<span class="string">`id`</span>) <span class="keyword">AS</span> <span class="string">`id`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`name`</span>, a.<span class="string">`name`</span>) <span class="keyword">AS</span> <span class="string">`name`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`age`</span>, a.<span class="string">`age`</span>) <span class="keyword">AS</span> <span class="string">`age`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`updated`</span>, a.<span class="string">`updated`</span>) <span class="keyword">AS</span> <span class="string">`updated`</span></div><div class="line"><span class="keyword">FROM</span> dw_stage.<span class="string">`user`</span> a</div><div class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</div><div class="line">  <span class="comment">-- 增量数据会包含重复数据，因此需要选择最新的那一条</span></div><div class="line">  <span class="keyword">SELECT</span> <span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span></div><div class="line">  <span class="keyword">FROM</span> (</div><div class="line">    <span class="keyword">SELECT</span> *, ROW_NUMBER() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`id`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`updated`</span> <span class="keyword">DESC</span>) <span class="keyword">AS</span> <span class="string">`n`</span></div><div class="line">    <span class="keyword">FROM</span> dw_stage.<span class="string">`user_delta`</span></div><div class="line">  ) b</div><div class="line">  <span class="keyword">WHERE</span> <span class="string">`n`</span> = <span class="number">1</span></div><div class="line">) b</div><div class="line"><span class="keyword">ON</span> a.<span class="string">`id`</span> = b.<span class="string">`id`</span>;</div></pre></td></tr></table></figure>
<p>Hive 0.13 引入了事务和 ACID 表，0.14 开始支持 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 语句，Hive 2.0.0 则又新增了 <a href="https://cwiki.apache.org/confluence/display/Hive/HCatalog+Streaming+Mutation+API" target="_blank" rel="external">Streaming Mutation API</a>，用以通过编程的方式批量更新 Hive 表中的记录。目前，ACID 表必须使用 ORC 文件格式进行存储，且须按主键进行分桶（Bucket）。Hive 会将变更记录保存在增量文件中，当 <code>OrcInputFormat</code> 读取数据时会自动定位到最新的那条记录。官方案例可以在这个<a href="https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java" target="_blank" rel="external">链接</a>中查看。</p>
<p>最后，我们可以使用 HBase 来实现表数据的更新，它是一种 KV 存储系统，同样基于 HDFS。HBase 的数据可以直接为 MapReduce 脚本使用，且 Hive 中可以创建外部映射表指向 HBase。更多信息请查看<a href="http://hbase.apache.org/" target="_blank" rel="external">官方网站</a>。</p>
<h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>数据抽取通常是按需进行的，在新增一张表时，数据源中可能已经有大量原始记录了。常见的做法是手工将这批数据全量导入至目标表中，但我们也可以复用 Canal 这套机制来实现历史数据的抽取。</p>
<p>首先，我们在数据源库中创建一张辅助表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`retl_buffer`</span> (</div><div class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span></div><div class="line">  ,table_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</div><div class="line">  ,pk_value <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</div><div class="line">);</div></pre></td></tr></table></figure>
<p>当需要全量抽取 <code>user</code> 表时，我们执行以下语句，将所有 <code>user.id</code> 写入辅助表中：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`retl_buffer`</span> (<span class="string">`table_name`</span>, <span class="string">`pk_value`</span>)</div><div class="line"><span class="keyword">SELECT</span> <span class="string">'user'</span>, <span class="string">`id`</span> <span class="keyword">FROM</span> <span class="string">`user`</span>;</div></pre></td></tr></table></figure>
<p>Canal 客户端在处理到 <code>retl_buffer</code> 表的数据变更时，可以从中解析出表名和主键的值，直接反查数据源，将数据写入目标表：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (<span class="string">"retl_buffer"</span>.equals(entry.getHeader().getTableName())) &#123;</div><div class="line">    String tableName = rowData.getAfterColumns(<span class="number">1</span>).getValue();</div><div class="line">    String pkValue = rowData.getAfterColumns(<span class="number">2</span>).getValue();</div><div class="line">    System.out.println(<span class="string">"SELECT * FROM "</span> + tableName + <span class="string">" WHERE id = "</span> + pkValue);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这一方法在阿里巴巴的另一个开源软件 <a href="https://github.com/alibaba/otter/wiki/Manager%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%87%AA-%E7%94%B1-%E9%97%A8" target="_blank" rel="external">Otter</a> 中使用。</p>
<h2 id="Canal-高可用"><a href="#Canal-高可用" class="headerlink" title="Canal 高可用"></a>Canal 高可用</h2><ul>
<li>Canal 服务端中的实例可以配置一个备用 MySQL，从而能够在双 Master 场景下自动选择正在工作的数据源。注意两台主库都需要打开 <code>log_slave_updates</code> 选项。Canal 会使用自己的心跳机制（定期更新辅助表的记录）来检测主库的存活。</li>
<li>Canal 自身也有 HA 配置，配合 Zookeeper，我们可以开启多个 Canal 服务端，当某台服务器宕机时，客户端可以从 ZK 中获取新的服务端地址，继续进行消费。更多信息可以参考 <a href="https://github.com/alibaba/canal/wiki/AdminGuide" target="_blank" rel="external">Canal AdminGuide</a>。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://github.com/alibaba/canal/wiki" target="_blank" rel="external">https://github.com/alibaba/canal/wiki</a></li>
<li><a href="https://github.com/alibaba/otter/wiki" target="_blank" rel="external">https://github.com/alibaba/otter/wiki</a></li>
<li><a href="https://www.phdata.io/4-strategies-for-updating-hive-tables/" target="_blank" rel="external">https://www.phdata.io/4-strategies-for-updating-hive-tables/</a></li>
<li><a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/" target="_blank" rel="external">https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据抽取是 ETL 流程的第一步。我们会将数据从 RDBMS 或日志服务器等外部系统抽取至数据仓库，进行清洗、转换、聚合等操作。在现代网站技术栈中，MySQL 是最常见的数据库管理系统，我们会从多个不同的 MySQL 实例中抽取数据，存入一个中心节点，或直接进入 Hive。市面上已有多种成熟的、基于 SQL 查询的抽取软件，如著名的开源项目 &lt;a href=&quot;http://sqoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apache Sqoop&lt;/a&gt;，然而这些工具并不支持实时的数据抽取。MySQL Binlog 则是一种实时的数据流，用于主从节点之间的数据复制，我们可以利用它来进行数据抽取。借助阿里巴巴开源的 &lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Canal&lt;/a&gt; 项目，我们能够非常便捷地将 MySQL 中的数据抽取到任意目标存储中。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/canal.png&quot; alt=&quot;Canal&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Canal-的组成部分&quot;&gt;&lt;a href=&quot;#Canal-的组成部分&quot; class=&quot;headerlink&quot; title=&quot;Canal 的组成部分&quot;&gt;&lt;/a&gt;Canal 的组成部分&lt;/h2&gt;&lt;p&gt;简单来说，Canal 会将自己伪装成 MySQL 从节点（Slave），并从主节点（Master）获取 Binlog，解析和贮存后供下游消费端使用。Canal 包含两个组成部分：服务端和客户端。服务端负责连接至不同的 MySQL 实例，并为每个实例维护一个事件消息队列；客户端则可以订阅这些队列中的数据变更事件，处理并存储到数据仓库中。下面我们来看如何快速搭建起一个 Canal 服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/cnblogs/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
      <category term="mysql" scheme="http://shzhangji.com/cnblogs/tags/mysql/"/>
    
      <category term="canal" scheme="http://shzhangji.com/cnblogs/tags/canal/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flume 如何解析消息中的事件时间</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/06/how-to-extract-event-time-in-apache-flume/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/06/how-to-extract-event-time-in-apache-flume/</id>
    <published>2017-08-06T01:09:06.000Z</published>
    <updated>2017-08-06T01:09:06.810Z</updated>
    
    <content type="html"><![CDATA[<p>数据开发工作中，从上游消息队列抽取数据是一项常规的 ETL 流程。在基于 Hadoop 构建的数据仓库体系中，我们通常会使用 Flume 将事件日志从 Kafka 抽取到 HDFS，然后针对其开发 MapReduce 脚本，或直接创建以时间分区的 Hive 外部表。这项流程中的关键一环是提取日志中的事件时间，因为实时数据通常会包含延迟，且在系统临时宕机的情况下，我们需要追回遗漏的数据，因而使用的时间戳必须是事件产生的时间。Flume 提供的诸多工具能帮助我们非常便捷地实现这一点。</p>
<p><img src="/cnblogs/images/flume.png" alt="Apache Flume"></p>
<h2 id="HDFS-Sink-和时间戳头信息"><a href="#HDFS-Sink-和时间戳头信息" class="headerlink" title="HDFS Sink 和时间戳头信息"></a>HDFS Sink 和时间戳头信息</h2><p>以下是一个基本的 HDFS Sink 配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a1.sinks = k1</div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d</div></pre></td></tr></table></figure>
<p><code>%Y%m%d</code> 是该 Sink 支持的时间占位符，它会使用头信息中 <code>timestamp</code> 的值来替换这些占位符。HDFS Sink 还提供了 <code>hdfs.useLocalTimeStamp</code> 选项，直接使用当前系统时间来替换时间占位符，但这并不是我们想要达到的目的。</p>
<p>我们还可以使用 Hive Sink 直接将事件日志导入成 Hive 表，它能直接和 Hive 元数据库通信，自动创建表分区，并支持分隔符分隔和 JSON 两种序列化形式。当然，它同样需要一个 <code>timestamp</code> 头信息。不过，我们没有选择 Hive Sink，主要出于以下原因：</p>
<ul>
<li>它不支持正则表达式，因此我们无法从类似访问日志这样的数据格式中提取字段列表；</li>
<li>它所提取的字段列表是根据 Hive 表信息产生的。假设上游数据源在 JSON 日志中加入了新的键值，直至我们主动更新 Hive 元信息，这些新增字段将被直接丢弃。对于数据仓库来说，完整保存原始数据是很有必要的。</li>
</ul>
<a id="more"></a>
<h2 id="正则表达式拦截器"><a href="#正则表达式拦截器" class="headerlink" title="正则表达式拦截器"></a>正则表达式拦截器</h2><p>Flume 提供了拦截器机制，我们可以在 Source 之后接上一系列操作，对数据进行基础的转换。例如，<code>TimestampInterceptor</code> 拦截器可以在消息头信息中增加当前时间。在本节中，我将演示如何借助拦截器来提取访问日志型和 JSON 型消息中的事件时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">0.123 [2017-06-27 09:08:00] GET /</div><div class="line">0.234 [2017-06-27 09:08:01] GET /</div></pre></td></tr></table></figure>
<p><a href="http://flume.apache.org/FlumeUserGuide.html#regex-extractor-interceptor" target="_blank" rel="external"><code>RegexExtractorInterceptor</code></a> 可以基于正则表达式来提取字符串，配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">a1.sources.r1.interceptors = i1</div><div class="line">a1.sources.r1.interceptors.i1.type = regex_extractor</div><div class="line">a1.sources.r1.interceptors.i1.regex = \\[(.*?)\\]</div><div class="line">a1.sources.r1.interceptors.i1.serializers = s1</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.name = timestamp</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.pattern = yyyy-MM-dd HH:mm:ss</div></pre></td></tr></table></figure>
<p>它会搜索字符串中满足 <code>\[(.*?)\]</code> 模式的子串，将第一个子模式即 <code>s1</code> 作为日期字符串进行解析，并将其转化成毫秒级时间戳，存入头信息 <code>timestamp</code>。</p>
<h3 id="搜索与替换拦截器"><a href="#搜索与替换拦截器" class="headerlink" title="搜索与替换拦截器"></a>搜索与替换拦截器</h3><p>对于 JSON 数据:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525680.023</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</div><div class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525681.349</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</div></pre></td></tr></table></figure>
<p>我们同样可以用正则拦截器将 <code>actionTime</code> 提取出来，但要注意的是该字段的单位是秒，而 HDFS Sink 要求的是毫秒，这就需要我们在提取之间对其进行转换，如直接将 <code>.</code> 去掉。<code>SearchAndReplaceInterceptor</code> 可以做到这一点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a1.sources.r1.interceptors = i1 i2</div><div class="line">a1.sources.r1.interceptors.i1.type = search_replace</div><div class="line">a1.sources.r1.interceptors.i1.searchPattern = \&quot;actionTime\&quot;:(\\d+)\\.(\\d+)</div><div class="line">a1.sources.r1.interceptors.i1.replaceString = \&quot;actionTime\&quot;:$1$2</div><div class="line">a1.sources.r1.interceptors.i2.type = regex_extractor</div><div class="line">a1.sources.r1.interceptors.i2.regex = \&quot;actionTime\&quot;:(\\d+)</div><div class="line">a1.sources.r1.interceptors.i2.serializers = s1</div><div class="line">a1.sources.r1.interceptors.i2.serializers.s1.name = timestamp</div></pre></td></tr></table></figure>
<p>这里我们串联了两个拦截器。首先使用正则替换将 <code>1498525680.023</code> 转换成 <code>1498525680023</code>，再用正则提取出 <code>actionTime</code> 并存入头信息。</p>
<h3 id="自定义拦截器"><a href="#自定义拦截器" class="headerlink" title="自定义拦截器"></a>自定义拦截器</h3><p>我们还可以编写自定义的拦截器，从而一次性完成提取、转换和更新操作。我们只需实现 <code>org.apache.flume.interceptor.Interceptor</code> 接口的 <code>intercept</code> 方法即可，源代码可以在 GitHub（<a href="https://github.com/jizhang/java-sandbox/blob/blog-flume/src/main/java/com/shzhangji/javasandbox/flume/ActionTimeInterceptor.java" target="_blank" rel="external">链接</a>）中找到，记得需要添加 <code>flume-ng-core</code> 依赖：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ActionTimeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            JsonNode node = mapper.readTree(<span class="keyword">new</span> ByteArrayInputStream(event.getBody()));</div><div class="line">            <span class="keyword">long</span> timestamp = (<span class="keyword">long</span>) (node.get(<span class="string">"actionTime"</span>).getDoubleValue() * <span class="number">1000</span>);</div><div class="line">            event.getHeaders().put(<span class="string">"timestamp"</span>, Long.toString(timestamp));</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            <span class="comment">// no-op</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> event;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="使用-Kafka-Channel-直接导入数据"><a href="#使用-Kafka-Channel-直接导入数据" class="headerlink" title="使用 Kafka Channel 直接导入数据"></a>使用 Kafka Channel 直接导入数据</h2><p>当上游消息系统是 Kafka，并且你能够完全控制消息的数据格式，那就可以省去 Source 一环，直接用 Kafka Channel 将数据导入至 HDFS。其中的关键在于要使用 <code>AvroFlumeEvent</code> 格式来存储消息，这样 <a href="http://flume.apache.org/FlumeUserGuide.html#kafka-channel" target="_blank" rel="external">Kafka Channel</a> 才能从消息体中解析出 <code>timestamp</code> 头信息。如果消息内容是纯文本，那下游的 HDFS Sink 就会报时间戳找不到的错误了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 构建 AvroFlumeEvent 消息，该类可以在 flume-ng-sdk 依赖中找到</span></div><div class="line">Map&lt;CharSequence, CharSequence&gt; headers = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">headers.put(<span class="string">"timestamp"</span>, <span class="string">"1498525680023"</span>);</div><div class="line">String body = <span class="string">"some message"</span>;</div><div class="line">AvroFlumeEvent event = <span class="keyword">new</span> AvroFlumeEvent(headers, ByteBuffer.wrap(body.getBytes()));</div><div class="line"></div><div class="line"><span class="comment">// 使用 Avro 编码器对消息进行序列化</span></div><div class="line">ByteArrayOutputStream out = <span class="keyword">new</span> ByteArrayOutputStream();</div><div class="line">BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(out, <span class="keyword">null</span>);</div><div class="line">SpecificDatumWriter&lt;AvroFlumeEvent&gt; writer = <span class="keyword">new</span> SpecificDatumWriter&lt;&gt;(AvroFlumeEvent.class);</div><div class="line">writer.write(event, encoder);</div><div class="line">encoder.flush();</div><div class="line"></div><div class="line"><span class="comment">// 发送字节码至 Kafka</span></div><div class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, <span class="keyword">byte</span>[]&gt;(<span class="string">"alog"</span>, out.toByteArray()));</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">http://flume.apache.org/FlumeUserGuide.html</a></li>
<li><a href="https://github.com/apache/flume" target="_blank" rel="external">https://github.com/apache/flume</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据开发工作中，从上游消息队列抽取数据是一项常规的 ETL 流程。在基于 Hadoop 构建的数据仓库体系中，我们通常会使用 Flume 将事件日志从 Kafka 抽取到 HDFS，然后针对其开发 MapReduce 脚本，或直接创建以时间分区的 Hive 外部表。这项流程中的关键一环是提取日志中的事件时间，因为实时数据通常会包含延迟，且在系统临时宕机的情况下，我们需要追回遗漏的数据，因而使用的时间戳必须是事件产生的时间。Flume 提供的诸多工具能帮助我们非常便捷地实现这一点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/flume.png&quot; alt=&quot;Apache Flume&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;HDFS-Sink-和时间戳头信息&quot;&gt;&lt;a href=&quot;#HDFS-Sink-和时间戳头信息&quot; class=&quot;headerlink&quot; title=&quot;HDFS Sink 和时间戳头信息&quot;&gt;&lt;/a&gt;HDFS Sink 和时间戳头信息&lt;/h2&gt;&lt;p&gt;以下是一个基本的 HDFS Sink 配置：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks = k1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks.k1.type = hdfs&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;%Y%m%d&lt;/code&gt; 是该 Sink 支持的时间占位符，它会使用头信息中 &lt;code&gt;timestamp&lt;/code&gt; 的值来替换这些占位符。HDFS Sink 还提供了 &lt;code&gt;hdfs.useLocalTimeStamp&lt;/code&gt; 选项，直接使用当前系统时间来替换时间占位符，但这并不是我们想要达到的目的。&lt;/p&gt;
&lt;p&gt;我们还可以使用 Hive Sink 直接将事件日志导入成 Hive 表，它能直接和 Hive 元数据库通信，自动创建表分区，并支持分隔符分隔和 JSON 两种序列化形式。当然，它同样需要一个 &lt;code&gt;timestamp&lt;/code&gt; 头信息。不过，我们没有选择 Hive Sink，主要出于以下原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它不支持正则表达式，因此我们无法从类似访问日志这样的数据格式中提取字段列表；&lt;/li&gt;
&lt;li&gt;它所提取的字段列表是根据 Hive 表信息产生的。假设上游数据源在 JSON 日志中加入了新的键值，直至我们主动更新 Hive 元信息，这些新增字段将被直接丢弃。对于数据仓库来说，完整保存原始数据是很有必要的。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="flume" scheme="http://shzhangji.com/cnblogs/tags/flume/"/>
    
      <category term="etl" scheme="http://shzhangji.com/cnblogs/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/cnblogs/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 中如何实现 Exactly-Once 语义</title>
    <link href="http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/"/>
    <id>http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/</id>
    <published>2017-08-01T04:54:47.000Z</published>
    <updated>2017-08-05T00:47:06.615Z</updated>
    
    <content type="html"><![CDATA[<p>Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实现 Exactly-once 的实时计算流程。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Spark Streaming"></p>
<h2 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h2><p>首先让我们实现一个简单而完整的实时计算流程。我们从 Kafka 接收用户访问日志，解析并提取其中的时间和日志级别，并统计每分钟错误日志的数量，结果保存到 MySQL 中。</p>
<p>示例日志:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2017-07-30 14:09:08 ERROR some message</div><div class="line">2017-07-30 14:09:20 INFO  some message</div><div class="line">2017-07-30 14:10:50 ERROR some message</div></pre></td></tr></table></figure>
<p>结果表结构，其中 <code>log_time</code> 字段会截取到分钟级别：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_log (</div><div class="line">  log_time datetime primary <span class="keyword">key</span>,</div><div class="line">  log_count <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="number">0</span></div><div class="line">);</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>Scala 项目通常使用 <code>sbt</code> 来管理。我们将下列依赖添加到 <code>build.sbt</code> 文件中。本例使用的是 Spark 2.2 和 Kafka 0.10，数据库操作类库使用了 ScalikeJDBC 3.0。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scalaVersion := <span class="string">"2.11.11"</span></div><div class="line"></div><div class="line">libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming"</span> % <span class="string">"2.2.0"</span> % <span class="string">"provided"</span>,</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming-kafka-0-10"</span> % <span class="string">"2.2.0"</span>,</div><div class="line">  <span class="string">"org.scalikejdbc"</span> %% <span class="string">"scalikejdbc"</span> % <span class="string">"3.0.1"</span>,</div><div class="line">  <span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.43"</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>完整的示例代码已上传至 GitHub（<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/ExactlyOnce.scala" target="_blank" rel="external">链接</a>），下面我仅选取重要的部分加以说明：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 初始化数据库连接</span></div><div class="line"><span class="type">ConnectionPool</span>.singleton(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"root"</span>, <span class="string">""</span>)</div><div class="line"></div><div class="line"><span class="comment">// 创建 Spark Streaming 上下文</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ExactlyOnce"</span>).setIfMissing(<span class="string">"spark.master"</span>, <span class="string">"local[2]"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</div><div class="line"></div><div class="line"><span class="comment">// 使用 Kafka Direct API 创建 DStream</span></div><div class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</div><div class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</div><div class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Seq</span>(<span class="string">"alog"</span>), kafkaParams))</div><div class="line"></div><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="comment">// 日志处理</span></div><div class="line">  <span class="keyword">val</span> result = rdd.map(_.value)</div><div class="line">    .flatMap(parseLog) <span class="comment">// 日志解析函数</span></div><div class="line">    .filter(_.level == <span class="string">"ERROR"</span>)</div><div class="line">    .map(log =&gt; log.time.truncatedTo(<span class="type">ChronoUnit</span>.<span class="type">MINUTES</span>) -&gt; <span class="number">1</span>)</div><div class="line">    .reduceByKey(_ + _)</div><div class="line">    .collect()</div><div class="line"></div><div class="line">  <span class="comment">// 结果保存至数据库</span></div><div class="line">  <span class="type">DB</span>.autoCommit &#123; <span class="keyword">implicit</span> session =&gt;</div><div class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</div><div class="line">      <span class="string">sql""</span><span class="string">"</span></div><div class="line"><span class="string">      insert into error_log (log_time, log_count)</span></div><div class="line"><span class="string">      value ($&#123;time&#125;, $&#123;count&#125;)</span></div><div class="line"><span class="string">      on duplicate key update log_count = log_count + values(log_count)</span></div><div class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="实时计算语义"><a href="#实时计算语义" class="headerlink" title="实时计算语义"></a>实时计算语义</h2><p>实时计算有三种语义，分别是 At-most-once、At-least-once、以及 Exactly-once。一个典型的 Spark Streaming 应用程序会包含三个处理阶段：接收数据、处理汇总、输出结果。每个阶段都需要做不同的处理才能实现相应的语义。</p>
<p>对于 <strong>接收数据</strong>，主要取决于上游数据源的特性。例如，从 HDFS 这类支持容错的文件系统中读取文件，能够直接支持 Exactly-once 语义。如果上游消息系统支持 ACK（如RabbitMQ），我们就可以结合 Spark 的 Write Ahead Log 特性来实现 At-least-once 语义。对于非可靠的数据接收器（如 <code>socketTextStream</code>），当 Worker 或 Driver 节点发生故障时就会产生数据丢失，提供的语义也是未知的。而 Kafka 消息系统是基于偏移量（Offset）的，它的 Direct API 可以提供 Exactly-once 语义。</p>
<p>在使用 Spark RDD 对数据进行 <strong>转换或汇总</strong> 时，我们可以天然获得 Exactly-once 语义，因为 RDD 本身就是一种具备容错性、不变性、以及计算确定性的数据结构。只要数据来源是可用的，且处理过程中没有副作用（Side effect），我们就能一直得到相同的计算结果。</p>
<p><strong>结果输出</strong> 默认符合 At-least-once 语义，因为 <code>foreachRDD</code> 方法可能会因为 Worker 节点失效而执行多次，从而重复写入外部存储。我们有两种方式解决这一问题，幂等更新和事务更新。下面我们将深入探讨这两种方式。</p>
<h2 id="使用幂等写入实现-Exactly-once"><a href="#使用幂等写入实现-Exactly-once" class="headerlink" title="使用幂等写入实现 Exactly-once"></a>使用幂等写入实现 Exactly-once</h2><p>如果多次写入会产生相同的结果数据，我们可以认为这类写入操作是幂等的。<code>saveAsTextFile</code> 就是一种典型的幂等写入。如果消息中包含唯一主键，那么多次写入相同的数据也不会在数据库中产生重复记录。这种方式也就能等价于 Exactly-once 语义了。但需要注意的是，幂等写入只适用于 Map-only 型的计算流程，即没有 Shuffle、Reduce、Repartition 等操作。此外，我们还需对 Kafka DStream 做一些额外设置：</p>
<ul>
<li>将 <code>enable.auto.commit</code> 设置为 <code>false</code>。默认情况下，Kafka DStream 会在接收到数据后立刻更新自己的偏移量，我们需要将这个动作推迟到计算完成之后。</li>
<li>打开 Spark Streaming 的 Checkpoint 特性，用于存放 Kafka 偏移量。但若应用程序代码发生变化，Checkpoint 数据也将无法使用，这就需要改用下面的操作：</li>
<li>在数据输出之后手动提交 Kafka 偏移量。<code>HasOffsetRanges</code> 类，以及 <code>commitAsync</code> API 可以做到这一点：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd.foreachPartition &#123; iter =&gt;</div><div class="line">    <span class="comment">// output to database</span></div><div class="line">  &#125;</div><div class="line">  messages.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="使用事务写入实现-Exactly-once"><a href="#使用事务写入实现-Exactly-once" class="headerlink" title="使用事务写入实现 Exactly-once"></a>使用事务写入实现 Exactly-once</h2><p>在使用事务型写入时，我们需要生成一个唯一 ID，这个 ID 可以使用当前批次的时间、分区号、或是 Kafka 偏移量来生成。之后，我们需要在一个事务中将处理结果和这个唯一 ID 一同写入数据库。这一原子性的操作将带给我们 Exactly-once 语义，而且该方法可以同时适用于 Map-only 以及包含汇聚操作的计算流程。</p>
<p>我们通常会在 <code>foreachPartition</code> 方法中来执行数据库写入操作。对于 Map-only 流程来说是适用的，因为这种流程下 Kafka 分区和 RDD 分区是一一对应的，我们可以用以下方式获取各分区的偏移量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd.foreachPartition &#123; iter =&gt;</div><div class="line">    <span class="keyword">val</span> offsetRange = offsetRanges(<span class="type">TaskContext</span>.get.partitionId)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>但对于包含 Shuffle 的计算流程（如上文的错误日志统计），我们需要先将处理结果拉取到 Driver 进程中，然后才能执行事务操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  <span class="keyword">val</span> result = processLogs(rdd).collect() <span class="comment">// parse log and count error</span></div><div class="line">  <span class="type">DB</span>.localTx &#123; <span class="keyword">implicit</span> session =&gt;</div><div class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</div><div class="line">      <span class="comment">// save to error_log table</span></div><div class="line">    &#125;</div><div class="line">    offsetRanges.foreach &#123; offsetRange =&gt;</div><div class="line">      <span class="keyword">val</span> affectedRows = <span class="string">sql""</span><span class="string">"</span></div><div class="line"><span class="string">      update kafka_offset set offset = $&#123;offsetRange.untilOffset&#125;</span></div><div class="line"><span class="string">      where topic = $&#123;topic&#125; and `partition` = $&#123;offsetRange.partition&#125;</span></div><div class="line"><span class="string">      and offset = $&#123;offsetRange.fromOffset&#125;</span></div><div class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (affectedRows != <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"fail to update offset"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果偏移量写入失败，或者重复处理了某一部分数据（<code>offset != $fromOffset</code> 判断条件不通过），该事务就会回滚，从而做到 Exactly-once。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实时计算中的 Exactly-once 是比较强的一种语义，因而会给你的应用程序引入额外的开销。此外，它尚不能很好地支持<a href="https://github.com/koeninger/kafka-exactly-once/blob/master/src/main/scala/example/Windowed.scala" target="_blank" rel="external">窗口型</a>操作。因此，是否要在代码中使用这一语义就需要开发者自行判断了。很多情况下，数据丢失或重复处理并不那么重要。不过，了解 Exactly-once 的开发流程还是有必要的，对学习 Spark Streaming 也会有所助益。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</a></li>
<li><a href="http://kafka.apache.org/documentation.html#semantics" target="_blank" rel="external">http://kafka.apache.org/documentation.html#semantics</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实现 Exactly-once 的实时计算流程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://spark.apache.org/docs/latest/img/streaming-arch.png&quot; alt=&quot;Spark Streaming&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;引例&quot;&gt;&lt;a href=&quot;#引例&quot; class=&quot;headerlink&quot; title=&quot;引例&quot;&gt;&lt;/a&gt;引例&lt;/h2&gt;&lt;p&gt;首先让我们实现一个简单而完整的实时计算流程。我们从 Kafka 接收用户访问日志，解析并提取其中的时间和日志级别，并统计每分钟错误日志的数量，结果保存到 MySQL 中。&lt;/p&gt;
&lt;p&gt;示例日志:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:09:08 ERROR some message&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:09:20 INFO  some message&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:10:50 ERROR some message&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;结果表结构，其中 &lt;code&gt;log_time&lt;/code&gt; 字段会截取到分钟级别：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;table&lt;/span&gt; error_log (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  log_time datetime primary &lt;span class=&quot;keyword&quot;&gt;key&lt;/span&gt;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  log_count &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;);&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/cnblogs/tags/spark/"/>
    
      <category term="scala" scheme="http://shzhangji.com/cnblogs/tags/scala/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/cnblogs/tags/stream-processing/"/>
    
      <category term="spark streaming" scheme="http://shzhangji.com/cnblogs/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://shzhangji.com/cnblogs/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>通过 SQL 查询学习 Pandas 数据处理</title>
    <link href="http://shzhangji.com/cnblogs/2017/07/23/learn-pandas-from-a-sql-perspective/"/>
    <id>http://shzhangji.com/cnblogs/2017/07/23/learn-pandas-from-a-sql-perspective/</id>
    <published>2017-07-23T12:57:00.000Z</published>
    <updated>2017-08-05T00:47:06.613Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://pandas.pydata.org/" target="_blank" rel="external">Pandas</a> 是一款广泛使用的数据处理工具。结合 NumPy 和 Matplotlib 类库，我们可以在内存中进行高性能的数据清洗、转换、分析及可视化工作。虽然 Python 本身是一门非常容易学习的语言，但要熟练掌握 Pandas 丰富的 API 接口及正确的使用方式，还是需要投入一定时间的。对于数据开发工程师或分析师而言，SQL 语言是标准的数据查询工具。本文提供了一系列的示例，如何将常见的 SQL 查询语句使用 Pandas 来实现。</p>
<p>Pandas 的安装和基本概念并不在本文讲述范围内，请读者到官网上阅读相关文档，或者阅读《<a href="https://book.douban.com/subject/25779298/" target="_blank" rel="external">利用 Python 进行数据分析</a>》一书。我推荐大家使用 <a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a> Python 套件，其中集成了 <a href="https://pythonhosted.org/spyder/" target="_blank" rel="external">Spyder</a> 集成开发环境。在运行下文的代码之前，请先引入 Pandas 和 NumPy 包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<h2 id="FROM-读取数据"><a href="#FROM-读取数据" class="headerlink" title="FROM - 读取数据"></a><code>FROM</code> - 读取数据</h2><p>首先，我们需要将数据加载到工作区（内存）。Pandas 原生支持非常多的数据格式，CSV 是较常见的一种。我们以航班延误时间数据集为例（<a href="/uploads/flights.csv">下载地址</a>）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">date,delay,distance,origin,destination</div><div class="line">02221605,3,358,BUR,SMF</div><div class="line">01022100,-5,239,HOU,DAL</div><div class="line">03210808,6,288,BWI,ALB</div></pre></td></tr></table></figure>
<p>我们可以使用 <code>pd.read_csv</code> 函数加载它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'flights.csv'</span>, dtype=&#123;<span class="string">'date'</span>: str&#125;)</div><div class="line">df.head()</div></pre></td></tr></table></figure>
<p>这条命令会将 <code>flights.csv</code> 文件读入内存，使用首行作为列名，并自动检测每一列的数据类型。其中，由于 <code>date</code> 一列的日期格式是 <code>%m%d%H%M</code>，自动转换成数字后会失去月份的前异零（02 月的 0），因此我们显式指定了该列的 <code>dtype</code>，告知 Pandas 保留原值。</p>
<a id="more"></a>
<p><code>df.head</code> 用于查看数据集的前 N 行，功能类似于 <code>LIMIT N</code>。如果要实现 <code>LIMIT 10, 100</code>，可以使用 <code>df.iloc[10:100]</code>。此外，IPython 终端默认只显示 60 行数据，我们可以通过以下方法修改设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pd.options.display.max_rows = <span class="number">100</span></div><div class="line">df.iloc[<span class="number">10</span>:<span class="number">100</span>]</div></pre></td></tr></table></figure>
<p>另外一种常见的数据源是关系型数据库，Pandas 也提供了内置支持：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">conn = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>)</div><div class="line">df = pd.read_sql(<span class="string">"""</span></div><div class="line"><span class="string">select `date`, `delay`, `distance`, `origin`, `destination`</span></div><div class="line"><span class="string">from flights limit 1000</span></div><div class="line"><span class="string">"""</span>, conn)</div></pre></td></tr></table></figure>
<p>如果要将 DataFrame 保存到文件或数据库中去，可以分别使用 <code>pd.to_csv</code> 和 <code>pd.to_sql</code> 函数。</p>
<h2 id="SELECT-选择列"><a href="#SELECT-选择列" class="headerlink" title="SELECT - 选择列"></a><code>SELECT</code> - 选择列</h2><p><code>SELECT</code> 语句在 SQL 中用于选择需要的列，并对数据做清洗和转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df[<span class="string">'date'</span>] <span class="comment"># SELECT `date`</span></div><div class="line">df[[<span class="string">'date'</span>, <span class="string">'delay'</span>]] <span class="comment"># SELECT `date`, `delay`</span></div><div class="line">df.loc[<span class="number">10</span>:<span class="number">100</span>, [<span class="string">'date'</span>, <span class="string">'delay'</span>]] <span class="comment"># SELECT `date, `delay` LIMIT 10, 100</span></div></pre></td></tr></table></figure>
<p>SQL 提供了诸多函数，大部分都可以用 Pandas 来实现，而且我们也很容易用 Python 编写自定义函数。下面我将列举一些常用的函数。</p>
<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p>Pandas 的字符串函数可以通过 DateFrame 和 Series 的 <code>str</code> 属性来调用，如 <code>df[&#39;origin&#39;].str.lower()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT CONCAT(origin, ' to ', destination)</span></div><div class="line">df[<span class="string">'origin'</span>].str.cat(df[<span class="string">'destination'</span>], sep=<span class="string">' to '</span>)</div><div class="line"></div><div class="line">df[<span class="string">'origin'</span>].str.strip() <span class="comment"># TRIM(origin)</span></div><div class="line">df[<span class="string">'origin'</span>].str.len() <span class="comment"># LENGTH(origin)</span></div><div class="line">df[<span class="string">'origin'</span>].str.replace(<span class="string">'a'</span>, <span class="string">'b'</span>) <span class="comment"># REPLACE(origin, 'a', 'b')</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT SUBSTRING(origin, 1, 1)</span></div><div class="line">df[<span class="string">'origin'</span>].str[<span class="number">0</span>:<span class="number">1</span>] <span class="comment"># 使用 Python 字符串索引</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT SUBSTRING_INDEX(domain, '.', 2)</span></div><div class="line"><span class="comment"># www.example.com -&gt; www.example</span></div><div class="line">df[<span class="string">'domain'</span>].str.split(<span class="string">'.'</span>).str[:<span class="number">2</span>].str.join(<span class="string">'.'</span>)</div><div class="line">df[<span class="string">'domain'</span>].str.extract(<span class="string">r'^([^.]+\.[^.]+)'</span>)</div></pre></td></tr></table></figure>
<p>Pandas 有一个名为广播的特性（broadcast），简单来说就是能够将低维数据（包括单个标量）和高维数据进行结合和处理。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df[<span class="string">'full_date'</span>] = <span class="string">'2001'</span> + df[<span class="string">'date'</span>] <span class="comment"># CONCAT('2001', `date`)</span></div><div class="line">df[<span class="string">'delay'</span>] / <span class="number">60</span></div><div class="line">df[<span class="string">'delay'</span>].div(<span class="number">60</span>) <span class="comment"># 同上</span></div></pre></td></tr></table></figure>
<p>Pandas 还内置了很多字符串函数，它们的用法和 SQL 有一定区别，但功能更强。完整列表可以参考文档 <a href="https://pandas.pydata.org/pandas-docs/stable/text.html" target="_blank" rel="external">Working with Text Data</a>。</p>
<h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><p><code>pd.to_datetime</code> 用于将各种日期字符串转换成标准的 <code>datetime64</code> 类型。日期类型的 Series 都会有一个 <code>dt</code> 属性，从中可以获取到有关日期时间的信息，具体请参考文档 <a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html" target="_blank" rel="external">Time Series / Date functionality</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT STR_TO_DATE(full_date, '%Y%m%d%H%i%s') AS `datetime`</span></div><div class="line">df[<span class="string">'datetime'</span>] = pd.to_datetime(df[<span class="string">'full_date'</span>], format=<span class="string">'%Y%m%d%H%M%S'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SELECT DATE_FORMAT(`datetime`, '%Y-%m-%d')</span></div><div class="line">df[<span class="string">'datetime'</span>].dt.strftime(<span class="string">'%Y-%m-%d'</span>)</div><div class="line"></div><div class="line">df[<span class="string">'datetime'</span>].dt.month <span class="comment"># MONTH(`datetime`)</span></div><div class="line">df[<span class="string">'datetime'</span>].dt.hour <span class="comment"># HOUR(`datetime`)</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT UNIX_TIMESTAMP(`datetime`)</span></div><div class="line">df[<span class="string">'datetime'</span>].view(<span class="string">'int64'</span>) // pd.Timedelta(<span class="number">1</span>, unit=<span class="string">'s'</span>).value</div><div class="line"></div><div class="line"><span class="comment"># SELECT FROM_UNIXTIME(`timestamp`)</span></div><div class="line">pd.to_datetime(df[<span class="string">'timestamp'</span>], unit=<span class="string">'s'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SELECT `datetime` + INTERVAL 1 DAY</span></div><div class="line">df[<span class="string">'datetime'</span>] + pd.Timedelta(<span class="number">1</span>, unit=<span class="string">'D'</span>)</div></pre></td></tr></table></figure>
<h2 id="WHERE-选择行"><a href="#WHERE-选择行" class="headerlink" title="WHERE - 选择行"></a><code>WHERE</code> - 选择行</h2><p>在 Pandas 中使用逻辑表达式后，会返回一个布尔型的 Series，通过它可以对数据集进行过滤：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>).head()</div><div class="line"><span class="comment"># 0  True</span></div><div class="line"><span class="comment"># 1 False</span></div><div class="line"><span class="comment"># 2  True</span></div><div class="line"><span class="comment"># dtype: bool</span></div><div class="line"></div><div class="line"><span class="comment"># WHERE delay &gt; 0</span></div><div class="line">df[df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>我们可以用位运算符来组合多个查询条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WHERE delay &gt; 0 AND distance &lt;= 500</span></div><div class="line">df[(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>) &amp; (df[<span class="string">'distance'</span>] &lt;= <span class="number">500</span>)]</div><div class="line"></div><div class="line"><span class="comment"># WHERE delay &gt; 0 OR origin = 'BUR'</span></div><div class="line">df[(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>) | (df[<span class="string">'origin'</span>] == <span class="string">'BUR'</span>)]</div><div class="line"></div><div class="line"><span class="comment"># WHERE NOT (delay &gt; 0)</span></div><div class="line">df[~(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>)]</div></pre></td></tr></table></figure>
<p>对于 <code>IS NULL</code> 和 <code>IS NOT NULL</code>，也提供了相应的内置函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df[df[<span class="string">'delay'</span>].isnull()] <span class="comment"># delay IS NULL</span></div><div class="line">df[df[<span class="string">'delay'</span>].notnull()] <span class="comment"># delay IS NOT NUL</span></div></pre></td></tr></table></figure>
<p>此外，Pandas 还提供了 <code>df.query</code> 方法，可以使用字符串表达式来编写过滤条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.query(<span class="string">'delay &gt; 0 and distaince &lt;= 500'</span>)</div><div class="line">df.query(<span class="string">'(delay &gt; 0) | (origin == "BUR")'</span>)</div></pre></td></tr></table></figure>
<p>其实，Pandas 提供了功能强大的数据选取工具，很多是无法用 SQL 表达出来的，建议详细阅读 <a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html" target="_blank" rel="external">Indexing and Selecting Data</a> 文档，其中包含了丰富的示例。</p>
<h2 id="GROUP-BY-汇总"><a href="#GROUP-BY-汇总" class="headerlink" title="GROUP BY - 汇总"></a><code>GROUP BY</code> - 汇总</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT origin, COUNT(*) FROM flights GROUP BY origin</span></div><div class="line">df.groupby(<span class="string">'origin'</span>).size()</div><div class="line"><span class="comment"># origin</span></div><div class="line"><span class="comment"># ABQ    22</span></div><div class="line"><span class="comment"># ALB     4</span></div><div class="line"><span class="comment"># AMA     4</span></div><div class="line"><span class="comment"># dtype: int64</span></div></pre></td></tr></table></figure>
<p>聚合运算包含了两个部分，一是分组字段，二是聚合函数。我们可以传递多个分组字段给 <code>df.groupby</code>，也能够指定多个聚合函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT origin, destination, SUM(delay), AVG(distance)</span></div><div class="line"><span class="comment"># GROUP BY origin, destination</span></div><div class="line">df.groupby([<span class="string">'origin'</span>, <span class="string">'destination'</span>]).agg(&#123;</div><div class="line">    <span class="string">'delay'</span>: np.sum,</div><div class="line">    <span class="string">'distance'</span>: np.mean</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="comment"># SELECT origin, MIN(delay), MAX(delay) GROUP BY origin</span></div><div class="line">df.groupby(<span class="string">'origin'</span>)[<span class="string">'delay'</span>].agg([<span class="string">'min'</span>, <span class="string">'max'</span>])</div></pre></td></tr></table></figure>
<p>我们还可以将函数的运行结果作为分组条件。更多示例请见 <a href="https://pandas.pydata.org/pandas-docs/stable/groupby.html" target="_blank" rel="external">Group By: split-apply-combine</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT LENGTH(origin), COUNT(*) GROUP BY LENGTH(origin)</span></div><div class="line">df.set_index(<span class="string">'origin'</span>).groupby(len).size()</div></pre></td></tr></table></figure>
<h2 id="ORDER-BY-排序"><a href="#ORDER-BY-排序" class="headerlink" title="ORDER BY - 排序"></a><code>ORDER BY</code> - 排序</h2><p>Pandas 中有两类排序，按索引和按数值。如果不了解 Pandas 的索引，还请自行查阅相关教程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ORDER BY origin</span></div><div class="line">df.set_index(<span class="string">'origin'</span>).sort_index()</div><div class="line">df.sort_values(by=<span class="string">'origin'</span>)</div><div class="line"></div><div class="line"><span class="comment"># ORDER BY origin ASC, destination DESC</span></div><div class="line">df.sort_values(by=[<span class="string">'origin'</span>, <span class="string">'destination'</span>], ascending=[<span class="keyword">True</span>, <span class="keyword">False</span>])</div></pre></td></tr></table></figure>
<h2 id="JOIN-关联查询"><a href="#JOIN-关联查询" class="headerlink" title="JOIN - 关联查询"></a><code>JOIN</code> - 关联查询</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># FROM product a LEFT JOIN category b ON a.cid = b.id</span></div><div class="line">pd.merge(df_product, df_category, left_on=<span class="string">'cid'</span>, right_on=<span class="string">'id'</span>, how=<span class="string">'left'</span>)</div></pre></td></tr></table></figure>
<p>如果联合查询的键是同名的，可以直接使用 <code>on=[&#39;k1&#39;, &#39;k2&#39;]</code>。默认的关联方式是 <code>INNER JOIN</code>（<code>how=&#39;inner&#39;</code>），其它还有左外连接（<code>left</code>）、右外连接（<code>right</code>）、以及 <code>FULL OUTER JOIN</code>（<code>outer</code>)。</p>
<p><code>pd.concat</code> 可用于实现 <code>UNION</code> 查询。 更多关联查询的示例请参考 <a href="https://pandas.pydata.org/pandas-docs/stable/merging.html" target="_blank" rel="external">Merge, join, and concatenate</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT * FROM a UNION SELECT * FROM b</span></div><div class="line">pd.concat([df_a, df_b]).drop_duplicates()</div></pre></td></tr></table></figure>
<h1 id="分组排名"><a href="#分组排名" class="headerlink" title="分组排名"></a>分组排名</h1><p>最后，我们经常会需要在分组中按某种规则排序，并获得前几位的记录。MySQL 中需要通过变量来实现，Pandas 中则可以使用 <code>rank</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rnk = df.groupby(<span class="string">'origin'</span>)[<span class="string">'delay'</span>].rank(method=<span class="string">'first'</span>, ascending=<span class="keyword">False</span>)</div><div class="line">df.assign(rnk=rnk).query(<span class="string">'rnk &lt;= 3'</span>).sort_values([<span class="string">'origin'</span>, <span class="string">'rnk'</span>])</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html" target="_blank" rel="external">https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html</a></li>
<li><a href="http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/" target="_blank" rel="external">http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/</a></li>
<li><a href="http://codingsight.com/pivot-tables-in-mysql/" target="_blank" rel="external">http://codingsight.com/pivot-tables-in-mysql/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://pandas.pydata.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Pandas&lt;/a&gt; 是一款广泛使用的数据处理工具。结合 NumPy 和 Matplotlib 类库，我们可以在内存中进行高性能的数据清洗、转换、分析及可视化工作。虽然 Python 本身是一门非常容易学习的语言，但要熟练掌握 Pandas 丰富的 API 接口及正确的使用方式，还是需要投入一定时间的。对于数据开发工程师或分析师而言，SQL 语言是标准的数据查询工具。本文提供了一系列的示例，如何将常见的 SQL 查询语句使用 Pandas 来实现。&lt;/p&gt;
&lt;p&gt;Pandas 的安装和基本概念并不在本文讲述范围内，请读者到官网上阅读相关文档，或者阅读《&lt;a href=&quot;https://book.douban.com/subject/25779298/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;利用 Python 进行数据分析&lt;/a&gt;》一书。我推荐大家使用 &lt;a href=&quot;https://www.continuum.io/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Anaconda&lt;/a&gt; Python 套件，其中集成了 &lt;a href=&quot;https://pythonhosted.org/spyder/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spyder&lt;/a&gt; 集成开发环境。在运行下文的代码之前，请先引入 Pandas 和 NumPy 包：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;FROM-读取数据&quot;&gt;&lt;a href=&quot;#FROM-读取数据&quot; class=&quot;headerlink&quot; title=&quot;FROM - 读取数据&quot;&gt;&lt;/a&gt;&lt;code&gt;FROM&lt;/code&gt; - 读取数据&lt;/h2&gt;&lt;p&gt;首先，我们需要将数据加载到工作区（内存）。Pandas 原生支持非常多的数据格式，CSV 是较常见的一种。我们以航班延误时间数据集为例（&lt;a href=&quot;/uploads/flights.csv&quot;&gt;下载地址&lt;/a&gt;）：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;date,delay,distance,origin,destination&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;02221605,3,358,BUR,SMF&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;01022100,-5,239,HOU,DAL&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;03210808,6,288,BWI,ALB&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;我们可以使用 &lt;code&gt;pd.read_csv&lt;/code&gt; 函数加载它：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;df = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;flights.csv&#39;&lt;/span&gt;, dtype=&amp;#123;&lt;span class=&quot;string&quot;&gt;&#39;date&#39;&lt;/span&gt;: str&amp;#125;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.head()&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这条命令会将 &lt;code&gt;flights.csv&lt;/code&gt; 文件读入内存，使用首行作为列名，并自动检测每一列的数据类型。其中，由于 &lt;code&gt;date&lt;/code&gt; 一列的日期格式是 &lt;code&gt;%m%d%H%M&lt;/code&gt;，自动转换成数字后会失去月份的前异零（02 月的 0），因此我们显式指定了该列的 &lt;code&gt;dtype&lt;/code&gt;，告知 Pandas 保留原值。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/cnblogs/tags/analytics/"/>
    
      <category term="pandas" scheme="http://shzhangji.com/cnblogs/tags/pandas/"/>
    
      <category term="sql" scheme="http://shzhangji.com/cnblogs/tags/sql/"/>
    
  </entry>
  
  <entry>
    <title>使用 WebSocket 和 Python 编写日志查看器</title>
    <link href="http://shzhangji.com/cnblogs/2017/07/16/log-tailer-with-websocket-and-python/"/>
    <id>http://shzhangji.com/cnblogs/2017/07/16/log-tailer-with-websocket-and-python/</id>
    <published>2017-07-16T07:55:05.000Z</published>
    <updated>2017-07-16T07:56:18.121Z</updated>
    
    <content type="html"><![CDATA[<p>在生产环境运维工作中，查看线上服务器日志是一项常规工作。如果这项工作可以在浏览器中进行，而无需登录服务器执行 <code>tail -f</code> 命令，就太方便了。我们可以使用 WebSocket 技术轻松实现这一目标。在本文中，我将带各位一起使用 Python 编写一个日志查看工具。</p>
<p><img src="/cnblogs/images/logviewer-websocket.png" alt="基于 WebSocket 的日志查看器"></p>
<h2 id="WebSocket-简介"><a href="#WebSocket-简介" class="headerlink" title="WebSocket 简介"></a>WebSocket 简介</h2><p>WebSocket 是一个标准化协议，构建在 TCP 之上，能够在客户端和服务端之间建立一个全双工的通信渠道。这里的客户端和服务端通常是用户浏览器和 Web 服务器。在 WebSocket 诞生之前，如果我们想保持这样的一个长连接，就需要使用诸如长轮询、永久帧、Comet 等技术。而现今 WebSocket 已经得到了所有主流浏览器的支持，我们可以使用它开发出在线聊天室、游戏、实时仪表盘等软件。此外，WebSocket 可以通过 HTTP Upgrade 请求来建立连接，并使用 80 端口通信，从而降低对现有网络环境的影响，如无需穿越防火墙。</p>
<a id="more"></a>
<h2 id="websockets-Python-类库"><a href="#websockets-Python-类库" class="headerlink" title="websockets Python 类库"></a><code>websockets</code> Python 类库</h2><p><code>websockets</code> 是第三方的 Python 类库，它能基于 Python 提供的 <code>asyncio</code> 包来实现 WebSocket 服务端以及客户端应用。我们可以使用 <code>pip</code> 来安装它，要求 Python 3.3 以上的版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install websockets</div><div class="line"><span class="comment"># For Python 3.3</span></div><div class="line">pip install asyncio</div></pre></td></tr></table></figure>
<p>下面是一段简单的 Echo 服务代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> asyncio</div><div class="line"><span class="keyword">import</span> websockets</div><div class="line"></div><div class="line"><span class="meta">@asyncio.coroutine</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">echo</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    message = <span class="keyword">yield</span> <span class="keyword">from</span> websocket.recv()</div><div class="line">    print(<span class="string">'recv'</span>, message)</div><div class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(message)</div><div class="line"></div><div class="line">start_server = websockets.serve(echo, <span class="string">'localhost'</span>, <span class="number">8765</span>)</div><div class="line"></div><div class="line">asyncio.get_event_loop().run_until_complete(start_server)</div><div class="line">asyncio.get_event_loop().run_forever()</div></pre></td></tr></table></figure>
<p>可以看到，我们使用 Python 的协程来处理客户端请求。协程是 Python 3.3 引入的新概念，简单来说，它能通过单个线程来实现并发编程，主要适用于处理套接字 I/O 请求等场景。Python 3.5 开始又引入了 <code>async</code> 和 <code>await</code> 关键字，方便程序员使用协程。以下是使用新关键字对 Echo 服务进行改写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">echo</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    message = <span class="keyword">await</span> websocket.recv()</div><div class="line">    <span class="keyword">await</span> websocket.send(message)</div></pre></td></tr></table></figure>
<p>对于客户端应用，我们直接使用浏览器内置的 <code>WebSocket</code> 类。将下面的代码直接粘贴到 Chrome 浏览器的 JavaScript 控制台中就可以运行了：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> ws = <span class="keyword">new</span> WebSocket(<span class="string">'ws://localhost:8765'</span>)</div><div class="line">ws.onmessage = <span class="function">(<span class="params">event</span>) =&gt;</span> &#123;</div><div class="line">  <span class="built_in">console</span>.log(event.data)</div><div class="line">&#125;</div><div class="line">ws.onopen = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</div><div class="line">  ws.send(<span class="string">'hello'</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="查看并监听日志"><a href="#查看并监听日志" class="headerlink" title="查看并监听日志"></a>查看并监听日志</h2><p>我们将通过以下几步来构建日志查看器：</p>
<ul>
<li>首先，客户端发起一个 WebSocket 请求，并将请求的文件路径包含在 URL 中，形如 <code>ws://localhost:8765/tmp/build.log?tail=1</code>；</li>
<li>服务端接受到请求后，将文件路径解析出来，顺带解析出是否要持续监听日志的标志位；</li>
<li>服务端打开日志文件，开始不断向客户端发送日志文件内容。</li>
</ul>
<p>完整的源代码可以在 <a href="https://github.com/jizhang/logviewer" target="_blank" rel="external">GitHub</a> 中查看，以下只截取重要的部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@asyncio.coroutine</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">view_log</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    parse_result = urllib.parse.urlparse(path)</div><div class="line">    file_path = os.path.abspath(parse_result.path)</div><div class="line">    query = urllib.parse.parse_qs(parse_result.query)</div><div class="line">    tail = query <span class="keyword">and</span> query[<span class="string">'tail'</span>] <span class="keyword">and</span> query[<span class="string">'tail'</span>][<span class="number">0</span>] == <span class="string">'1'</span></div><div class="line">    <span class="keyword">with</span> open(file_path) <span class="keyword">as</span> f:</div><div class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(f.read())</div><div class="line">        <span class="keyword">if</span> tail:</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                content = f.read()</div><div class="line">                <span class="keyword">if</span> content:</div><div class="line">                    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(content)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> websocket.close()</div></pre></td></tr></table></figure>
<h2 id="其它特性"><a href="#其它特性" class="headerlink" title="其它特性"></a>其它特性</h2><ul>
<li>在实际应用中发现，浏览器有时不会正确关闭 WebSocket 连接，导致服务端资源浪费，因此我们添加一个简单的心跳机制：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> time.time() - last_heartbeat &gt; HEARTBEAT_INTERVAL:</div><div class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(<span class="string">'ping'</span>)</div><div class="line">    pong = <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.wait_for(websocket.recv(), <span class="number">5</span>)</div><div class="line">    <span class="keyword">if</span> pong != <span class="string">'pong'</span>:</div><div class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Ping error'</span>))</div><div class="line">    last_heartbeat = time.time()</div></pre></td></tr></table></figure>
<ul>
<li>日志文件中有时会包含 ANSI 颜色高亮（如日志级别），我们可以使用 <code>ansi2html</code> 包来将高亮部分转换成 HTML 代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> ansi2html <span class="keyword">import</span> Ansi2HTMLConverter</div><div class="line">conv = Ansi2HTMLConverter(inline=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(conv.convert(content, full=<span class="keyword">False</span>))</div></pre></td></tr></table></figure>
<ul>
<li>最后，日志文件路径也需要进行权限检查，本例中是将客户端传递的路径转换成绝对路径后，简单判断了路径前缀，以作权限控制。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/WebSocket" target="_blank" rel="external">WebSocket - Wikipedia</a></li>
<li><a href="https://websockets.readthedocs.io/en/stable/intro.html" target="_blank" rel="external">websockets - Get Started</a></li>
<li><a href="https://docs.python.org/3/library/asyncio-task.html" target="_blank" rel="external">Tasks and coroutines</a></li>
<li><a href="https://stackoverflow.com/questions/12523044/how-can-i-tail-a-log-file-in-python" target="_blank" rel="external">How can I tail a log file in Python?</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在生产环境运维工作中，查看线上服务器日志是一项常规工作。如果这项工作可以在浏览器中进行，而无需登录服务器执行 &lt;code&gt;tail -f&lt;/code&gt; 命令，就太方便了。我们可以使用 WebSocket 技术轻松实现这一目标。在本文中，我将带各位一起使用 Python 编写一个日志查看工具。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/logviewer-websocket.png&quot; alt=&quot;基于 WebSocket 的日志查看器&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;WebSocket-简介&quot;&gt;&lt;a href=&quot;#WebSocket-简介&quot; class=&quot;headerlink&quot; title=&quot;WebSocket 简介&quot;&gt;&lt;/a&gt;WebSocket 简介&lt;/h2&gt;&lt;p&gt;WebSocket 是一个标准化协议，构建在 TCP 之上，能够在客户端和服务端之间建立一个全双工的通信渠道。这里的客户端和服务端通常是用户浏览器和 Web 服务器。在 WebSocket 诞生之前，如果我们想保持这样的一个长连接，就需要使用诸如长轮询、永久帧、Comet 等技术。而现今 WebSocket 已经得到了所有主流浏览器的支持，我们可以使用它开发出在线聊天室、游戏、实时仪表盘等软件。此外，WebSocket 可以通过 HTTP Upgrade 请求来建立连接，并使用 80 端口通信，从而降低对现有网络环境的影响，如无需穿越防火墙。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/cnblogs/tags/python/"/>
    
      <category term="ops" scheme="http://shzhangji.com/cnblogs/tags/ops/"/>
    
      <category term="websocket" scheme="http://shzhangji.com/cnblogs/tags/websocket/"/>
    
  </entry>
  
  <entry>
    <title>为什么不用 ES6 完全替换 Lodash</title>
    <link href="http://shzhangji.com/cnblogs/2017/06/29/why-use-lodash-when-es6-is-available/"/>
    <id>http://shzhangji.com/cnblogs/2017/06/29/why-use-lodash-when-es6-is-available/</id>
    <published>2017-06-29T06:49:14.000Z</published>
    <updated>2017-06-29T12:35:51.008Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://lodash.com/" target="_blank" rel="external">Lodash</a> 是一款非常知名的 JavaScript 工具库，能够让开发者十分便捷地操纵数组和对象。我则是非常喜欢用它提供的函数式编程风格来操作集合类型，特别是链式调用和惰性求值。然而，随着 <a href="http://www.ecma-international.org/ecma-262/6.0/" target="_blank" rel="external">ECMAScript 2015 Standard (ES6)</a> 得到越来越多主流浏览器的支持，以及像 <a href="https://babeljs.io/" target="_blank" rel="external">Babel</a> 这样，能够将 ES6 代码编译成 ES5 从而在旧浏览器上运行的工具日渐流行，人们会发现许多 Lodash 提供的功能已经可以用 ES6 来替换了。然而真的如此吗？我认为，Lodash 仍然会非常流行，因为它可以为程序员提供更多的便利，并且优化我们编程的方式。</p>
<h2 id="map-和-Array-map-是有区别的"><a href="#map-和-Array-map-是有区别的" class="headerlink" title="_.map 和 Array#map 是有区别的"></a><code>_.map</code> 和 <code>Array#map</code> 是有区别的</h2><p>在处理集合对象时，<code>_.map</code>、<code>_.reduce</code>、<code>_.filter</code>、以及 <code>_.forEach</code> 的使用频率很高，而今 ES6 已经能够原生支持这些操作：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">_.map([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; i + <span class="number">1</span>)</div><div class="line">_.reduce([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (sum, i) =&gt; sum + i, <span class="number">0</span>)</div><div class="line">_.filter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; i &gt; <span class="number">1</span>)</div><div class="line">_.forEach([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; &#123; <span class="built_in">console</span>.log(i) &#125;)</div><div class="line"></div><div class="line"><span class="comment">// 使用 ES6 改写</span></div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].map(<span class="function">(<span class="params">i</span>) =&gt;</span> i + <span class="number">1</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].reduce(<span class="function">(<span class="params">sum, i</span>) =&gt;</span> sum + i, <span class="number">0</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].filter(<span class="function">(<span class="params">i</span>) =&gt;</span> i &gt; <span class="number">1</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].forEach(<span class="function">(<span class="params">i</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(i) &#125;)</div></pre></td></tr></table></figure>
<p>但是，Lodash 的 <code>_.map</code> 函数功能更强大，它能够操作对象类型，提供了遍历和过滤的快捷方式，能够惰性求值，对 <code>null</code> 值容错，并且有着更好的性能。</p>
<a id="more"></a>
<h3 id="遍历对象类型"><a href="#遍历对象类型" class="headerlink" title="遍历对象类型"></a>遍历对象类型</h3><p>ES6 中有以下几种方式来遍历对象：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">in</span> obj) &#123; <span class="built_in">console</span>.log(obj[key]) &#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">of</span> <span class="built_in">Object</span>.keys(obj)) &#123; <span class="built_in">console</span>.log(obj[key]) &#125;</div><div class="line"><span class="built_in">Object</span>.keys(obj).forEach(<span class="function">(<span class="params">key</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(obj[key]) &#125;)</div></pre></td></tr></table></figure>
<p>Lodash 中则有一个统一的方法 <code>_.forEach</code>：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_.forEach(obj, (value, key) =&gt; &#123; <span class="built_in">console</span>.log(value) &#125;)</div></pre></td></tr></table></figure>
<p>虽然 ES6 的 <code>Map</code> 类型也<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map" target="_blank" rel="external">提供</a>了 <code>forEach</code> 方法，但我们需要先费些功夫将普通的对象类型转换为 <code>Map</code> 类型：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// http://stackoverflow.com/a/36644532/1030720</span></div><div class="line"><span class="keyword">const</span> buildMap = <span class="function"><span class="params">o</span> =&gt;</span> <span class="built_in">Object</span>.keys(o).reduce(<span class="function">(<span class="params">m, k</span>) =&gt;</span> m.set(k, o[k]), <span class="keyword">new</span> <span class="built_in">Map</span>());</div></pre></td></tr></table></figure>
<h3 id="遍历和过滤的快捷方式"><a href="#遍历和过滤的快捷方式" class="headerlink" title="遍历和过滤的快捷方式"></a>遍历和过滤的快捷方式</h3><p>比如我们想从一组对象中摘取出某个属性的值：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> arr = [&#123; <span class="attr">n</span>: <span class="number">1</span> &#125;, &#123; <span class="attr">n</span>: <span class="number">2</span> &#125;]</div><div class="line"><span class="comment">// ES6</span></div><div class="line">arr.map(<span class="function">(<span class="params">obj</span>) =&gt;</span> obj.n)</div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.map(arr, <span class="string">'n'</span>)</div></pre></td></tr></table></figure>
<p>当对象类型的嵌套层级很多时，Lodash 的快捷方式就更实用了：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> arr = [</div><div class="line">  &#123; <span class="attr">a</span>: [ &#123; <span class="attr">n</span>: <span class="number">1</span> &#125; ]&#125;,</div><div class="line">  &#123; <span class="attr">b</span>: [ &#123; <span class="attr">n</span>: <span class="number">1</span> &#125; ]&#125;</div><div class="line">]</div><div class="line"><span class="comment">// ES6</span></div><div class="line">arr.map(<span class="function">(<span class="params">obj</span>) =&gt;</span> obj.a[<span class="number">0</span>].n) <span class="comment">// TypeError: 属性 'a' 在 arr[1] 中未定义</span></div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.map(arr, <span class="string">'a[0].n'</span>) <span class="comment">// =&gt; [1, undefined]</span></div></pre></td></tr></table></figure>
<p>可以看到，Lodash 的快捷方式还对 <code>null</code> 值做了容错处理。此外还有过滤快捷方式，以下是从 Lodash 官方文档中摘取的示例代码：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> users = [</div><div class="line">  &#123; <span class="string">'user'</span>: <span class="string">'barney'</span>, <span class="string">'age'</span>: <span class="number">36</span>, <span class="string">'active'</span>: <span class="literal">true</span> &#125;,</div><div class="line">  &#123; <span class="string">'user'</span>: <span class="string">'fred'</span>,   <span class="string">'age'</span>: <span class="number">40</span>, <span class="string">'active'</span>: <span class="literal">false</span> &#125;</div><div class="line">];</div><div class="line"><span class="comment">// ES6</span></div><div class="line">users.filter(<span class="function">(<span class="params">o</span>) =&gt;</span> o.active)</div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.filter(users, <span class="string">'active'</span>)</div><div class="line">_.filter(users, [<span class="string">'active'</span>, <span class="literal">true</span>])</div><div class="line">_.filter(users, &#123;<span class="string">'active'</span>: <span class="literal">true</span>, <span class="string">'age'</span>: <span class="number">36</span>&#125;)</div></pre></td></tr></table></figure>
<h3 id="链式调用和惰性求值"><a href="#链式调用和惰性求值" class="headerlink" title="链式调用和惰性求值"></a>链式调用和惰性求值</h3><p>Lodash 的这一特性就非常有趣和实用了。现在很流行使用短小可测的函数，结合链式调用和惰性求值，来操作集合类型。大部分 Lodash 函数都可以进行链式调用，下面是一个典型的 <strong>WordCount</strong> 示例：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> lines = <span class="string">`</span></div><div class="line"><span class="string">an apple orange the grape</span></div><div class="line"><span class="string">banana an apple melon</span></div><div class="line"><span class="string">an orange banana apple</span></div><div class="line"><span class="string">`</span>.split(<span class="string">'\n'</span>)</div><div class="line"></div><div class="line">_.chain(lines)</div><div class="line">  .flatMap(<span class="function"><span class="params">line</span> =&gt;</span> line.split(<span class="regexp">/\s+/</span>))</div><div class="line">  .filter(<span class="function"><span class="params">word</span> =&gt;</span> word.length &gt; <span class="number">3</span>)</div><div class="line">  .groupBy(_.identity)</div><div class="line">  .mapValues(_.size)</div><div class="line">  .forEach(<span class="function">(<span class="params">count, word</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(word, count) &#125;)</div><div class="line"></div><div class="line"><span class="comment">// apple 3</span></div><div class="line"><span class="comment">// orange 2</span></div><div class="line"><span class="comment">// grape 1</span></div><div class="line"><span class="comment">// banana 2</span></div><div class="line"><span class="comment">// melon 1</span></div></pre></td></tr></table></figure>
<h2 id="解构赋值和箭头函数"><a href="#解构赋值和箭头函数" class="headerlink" title="解构赋值和箭头函数"></a>解构赋值和箭头函数</h2><p>ES6 引入了解构赋值、箭头函数等新的语言特性，可以用来替换 Lodash：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.head([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment">// =&gt; 1</span></div><div class="line">_.tail([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment">// =&gt; [2, 3]</span></div><div class="line"><span class="comment">// ES6 解构赋值（destructuring syntax）</span></div><div class="line"><span class="keyword">const</span> [head, ...tail] = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> say = _.rest(<span class="function">(<span class="params">who, fruits</span>) =&gt;</span> who + <span class="string">' likes '</span> + fruits.join(<span class="string">','</span>))</div><div class="line">say(<span class="string">'Jerry'</span>, <span class="string">'apple'</span>, <span class="string">'grape'</span>)</div><div class="line"><span class="comment">// ES6 spread syntax</span></div><div class="line">say = <span class="function">(<span class="params">who, ...fruits</span>) =&gt;</span> who + <span class="string">' likes '</span> + fruits.join(<span class="string">','</span>)</div><div class="line">say(<span class="string">'Mary'</span>, <span class="string">'banana'</span>, <span class="string">'orange'</span>)</div><div class="line"></div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.constant(<span class="number">1</span>)() <span class="comment">// =&gt; 1</span></div><div class="line">_.identity(<span class="number">2</span>) <span class="comment">// =&gt; 2</span></div><div class="line"><span class="comment">// ES6</span></div><div class="line">(<span class="function"><span class="params">x</span> =&gt;</span> (<span class="function"><span class="params">()</span> =&gt;</span> x))(<span class="number">1</span>)() <span class="comment">// =&gt; 1</span></div><div class="line">(<span class="function"><span class="params">x</span> =&gt;</span> x)(<span class="number">2</span>) <span class="comment">// =&gt; 2</span></div><div class="line"></div><div class="line"><span class="comment">// 偏应用（Partial application）</span></div><div class="line"><span class="keyword">let</span> add = <span class="function">(<span class="params">a, b</span>) =&gt;</span> a + b</div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> add1 = _.partial(add, <span class="number">1</span>)</div><div class="line"><span class="comment">// ES6</span></div><div class="line">add1 = <span class="function"><span class="params">b</span> =&gt;</span> add(<span class="number">1</span>, b)</div><div class="line"></div><div class="line"><span class="comment">// 柯里化（Curry）</span></div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> curriedAdd = _.curry(add)</div><div class="line"><span class="keyword">let</span> add1 = curriedAdd(<span class="number">1</span>)</div><div class="line"><span class="comment">// ES6</span></div><div class="line">curriedAdd = <span class="function"><span class="params">a</span> =&gt;</span> b =&gt; a + b</div><div class="line">add1 = curriedAdd(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>对于集合类操作，我更倾向于使用 Lodash 函数，因为它们的定义更准确，而且可以串联成链；对于那些可以用箭头函数来重写的用例，Lodash 同样显得更简单和清晰一些。此外，<a href="#参考资料">参考资料</a>里的几篇文章中也提到，在函数式编程场景下，Lodash 提供了更为实用的柯里化、<a href="https://lodash.com/docs/#add" target="_blank" rel="external">运算符函数</a>、<a href="https://github.com/lodash/lodash/wiki/FP-Guide" target="_blank" rel="external">FP 模式</a>等特性。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Lodash 为 JavaScript 语言增添了诸多特性，程序员可以用它轻松写出语义精确、执行高效的代码。此外，Lodash 已经完全<a href="https://lodash.com/custom-builds" target="_blank" rel="external">模块化</a>了。虽然它的某些特性最终会被淘汰，但仍有许多功能是值得我们继续使用的。同时，Lodash 这样的类库也在不断推动 JavaScript 语言本身的发展。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://www.sitepoint.com/lodash-features-replace-es6/" target="_blank" rel="external">10 Lodash Features You Can Replace with ES6</a></li>
<li><a href="https://derickbailey.com/2016/09/12/does-es6-mean-the-end-of-underscore-lodash/" target="_blank" rel="external">Does ES6 Mean The End Of Underscore / Lodash?</a></li>
<li><a href="https://www.reddit.com/r/javascript/comments/41fq2s/why_should_i_use_lodash_or_rather_what_lodash/" target="_blank" rel="external">Why should I use lodash - reddit</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://lodash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lodash&lt;/a&gt; 是一款非常知名的 JavaScript 工具库，能够让开发者十分便捷地操纵数组和对象。我则是非常喜欢用它提供的函数式编程风格来操作集合类型，特别是链式调用和惰性求值。然而，随着 &lt;a href=&quot;http://www.ecma-international.org/ecma-262/6.0/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ECMAScript 2015 Standard (ES6)&lt;/a&gt; 得到越来越多主流浏览器的支持，以及像 &lt;a href=&quot;https://babeljs.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Babel&lt;/a&gt; 这样，能够将 ES6 代码编译成 ES5 从而在旧浏览器上运行的工具日渐流行，人们会发现许多 Lodash 提供的功能已经可以用 ES6 来替换了。然而真的如此吗？我认为，Lodash 仍然会非常流行，因为它可以为程序员提供更多的便利，并且优化我们编程的方式。&lt;/p&gt;
&lt;h2 id=&quot;map-和-Array-map-是有区别的&quot;&gt;&lt;a href=&quot;#map-和-Array-map-是有区别的&quot; class=&quot;headerlink&quot; title=&quot;_.map 和 Array#map 是有区别的&quot;&gt;&lt;/a&gt;&lt;code&gt;_.map&lt;/code&gt; 和 &lt;code&gt;Array#map&lt;/code&gt; 是有区别的&lt;/h2&gt;&lt;p&gt;在处理集合对象时，&lt;code&gt;_.map&lt;/code&gt;、&lt;code&gt;_.reduce&lt;/code&gt;、&lt;code&gt;_.filter&lt;/code&gt;、以及 &lt;code&gt;_.forEach&lt;/code&gt; 的使用频率很高，而今 ES6 已经能够原生支持这些操作：&lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;_.map([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; i + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.reduce([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (sum, i) =&amp;gt; sum + i, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.filter([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; i &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.forEach([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; &amp;#123; &lt;span class=&quot;built_in&quot;&gt;console&lt;/span&gt;.log(i) &amp;#125;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 使用 ES6 改写&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].map(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; i + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].reduce(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;sum, i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; sum + i, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].filter(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; i &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].forEach(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; &amp;#123; &lt;span class=&quot;built_in&quot;&gt;console&lt;/span&gt;.log(i) &amp;#125;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;但是，Lodash 的 &lt;code&gt;_.map&lt;/code&gt; 函数功能更强大，它能够操作对象类型，提供了遍历和过滤的快捷方式，能够惰性求值，对 &lt;code&gt;null&lt;/code&gt; 值容错，并且有着更好的性能。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="javascript" scheme="http://shzhangji.com/cnblogs/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/cnblogs/tags/frontend/"/>
    
      <category term="es6" scheme="http://shzhangji.com/cnblogs/tags/es6/"/>
    
      <category term="lodash" scheme="http://shzhangji.com/cnblogs/tags/lodash/"/>
    
  </entry>
  
  <entry>
    <title>使用 Crossfilter 和 dc.js 构建交互式报表</title>
    <link href="http://shzhangji.com/cnblogs/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/"/>
    <id>http://shzhangji.com/cnblogs/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/</id>
    <published>2017-06-18T10:59:53.000Z</published>
    <updated>2017-06-18T11:02:15.806Z</updated>
    
    <content type="html"><![CDATA[<p>在对多维数据集进行图表分析时，我们希望在图表之间建立联系，选择图表中的一部分数据后，其他图表也会相应变动。这项工作可以通过开发完成，即在服务端对数据进行过滤，并更新所有图表。此外，我们还可以借助 Crossfilter 和 dc.js 这两个工具，直接在浏览器中对数据进行操作。</p>
<h2 id="航班延误统计"><a href="#航班延误统计" class="headerlink" title="航班延误统计"></a>航班延误统计</h2><p>这是 Crossfilter 官方网站提供的示例，基于 <a href="http://stat-computing.org/dataexpo/2009/" target="_blank" rel="external">ASA Data Expo</a> 数据集的航班延误统计。下面我们将介绍如何用 dc.js 来实现这份交互式报表。项目源码可以在 <a href="https://jsfiddle.net/zjerryj/gjao9sws/" target="_blank" rel="external">JSFiddle</a> 中浏览，演示的数据量减少到 1000 条。</p>
<p><img src="/cnblogs/images/airline-ontime-performance.png" alt=""></p>
<a id="more"></a>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter</a> 是一个 JavaScript 类库，能够在浏览器端对大量数据进行多维分析。它的特点是可以在不同的 Group By 查询之间实现“交叉过滤”，自动连接和更新查询结果。结合 <a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js</a> 图表类库，我们就可以构建出高性能、交互式的分析报表了。</p>
<h2 id="数据集、维度、度量"><a href="#数据集、维度、度量" class="headerlink" title="数据集、维度、度量"></a>数据集、维度、度量</h2><p>Crossfilter 中有维度、度量等概念。如果你对数据仓库或统计分析有所了解，这些术语和 OLAP 立方体中的定义是相似的。</p>
<ul>
<li>数据集：即一张二维表，包含行和列，在 JavaScript 中通常是由对象组成的数组；</li>
<li>维度：用于进行 Group By 操作的字段，它通常是可枚举的值，如日期、性别，也可以是数值范围，如年龄范围等；</li>
<li>度量：可以进行合计、计算标准差等操作，通常是数值型的，如收入、子女人数；记录数也是一种度量；</li>
</ul>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> flights = d3.csv.parse(flightsCsv)</div><div class="line"><span class="keyword">let</span> flight = crossfilter(flights)</div><div class="line"><span class="keyword">let</span> hour = flight.dimension(<span class="function">(<span class="params">d</span>) =&gt;</span> d.date.getHours() + d.date.getMinutes() / <span class="number">60</span>)</div><div class="line"><span class="keyword">let</span> hours = hour.group(<span class="built_in">Math</span>.floor)</div></pre></td></tr></table></figure>
<p>这段代码首先创建了一个 crossfilter 对象，数据来源是 CSV 格式的文本。之后，我们定义了一个“小时”的维度，它是从 <code>date</code> 字段计算得到的，并近似到了整数，用作 Group By 条件。定义完这个查询后，我们就可以获取航班延误次数最多的三个小时的数据了：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hours.top(<span class="number">3</span>)</div><div class="line"><span class="comment">// 输出结果</span></div><div class="line">[</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">13</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">20</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>:  <span class="number">8</span>, <span class="attr">value</span>: <span class="number">71</span> &#125;,</div><div class="line">]</div></pre></td></tr></table></figure>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>我们将 24 个小时的延误次数用柱状图展现出来：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> hourChart = dc.barChart(<span class="string">'#hour-chart'</span>)</div><div class="line">hourChart</div><div class="line">  .width(<span class="number">350</span>)</div><div class="line">  .height(<span class="number">150</span>)</div><div class="line">  .dimension(hour)</div><div class="line">  .group(hours)</div><div class="line">  .x(d3.scale.linear()</div><div class="line">    .domain([<span class="number">0</span>, <span class="number">24</span>])</div><div class="line">    .rangeRound([<span class="number">0</span>, <span class="number">10</span> * <span class="number">24</span>]))</div><div class="line">  .controlsUseVisibility(<span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>对应的 HTML 代码是：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"hour-chart"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Time of Day</div><div class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"reset"</span> <span class="attr">href</span>=<span class="string">"javascript:;"</span> <span class="attr">style</span>=<span class="string">"visibility: hidden;"</span>&gt;</span>reset<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure>
<p>我们可以看到，dc.js 和 crossfilter 结合的非常好，只需将维度信息传入 dc.js ，并对坐标轴进行适当配置，就可以完成可视化。在这个例子中，X 轴是 0 到 24 个小时（维度），Y 轴则是航班延误次数（度量）。</p>
<p>注意这里 <code>class=&quot;reset&quot;</code> 的用法，它和 <code>controlUseVisibility</code> 一起使用，会在指定位置产生一个 <code>reset</code> 按钮。你可以试着在图表上进行拖拽，对数据进行筛选后就能看到 <code>reset</code> 按钮了。</p>
<h2 id="交叉过滤"><a href="#交叉过滤" class="headerlink" title="交叉过滤"></a>交叉过滤</h2><p>接下来我们可以创建更多图表，如航班延误时间的直方图，源代码可以在 JSFiddle 中找到。当你对其中一个图表进行过滤时，其他图表就会自动过滤和更新，这样我们就可以查看某些限定条件下数据的分布情况了。</p>
<p>dc.js 还提供了许多可视化类型，如饼图、表格、自定义HTML等。当然，要完全掌握这些，还需要学习 d3.js ，市面上很多绘图类库都是构建在它之上的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter - Fast Multidimensional Filtering for Coordinated Views</a></li>
<li><a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js - Dimensional Charting Javascript Library</a></li>
<li><a href="http://blog.rusty.io/2012/09/17/crossfilter-tutorial/" target="_blank" rel="external">Crossfiler Tutorial</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在对多维数据集进行图表分析时，我们希望在图表之间建立联系，选择图表中的一部分数据后，其他图表也会相应变动。这项工作可以通过开发完成，即在服务端对数据进行过滤，并更新所有图表。此外，我们还可以借助 Crossfilter 和 dc.js 这两个工具，直接在浏览器中对数据进行操作。&lt;/p&gt;
&lt;h2 id=&quot;航班延误统计&quot;&gt;&lt;a href=&quot;#航班延误统计&quot; class=&quot;headerlink&quot; title=&quot;航班延误统计&quot;&gt;&lt;/a&gt;航班延误统计&lt;/h2&gt;&lt;p&gt;这是 Crossfilter 官方网站提供的示例，基于 &lt;a href=&quot;http://stat-computing.org/dataexpo/2009/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ASA Data Expo&lt;/a&gt; 数据集的航班延误统计。下面我们将介绍如何用 dc.js 来实现这份交互式报表。项目源码可以在 &lt;a href=&quot;https://jsfiddle.net/zjerryj/gjao9sws/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;JSFiddle&lt;/a&gt; 中浏览，演示的数据量减少到 1000 条。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/airline-ontime-performance.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="crossfilter" scheme="http://shzhangji.com/cnblogs/tags/crossfilter/"/>
    
      <category term="dc.js" scheme="http://shzhangji.com/cnblogs/tags/dc-js/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/cnblogs/tags/analytics/"/>
    
  </entry>
  
  <entry>
    <title>Hive+Druid 实现快速查询；回归分析是机器学习吗；StructuredStreaming 可用于生产环境</title>
    <link href="http://shzhangji.com/cnblogs/2017/06/18/druid-machine-learning-spark/"/>
    <id>http://shzhangji.com/cnblogs/2017/06/18/druid-machine-learning-spark/</id>
    <published>2017-06-18T02:41:59.000Z</published>
    <updated>2017-06-29T12:35:51.007Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结合-Apache-Hive-和-Druid-实现高速-OLAP-查询"><a href="#结合-Apache-Hive-和-Druid-实现高速-OLAP-查询" class="headerlink" title="结合 Apache Hive 和 Druid 实现高速 OLAP 查询"></a>结合 Apache Hive 和 Druid 实现高速 OLAP 查询</h2><p><img src="https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Part1Image2.png" alt="使用 HiveQL 预汇总数据并保存至 Druid"></p>
<p>Hadoop 生态中，我们使用 Hive 将 SQL 语句编译为 MapReduce 任务，对海量数据进行操作；Druid 则是一款独立的分布式列式存储系统，通常用于执行面向最终用户的即席查询和实时分析。</p>
<p>Druid 的高速查询主要得益于列式存储和倒排索引，其中倒排索引是和 Hive 的主要区别。数据表中的维度字段越多，查询速度也会越快。不过 Druid 也有其不适用的场景，如无法支持大数据量的 Join 操作，对标准 SQL 的实现也十分有限。</p>
<p>Druid 和 Hive 的结合方式是这样的：首先使用 Hive 对数据进行预处理，生成 OLAP Cube 存入 Druid；当发生查询时，使用 Calcite 优化器进行分析，使用合适的引擎（Hive 或 Druid）执行操作。如，Druid 擅长执行维度汇总、TopN、时间序列查询，而 Hive 则能胜任 Join、子查询、UDF 等操作。</p>
<p>原文：<a href="https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru" target="_blank" rel="external">https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru</a></p>
<a id="more"></a>
<h2 id="回归分析是机器学习吗？"><a href="#回归分析是机器学习吗？" class="headerlink" title="回归分析是机器学习吗？"></a>回归分析是机器学习吗？</h2><p><img src="http://www.kdnuggets.com/wp-content/uploads/is-regression-machine-learning.jpg" alt=""></p>
<p>作者认为纠结这个问题的人们其实是“将重点放在了森林而忽视了树木”。我们应该将统计分析和机器学习都用作是实现“理解数据”这一目标的工具。用哪种工具并不重要，如何用好工具、建立恰当的模型、增强对数据的认知，才是最重要的。</p>
<p>过去人们也曾争论过数据挖掘和机器学习的区别。作者认为数据挖掘是一种 <em>过程</em> ，机器学习则是过程中使用的 <em>工具</em> 。因此，数据挖掘可以总结为 <em>对海量数据进行高效的统计分析</em> 。</p>
<p>机器学习由三个元素组成：数据，模型，成本函数。假设我们有 10 个数据点，使用其中的 9 个来训练模型，最后一个用来测试，这个过程人工就可以解决。那当数据越来越多，或特征值增加，或使用计算机而非人工，是否就由统计学演变成机器学习了呢？所以说，传统统计学和机器学习之间是有一个过渡的，这个过渡过程我们无法衡量，也无需衡量。</p>
<p>原文：<a href="http://www.kdnuggets.com/2017/06/regression-analysis-really-machine-learning.html" target="_blank" rel="external">http://www.kdnuggets.com/2017/06/regression-analysis-really-machine-learning.html</a></p>
<h2 id="StructuredStreaming-可用于生产环境"><a href="#StructuredStreaming-可用于生产环境" class="headerlink" title="StructuredStreaming 可用于生产环境"></a>StructuredStreaming 可用于生产环境</h2><p><img src="https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png" alt="将数据流看做一张无边界的表"></p>
<p>2016 年中，Spark 推出了 StructredStreaming，随着时间的推移，这一创新的实时计算框架也日臻成熟。</p>
<p>实时计算的复杂性在于三个方面：数据来源多样，且存在脏数据、乱序等情况；执行逻辑复杂，需要处理事件时间，结合机器学习等；运行平台也多种多样，需要应对各种系统失效。</p>
<p>StructuredStreaming 构建在 Spark SQL 之上，提供了统一的 API，同时做到高效、可扩、容错。它的理念是 <em>实时计算无需考虑数据是否实时</em> ，将实时数据流当做一张没有边界的数据表来看待。</p>
<p>传统 ETL 的流程是先将数据导出成文件，再将文件导入到表中使用，这个过程通常以小时计。而实时 ETL 则可以直接从数据源进入到表，过程以秒计。对于更为复杂的实时计算，StructuredStreaming 也提供了相应方案，包括事件时间计算、有状态的聚合算子、水位线等。</p>
<p>原文：<a href="https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/" target="_blank" rel="external">https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;结合-Apache-Hive-和-Druid-实现高速-OLAP-查询&quot;&gt;&lt;a href=&quot;#结合-Apache-Hive-和-Druid-实现高速-OLAP-查询&quot; class=&quot;headerlink&quot; title=&quot;结合 Apache Hive 和 Druid 实现高速 OLAP 查询&quot;&gt;&lt;/a&gt;结合 Apache Hive 和 Druid 实现高速 OLAP 查询&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Part1Image2.png&quot; alt=&quot;使用 HiveQL 预汇总数据并保存至 Druid&quot;&gt;&lt;/p&gt;
&lt;p&gt;Hadoop 生态中，我们使用 Hive 将 SQL 语句编译为 MapReduce 任务，对海量数据进行操作；Druid 则是一款独立的分布式列式存储系统，通常用于执行面向最终用户的即席查询和实时分析。&lt;/p&gt;
&lt;p&gt;Druid 的高速查询主要得益于列式存储和倒排索引，其中倒排索引是和 Hive 的主要区别。数据表中的维度字段越多，查询速度也会越快。不过 Druid 也有其不适用的场景，如无法支持大数据量的 Join 操作，对标准 SQL 的实现也十分有限。&lt;/p&gt;
&lt;p&gt;Druid 和 Hive 的结合方式是这样的：首先使用 Hive 对数据进行预处理，生成 OLAP Cube 存入 Druid；当发生查询时，使用 Calcite 优化器进行分析，使用合适的引擎（Hive 或 Druid）执行操作。如，Druid 擅长执行维度汇总、TopN、时间序列查询，而 Hive 则能胜任 Join、子查询、UDF 等操作。&lt;/p&gt;
&lt;p&gt;原文：&lt;a href=&quot;https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://dzone.com/articles/ultra-fast-olap-analytics-with-apache-hive-and-dru&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Digest" scheme="http://shzhangji.com/cnblogs/categories/Digest/"/>
    
    
      <category term="hive" scheme="http://shzhangji.com/cnblogs/tags/hive/"/>
    
      <category term="spark" scheme="http://shzhangji.com/cnblogs/tags/spark/"/>
    
      <category term="druid" scheme="http://shzhangji.com/cnblogs/tags/druid/"/>
    
      <category term="machine learning" scheme="http://shzhangji.com/cnblogs/tags/machine-learning/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/cnblogs/tags/stream-processing/"/>
    
  </entry>
  
  <entry>
    <title>开发人员必知的5种开源框架</title>
    <link href="http://shzhangji.com/cnblogs/2016/03/13/top-5-frameworks/"/>
    <id>http://shzhangji.com/cnblogs/2016/03/13/top-5-frameworks/</id>
    <published>2016-03-13T08:25:00.000Z</published>
    <updated>2017-08-11T23:26:39.542Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://opensource.com/business/15/12/top-5-frameworks" target="_blank" rel="external">John Esposito</a></p>
<p>软件侵吞着世界<a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460" target="_blank" rel="external">已经四年多了</a>，但开发人员看待软件的方式稍有不同。我们一直在致力于<a href="http://www.dougengelbart.org/pubs/augment-3906.html" target="_blank" rel="external">解决实际问题</a>，而<a href="http://worrydream.com/refs/Brooks-NoSilverBullet.pdf" target="_blank" rel="external">很少思考软件开发的基石</a>。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html" target="_blank" rel="external">产生泄漏</a>的抽象工具就显得越来越重要。</p>
<p>简单地来说，在那些追求生产效率的开发者眼中，<em>框架</em> 正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？</p>
<p>开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。</p>
<h2 id="1-展现层：Bootstrap"><a href="#1-展现层：Bootstrap" class="headerlink" title="1. 展现层：Bootstrap"></a>1. 展现层：Bootstrap</h2><p>我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是<a href="http://getbootstrap.com/" target="_blank" rel="external">Bootstrap</a>。Bootstrap的<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln" target="_blank" rel="external">流行度</a>非常之惊人，<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">远远甩开</a>了它的老对手<a href="http://foundation.zurb.com/" target="_blank" rel="external">Foundation</a>，以及新星<a href="http://www.getmdl.io/" target="_blank" rel="external">Material Design Lite</a>。在<a href="http://trends.builtwith.com/docinfo/Twitter-Bootstrap" target="_blank" rel="external">BuiltWith</a>上，Bootstrap占据主导地位；而在GitHub上则长期保持<a href="https://github.com/search?q=stars:%3E1&amp;s=stars&amp;type=Repositories" target="_blank" rel="external">Star数</a>和<a href="https://github.com/search?o=desc&amp;q=stars:%3E1&amp;s=forks&amp;type=Repositories" target="_blank" rel="external">Fork数</a>最多的记录。</p>
<p>如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了<a href="http://v4-alpha.getbootstrap.com/" target="_blank" rel="external">v4</a><a href="http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/" target="_blank" rel="external">内测版</a>，庆祝它的四岁生日。这个版本是对现有功能的<a href="http://v4-alpha.getbootstrap.com/migration/" target="_blank" rel="external">简化和扩充</a>，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了<a href="http://themes.getbootstrap.com/" target="_blank" rel="external">官方主题市场</a>，进一步扩充现有的<a href="https://www.google.com/search?q=bootstrap+theme+sites" target="_blank" rel="external">主题生态</a>。</p>
<h2 id="2-网页MVC：AngularJS"><a href="#2-网页MVC：AngularJS" class="headerlink" title="2. 网页MVC：AngularJS"></a>2. 网页MVC：AngularJS</h2><p>随着网页平台技术越来越<a href="https://www.w3.org/blog/news/" target="_blank" rel="external">成熟</a>，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">最</a><a href="https://www.pluralsight.com/browse#tab-courses-popular" target="_blank" rel="external">流行</a>的SPA框架当属<a href="https://angularjs.org/" target="_blank" rel="external">AngularJS</a>。</p>
<p>AngularJS有什么特别之处呢？一个词：指令（<a href="https://docs.angularjs.org/guide/directive" target="_blank" rel="external">directive</a>）。一个简单的<code>ng-</code>就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。</p>
<p>有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在<a href="https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2" target="_blank" rel="external">Angular2</a>中移除，已经<a href="http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html" target="_blank" rel="external">临近公测</a>。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。</p>
<a id="more"></a>
<h2 id="3-企业级Java：Spring-Boot"><a href="#3-企业级Java：Spring-Boot" class="headerlink" title="3. 企业级Java：Spring Boot"></a>3. 企业级Java：Spring Boot</h2><p>Java的优点是什么？运行速度快，成熟，完善的类库，庞大的生态环境，一处编译处处执行，活跃的社区等等——除了痛苦的项目起始阶段。即便是最忠实的Java开发者也会转而使用Ruby或Python来快速编写一些只会用到一次的小型脚本（别不承认）。然而，鉴于以上种种原因，Java仍然是企业级应用的首选语言。</p>
<p>这时，Spring Boot出现了，它是模板代码的终结者，有了它，你就能在一条推文中写出一个Java应用程序来：</p>
<p><a href="https://twitter.com/rob_winch/status/364871658483351552" target="_blank" rel="external">https://twitter.com/rob_winch/status/364871658483351552</a></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spring run app.groovy</span></div><div class="line"><span class="meta">@Controller</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThisWillActuallyRun</span> &#123;</span></div><div class="line">  <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</div><div class="line">  <span class="meta">@ResponseBody</span></div><div class="line">  String home() &#123;</div><div class="line">    <span class="string">"Hello World!"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>没有让人不快的XML配置，无需生成别扭的代码。这怎么可能？很简单，Spring Boot在背后做了很多工作，读上面的代码就能看出，框架会自动生成一个内嵌的servlet容器，监听8080端口，处理接收到的请求。这些都无需用户配置，而是遵从Spring Boot的约定。</p>
<p>Spring Boot有多流行？它是目前为止<a href="https://github.com/spring-projects" target="_blank" rel="external">fork数</a>和下载量最高的Spring应用（主框架除外）。2015年，<a href="https://www.google.com/trends/explore#q=spring%20boot%2C%20spring%20framework&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">在谷歌上搜索Spring Boot的人数首次超过搜索Spring框架的人</a>。</p>
<h2 id="4-数据处理：Apache-Spark"><a href="#4-数据处理：Apache-Spark" class="headerlink" title="4. 数据处理：Apache Spark"></a>4. 数据处理：Apache Spark</h2><p>很久很久以前（2004年），谷歌研发出一种编程模型（<a href="http://ayende.com/blog/4435/map-reduce-a-visual-explanation" target="_blank" rel="external">MapReduce</a>），将分布式批处理任务通用化了，并撰写了一篇<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank" rel="external">著名的论文</a>。之后，Yahoo的工程师用Java编写了一个框架（<a href="https://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a>）实现了MapReduce，以及一个<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="external">分布式文件系统</a>，使得MapReduce任务能够更简便地读写数据。</p>
<p>将近十年的时间，Hadoop主宰了大数据处理框架的生态系统，即使批处理只能解决有限的问题——也许大多数企业和科学工作者已经习惯了大数据量的批处理分析吧。然而，并不是所有大型数据集都适合使用批处理。特别地，流式数据（如传感器数据）和迭代式数据分析（机器学习算法中最为常见）都不适合使用批处理。因此，大数据领域诞生了很多<a href="https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan" target="_blank" rel="external">新的编程模型、应用架构</a>、以及各类<a href="http://www.journalofbigdata.com/content/2/1/18" target="_blank" rel="external">数据存储</a>也逐渐流行开来（甚至还从MapReduce中分离出了一种<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">新的集群管理系统</a>）。</p>
<p>在这些新兴的系统之中，<a href="http://spark.apache.org/research.html" target="_blank" rel="external">伯克利AMPLab</a>研发的<a href="http://spark.apache.org/" target="_blank" rel="external">Apache Spark</a>在2015年脱颖而出。各类调研和报告（<a href="https://dzone.com/guides/big-data-business-intelligence-and-analytics-2015" target="_blank" rel="external">DZone</a>、<a href="http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf" target="_blank" rel="external">Databricks</a>、<a href="https://info.typesafe.com/COLL-20XX-Spark-Survey-Report_LP.html?lst=PR&amp;lsd=COLL-20XX-Spark-Survey-Trends-Adoption-Report" target="_blank" rel="external">Typesafe</a>）都显示Spark的成长速度非常之快。GitHub提交数从2013年开始就呈<a href="https://github.com/apache/spark/graphs/contributors" target="_blank" rel="external">线性增长</a>，而谷歌趋势则在2015年呈现出<a href="https://www.google.com/trends/explore#q=%2Fm%2F0ndhxqz" target="_blank" rel="external">指数级的增长</a>。</p>
<p>Spark如此流行，它究竟是做什么的呢？答案很简单，非常快速的批处理，不过这点是构建在Spark的一个杀手级特性之上的，能够应用到比Hadoop多得多的编程模型中。Spark将数据表达为弹性分布式数据集（RDD），处理结果保存在多个节点的内存中，不进行复制，只是记录数据的计算过程（这点可以和<a href="http://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">CQRS</a>、<a href="http://plato.stanford.edu/entries/peirce/" target="_blank" rel="external">实用主义</a>、<a href="http://people.cs.uchicago.edu/~fortnow/papers/kaikoura.pdf" target="_blank" rel="external">Kolmogorov复杂度</a>相较）。这一特点可以让迭代算法无需从底层（较慢的）分布式存储层读取数据。同时也意味着批处理流程无需再背负Nathan Marz在<a href="http://lambda-architecture.net/" target="_blank" rel="external">Lambda架构</a>中所描述的“数据卡顿”之恶名。RDD还能让Spark模拟实时流数据处理，通过将数据切分成小块，降低延迟时间，达到大部分应用对“准实时”的要求。</p>
<h2 id="5-软件交付：Docker"><a href="#5-软件交付：Docker" class="headerlink" title="5. 软件交付：Docker"></a>5. 软件交付：Docker</h2><p>严格意义上说，<a href="https://www.docker.com/" target="_blank" rel="external">Docker</a>并不是符合“框架”的定义：代码库，通用性好，使用一系列特殊约定来解决大型重复性问题。但是，如果框架指的是能够让程序员在一种舒适的抽象层之上进行编码，那Docker将是框架中的佼佼者（我们可以称它为“外壳型框架”，多制造一些命名上的混乱吧）。而且，如果将本文的标题改为“开发人员必知的5样东西”，又不把Docker包含进来，就显得太奇怪了。</p>
<p>为什么Docker很出色？首先我们应该问什么容器很受欢迎（FreeBSD Jail, Solaries Zones, OpenVZ, LXC）？很简单：无需使用一个完整的操作系统来实现隔离；或者说，在获得安全性和便利性的提升时，无需承担额外的开销。但是，隔离也有很多种形式（比如最先想到的<code>chroot</code>，以及各种虚拟内存技术），而且可以非常方便地用<code>systemd-nspawn</code>来启动程序，而不使用Docker。然而，仅仅隔离进程还不够，那Docker有什么<a href="http://techapostle.blogspot.com/2015/04/the-3-reasons-why-docker-got-it-right.html" target="_blank" rel="external">过人之处</a>呢？</p>
<p>两个原因：Dockefile（新型的tar包）增加了便携性；它的格式成为了现行的标准。第一个原因使得应用程序交付变得容易（之前人们是通过创建轻型虚拟机来实现的）。第二个原因则让容器更容易分享（不单是<a href="https://hub.docker.com/" target="_blank" rel="external">DockerHub</a>）。我可以很容易地尝试你编写的应用程序，而不是先要做些不相关的事（想想<code>apt-get</code>给你的体验）。</p>
<h2 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h2><p><img src="https://opensource.com/sites/default/files/styles/profile_pictures/public/pictures/john-esposito.jpg?itok=xPVFPzr2" alt="John Esposito"></p>
<p>John Esposito是DZone的主编，最近刚刚完成古典学博士学位的学习，养了两只猫。之前他是VBA和Force.com的开发者、DBA、网络管理员。（但说真的，Lisp是最棒的！）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://opensource.com/business/15/12/top-5-frameworks&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;John Esposito&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;软件侵吞着世界&lt;a href=&quot;http://www.wsj.com/articles/SB10001424053111903480904576512250915629460&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;已经四年多了&lt;/a&gt;，但开发人员看待软件的方式稍有不同。我们一直在致力于&lt;a href=&quot;http://www.dougengelbart.org/pubs/augment-3906.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;解决实际问题&lt;/a&gt;，而&lt;a href=&quot;http://worrydream.com/refs/Brooks-NoSilverBullet.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;很少思考软件开发的基石&lt;/a&gt;。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么&lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;产生泄漏&lt;/a&gt;的抽象工具就显得越来越重要。&lt;/p&gt;
&lt;p&gt;简单地来说，在那些追求生产效率的开发者眼中，&lt;em&gt;框架&lt;/em&gt; 正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？&lt;/p&gt;
&lt;p&gt;开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。&lt;/p&gt;
&lt;h2 id=&quot;1-展现层：Bootstrap&quot;&gt;&lt;a href=&quot;#1-展现层：Bootstrap&quot; class=&quot;headerlink&quot; title=&quot;1. 展现层：Bootstrap&quot;&gt;&lt;/a&gt;1. 展现层：Bootstrap&lt;/h2&gt;&lt;p&gt;我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是&lt;a href=&quot;http://getbootstrap.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Bootstrap&lt;/a&gt;。Bootstrap的&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;流行度&lt;/a&gt;非常之惊人，&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;远远甩开&lt;/a&gt;了它的老对手&lt;a href=&quot;http://foundation.zurb.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Foundation&lt;/a&gt;，以及新星&lt;a href=&quot;http://www.getmdl.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Material Design Lite&lt;/a&gt;。在&lt;a href=&quot;http://trends.builtwith.com/docinfo/Twitter-Bootstrap&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;BuiltWith&lt;/a&gt;上，Bootstrap占据主导地位；而在GitHub上则长期保持&lt;a href=&quot;https://github.com/search?q=stars:%3E1&amp;amp;s=stars&amp;amp;type=Repositories&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Star数&lt;/a&gt;和&lt;a href=&quot;https://github.com/search?o=desc&amp;amp;q=stars:%3E1&amp;amp;s=forks&amp;amp;type=Repositories&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Fork数&lt;/a&gt;最多的记录。&lt;/p&gt;
&lt;p&gt;如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了&lt;a href=&quot;http://v4-alpha.getbootstrap.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;v4&lt;/a&gt;&lt;a href=&quot;http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;内测版&lt;/a&gt;，庆祝它的四岁生日。这个版本是对现有功能的&lt;a href=&quot;http://v4-alpha.getbootstrap.com/migration/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;简化和扩充&lt;/a&gt;，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了&lt;a href=&quot;http://themes.getbootstrap.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方主题市场&lt;/a&gt;，进一步扩充现有的&lt;a href=&quot;https://www.google.com/search?q=bootstrap+theme+sites&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;主题生态&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;2-网页MVC：AngularJS&quot;&gt;&lt;a href=&quot;#2-网页MVC：AngularJS&quot; class=&quot;headerlink&quot; title=&quot;2. 网页MVC：AngularJS&quot;&gt;&lt;/a&gt;2. 网页MVC：AngularJS&lt;/h2&gt;&lt;p&gt;随着网页平台技术越来越&lt;a href=&quot;https://www.w3.org/blog/news/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;成熟&lt;/a&gt;，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;最&lt;/a&gt;&lt;a href=&quot;https://www.pluralsight.com/browse#tab-courses-popular&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;流行&lt;/a&gt;的SPA框架当属&lt;a href=&quot;https://angularjs.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AngularJS&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;AngularJS有什么特别之处呢？一个词：指令（&lt;a href=&quot;https://docs.angularjs.org/guide/directive&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;directive&lt;/a&gt;）。一个简单的&lt;code&gt;ng-&lt;/code&gt;就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。&lt;/p&gt;
&lt;p&gt;有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在&lt;a href=&quot;https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Angular2&lt;/a&gt;中移除，已经&lt;a href=&quot;http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;临近公测&lt;/a&gt;。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/cnblogs/categories/Programming/"/>
    
    
      <category term="translation" scheme="http://shzhangji.com/cnblogs/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>使用Spring AOP向领域模型注入依赖</title>
    <link href="http://shzhangji.com/cnblogs/2015/09/12/model-dependency-injection-with-spring-aop/"/>
    <id>http://shzhangji.com/cnblogs/2015/09/12/model-dependency-injection-with-spring-aop/</id>
    <published>2015-09-12T14:03:00.000Z</published>
    <updated>2017-08-13T00:13:31.527Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://shzhangji.com/cnblogs/2015/09/05/anemic-domain-model/">贫血领域模型</a>这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用<code>new</code>关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。</p>
<h2 id="面向切面编程"><a href="#面向切面编程" class="headerlink" title="面向切面编程"></a>面向切面编程</h2><p>面向切面编程，或<a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming" target="_blank" rel="external">AOP</a>，是一种编程范式，和面向对象编程（<a href="https://en.wikipedia.org/wiki/Object-oriented_programming" target="_blank" rel="external">OOP</a>）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。</p>
<p>对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，<a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop" target="_blank" rel="external">Spring AOP</a>已经提供了<code>@Configurable</code>注解来帮助我们实现这一需求。</p>
<a id="more"></a>
<h2 id="Configurable注解"><a href="#Configurable注解" class="headerlink" title="Configurable注解"></a>Configurable注解</h2><p>Spring应用程序会定义一个上下文容器，在该容器内创建的对象会由Spring负责注入依赖。对于容器外创建的对象，我们可以使用<code>@Configurable</code>来修饰类，告知Spring对这些类的实例也进行依赖注入。</p>
<p>假设有一个<code>Report</code>类（领域模型），其中一个方法需要解析JSON，我们可以使用<code>@Configurable</code>将容器内的<code>ObjectMapper</code>对象注入到类的实例中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Entity</span></div><div class="line"><span class="meta">@Configurable</span>(autowire = Autowire.BY_TYPE)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Report</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Id</span> <span class="meta">@GeneratedValue</span></div><div class="line">    <span class="keyword">private</span> Integer id;</div><div class="line"></div><div class="line">    <span class="meta">@Autowired</span> <span class="meta">@Transient</span></div><div class="line">    <span class="keyword">private</span> ObjectMapper mapper;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">render</span><span class="params">()</span> </span>&#123;</div><div class="line">        mapper.readValue(...);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>autowire</code>参数默认是<code>NO</code>，因此需要显式打开，否则只能使用XML定义依赖。<code>@Autowired</code>是目前比较推荐的注入方式。</li>
<li><code>@Transient</code>用于告知JPA该属性不需要进行持久化。你也可以使用<code>transient</code>关键字来声明，效果相同。</li>
<li>项目依赖中需要包含<code>spring-aspects</code>。如果已经使用了<code>spring-boot-starter-data-jpa</code>，则无需配置。</li>
<li>应用程序配置中需要加入<code>@EnableSpringConfigured</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SpringBootApplication</span></div><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div><div class="line"><span class="meta">@EnableSpringConfigured</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        SpringApplication.run(Application.class, args);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>在<code>src/main/resources</code>目录下，新建<code>META-INF/aop.xml</code>文件，用来限定哪些包会用到AOP。否则，AOP的织入操作会作用于所有的类（包括第三方类库），产生不必要的的报错信息。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&lt;!DOCTYPE aspectj PUBLIC "-//AspectJ//DTD//EN" "http://www.eclipse.org/aspectj/dtd/aspectj.dtd"&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">aspectj</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">weaver</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">include</span> <span class="attr">within</span>=<span class="string">"com.foobar..*"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">weaver</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">aspectj</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="运行时织入（Load-Time-Weaving-LTW）"><a href="#运行时织入（Load-Time-Weaving-LTW）" class="headerlink" title="运行时织入（Load-Time Weaving, LTW）"></a>运行时织入（Load-Time Weaving, LTW）</h2><p>除了项目依赖和应用程序配置，我们还需要选择一种织入方式来使AOP生效。Spring AOP推荐的方式是运行时织入，并提供了一个专用的Jar包。运行时织入的原理是：当类加载器在读取类文件时，动态修改类的字节码。这一机制是从<a href="http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/instrument/package-summary.html" target="_blank" rel="external">JDK1.5</a>开始提供的，需要使用<code>-javaagent</code>参数开启，如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/spring-instrument.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>在测试时发现，Spring AOP提供的这一Jar包对普通的类是有效果的，但对于使用<code>@Entity</code>修饰的类就没有作用了。因此，我们改用AspectJ提供的Jar包（可到<a href="http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.aspectj%22%20AND%20a%3A%22aspectjweaver%22" target="_blank" rel="external">Maven中央仓库</a>下载）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/aspectjweaver.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>对于<a href="http://projects.spring.io/spring-boot/" target="_blank" rel="external">Spring Boot</a>应用程序，可以在Maven命令中加入以下参数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mvn spring-boot:run -Drun.agent=/path/to/aspectjweaver.jar</div></pre></td></tr></table></figure>
<p>此外，在使用AspectJ作为LTW的提供方后，会影响到Spring的事务管理，因此需要在应用程序配置中加入：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div></pre></td></tr></table></figure>
<h2 id="AnnotationBeanConfigurerAspect"><a href="#AnnotationBeanConfigurerAspect" class="headerlink" title="AnnotationBeanConfigurerAspect"></a>AnnotationBeanConfigurerAspect</h2><p>到这里我们已经通过简单配置完成了领域模型的依赖注入，这背后都是Spring中的<code>AnnotationBeanConfigurerAspect</code>在做工作。我们不妨浏览一下精简后的源码：</p>
<p><a href="https://github.com/spring-projects/spring-framework/blob/master/spring-aspects/src/main/java/org/springframework/beans/factory/aspectj/AnnotationBeanConfigurerAspect.aj" target="_blank" rel="external">AnnotationBeanConfigurerAspect.aj</a></p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">aspect</span> <span class="title">AnnotationBeanConfigurerAspect</span> <span class="keyword">implements</span> <span class="title">BeanFactoryAware</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="keyword">private</span> BeanConfigurerSupport beanConfigurerSupport = <span class="keyword">new</span> BeanConfigurerSupport();</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setBeanFactory</span><span class="params">(BeanFactory beanFactory)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.setBeanFactory(beanFactory);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">configureBean</span><span class="params">(Object bean)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.configureBean(bean);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">inConfigurableBean</span>() : @<span class="keyword">this</span>(Configurable);</div><div class="line"></div><div class="line">	<span class="keyword">declare</span> <span class="keyword">parents</span>: @Configurable * <span class="keyword">implements</span> ConfigurableObject;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">beanConstruction</span>(Object bean) :</div><div class="line">			<span class="keyword">initialization</span>(ConfigurableObject+.<span class="keyword">new</span>(..)) &amp;&amp; <span class="keyword">this</span>(bean);</div><div class="line"></div><div class="line">	<span class="keyword">after</span>(Object bean) <span class="keyword">returning</span> :</div><div class="line">		<span class="title">beanConstruction</span>(bean) &amp;&amp; inConfigurableBean() &#123;</div><div class="line">		configureBean(bean);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>.aj</code>文件是AspectJ定义的语言，增加了pointcut、after等关键字，用来定义切点、通知等；</li>
<li><code>inConfigurationBean</code>切点用于匹配使用<code>Configurable</code>修饰的类型；</li>
<li><code>declare parents</code>将这些类型声明为<code>ConfigurableObject</code>接口，从而匹配<code>beanConstruction</code>切点；</li>
<li><code>ConfigurableObject+.new(..)</code>表示匹配该类型所有的构造函数；</li>
<li><code>after</code>定义一个通知，表示对象创建完成后执行<code>configureBean</code>方法；</li>
<li>该方法会调用<code>BeanConfigurerSupport</code>来对新实例进行依赖注入。</li>
</ul>
<h2 id="其它方案"><a href="#其它方案" class="headerlink" title="其它方案"></a>其它方案</h2><ol>
<li>将依赖作为参数传入。比如上文中的<code>render</code>方法可以定义为<code>render(ObjectMapper mapper)</code>。</li>
<li>将<code>ApplicationContext</code>作为某个类的静态成员，领域模型通过这个引用来获取依赖。</li>
<li>编写一个工厂方法，所有新建对象都要通过这个方法生成，进行依赖注入。</li>
<li>如果领域模型大多从数据库获得，并且JPA的提供方是Hibernate，则可以使用它的拦截器功能进行依赖注入。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable" target="_blank" rel="external">http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable</a></li>
<li><a href="http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html" target="_blank" rel="external">http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html</a></li>
<li><a href="http://jblewitt.com/blog/?p=129" target="_blank" rel="external">http://jblewitt.com/blog/?p=129</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://shzhangji.com/cnblogs/2015/09/05/anemic-domain-model/&quot;&gt;贫血领域模型&lt;/a&gt;这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用&lt;code&gt;new&lt;/code&gt;关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。&lt;/p&gt;
&lt;h2 id=&quot;面向切面编程&quot;&gt;&lt;a href=&quot;#面向切面编程&quot; class=&quot;headerlink&quot; title=&quot;面向切面编程&quot;&gt;&lt;/a&gt;面向切面编程&lt;/h2&gt;&lt;p&gt;面向切面编程，或&lt;a href=&quot;https://en.wikipedia.org/wiki/Aspect-oriented_programming&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AOP&lt;/a&gt;，是一种编程范式，和面向对象编程（&lt;a href=&quot;https://en.wikipedia.org/wiki/Object-oriented_programming&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;OOP&lt;/a&gt;）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。&lt;/p&gt;
&lt;p&gt;对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，&lt;a href=&quot;http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spring AOP&lt;/a&gt;已经提供了&lt;code&gt;@Configurable&lt;/code&gt;注解来帮助我们实现这一需求。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>贫血领域模型</title>
    <link href="http://shzhangji.com/cnblogs/2015/09/05/anemic-domain-model/"/>
    <id>http://shzhangji.com/cnblogs/2015/09/05/anemic-domain-model/</id>
    <published>2015-09-05T11:02:00.000Z</published>
    <updated>2017-03-09T22:24:58.561Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.martinfowler.com/bliki/AnemicDomainModel.html" target="_blank" rel="external">http://www.martinfowler.com/bliki/AnemicDomainModel.html</a></p>
<p>贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为<a href="http://martinfowler.com/eaaCatalog/domainModel.html" target="_blank" rel="external">领域模型</a>的推广者，我们觉得这不是一件好事。</p>
<p>贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。</p>
<p>这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。</p>
<a id="more"></a>
<p>如今，面向对象的概念已经传播得很广泛了，而要反对这种贫血领域模型的做法，我还需要更多论据。贫血领域模型的根本问题在于，它引入了领域模型设计的所有成本，却没有带来任何好处。最主要的成本是将对象映射到数据库中，从而产生了一个对象关系映射层。只有当你充分使用了面向对象设计来组织复杂的业务逻辑后，这一成本才能够被抵消。如果将所有行为都写入到Service对象，那最终你会得到一组<a href="http://martinfowler.com/eaaCatalog/transactionScript.html" target="_blank" rel="external">事务处理脚本</a>，从而错过了领域模型带来的好处。正如我在<a href="http://martinfowler.com/books/eaa.html" target="_blank" rel="external">企业应用架构模式</a>一书中说到的，领域模型并不一定是最好的工具。</p>
<p>还需要强调的是，将行为放入领域模型，这点和分层设计（领域层、持久化层、展现层等）并不冲突。因为领域模型中放入的是和领域相关的逻辑——验证、计算、商业规则等。如果你要讨论能否将数据访问和展现逻辑放入到领域模型中，这就不在本文论述范围之内了。</p>
<p>一些面向对象专家的观点有时会让人产生疑惑，他们认为的确应该有一个面向过程的<a href="http://martinfowler.com/eaaCatalog/serviceLayer.html" target="_blank" rel="external">服务层</a>。但是，这并不意味着领域模型就不应该包含行为。事实上，服务层需要和一组富含行为的领域模型结合起来使用。</p>
<p>Eric Evans的<a href="http://domaindrivendesign.org/books/" target="_blank" rel="external">领域驱动设计</a>一书中有关于分层的论述：</p>
<blockquote>
<p>应用层（也就是上文中的服务层）：用来描述应用程序所要做的工作，并调度丰富的领域模型来完成它。这个层次的任务是描述业务逻辑，或和其它项目的应用层做交互。这个层次很薄，它不包含任何业务规则或知识，仅用于调度和派发任务给下一层的领域模型。这个层次没有业务状态，但可以为用户或程序提供任务状态。</p>
<p>领域层（或者叫模型层）：用于表示业务逻辑、业务场景和规则。这个层次会控制和使用业务状态，即使这些状态最终会交由持久化层来存储。总之，这个层次是软件的核心。</p>
</blockquote>
<p>关键点在于服务层是很薄的——所有重要的业务逻辑都写在领域层。他在服务模式中复述了这一观点：</p>
<blockquote>
<p>如今人们常犯的错误是不愿花时间将业务逻辑放置到合适的领域模型中，从而逐渐形成面向过程的程序设计。</p>
</blockquote>
<p>我不清楚为什么这种反模式会那么常见。我怀疑是因为大多数人并没有使用过一个设计良好的领域模型，特别是那些以数据为中心的开发人员。此外，有些技术也会推动这种反模式，比如J2EE的Entity Bean，这会让我更倾向于使用<a href="http://www.martinfowler.com/bliki/POJO.html" target="_blank" rel="external">POJO</a>领域模型。</p>
<p>总之，如果你将大部分行为都放置在服务层，那么你就会失去领域模型带来的好处。如果你将所有行为都放在服务层，那就无可救药了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.martinfowler.com/bliki/AnemicDomainModel.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.martinfowler.com/bliki/AnemicDomainModel.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为&lt;a href=&quot;http://martinfowler.com/eaaCatalog/domainModel.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;领域模型&lt;/a&gt;的推广者，我们觉得这不是一件好事。&lt;/p&gt;
&lt;p&gt;贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。&lt;/p&gt;
&lt;p&gt;这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/cnblogs/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>HotSpot JVM中的对象指针压缩</title>
    <link href="http://shzhangji.com/cnblogs/2015/06/25/compressed-oops-in-the-hotspot-jvm/"/>
    <id>http://shzhangji.com/cnblogs/2015/06/25/compressed-oops-in-the-hotspot-jvm/</id>
    <published>2015-06-25T09:41:00.000Z</published>
    <updated>2017-03-09T22:24:58.559Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://wiki.openjdk.java.net/display/HotSpot/CompressedOops" target="_blank" rel="external">https://wiki.openjdk.java.net/display/HotSpot/CompressedOops</a></p>
<h2 id="什么是一般对象指针？"><a href="#什么是一般对象指针？" class="headerlink" title="什么是一般对象指针？"></a>什么是一般对象指针？</h2><p>一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。</p>
<p>在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：</p>
<ul>
<li><a href="https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh" target="_blank" rel="external">Self</a>：一门基于原型的语言，是Smalltalk的近亲</li>
<li><a href="http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects" target="_blank" rel="external">Strongtalk</a>：Smalltalk的一种实现</li>
<li><a href="http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp" target="_blank" rel="external">Hotspot</a></li>
<li><a href="http://code.google.com/p/v8/source/browse/trunk/src/objects.h" target="_blank" rel="external">V8</a></li>
</ul>
<p>部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。</p>
<h2 id="为什么需要压缩？"><a href="#为什么需要压缩？" class="headerlink" title="为什么需要压缩？"></a>为什么需要压缩？</h2><p>在<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">LP64</a>系统中，指针需要使用64位来表示；<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">ILP32</a>系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。</p>
<p>在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。</p>
<p>压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。</p>
<p>我们使用<em>解码</em>来表示从32位对象指针转换成64位地址的过程，其反过程则称为<em>编码</em>。</p>
<a id="more"></a>
<h2 id="什么情况下会进行压缩？"><a href="#什么情况下会进行压缩？" class="headerlink" title="什么情况下会进行压缩？"></a>什么情况下会进行压缩？</h2><p>运行在ILP32模式下的Java虚拟机，或在运行时将<code>UseCompressedOops</code>标志位关闭，则所有的对象指针都不会被压缩。</p>
<p>如果<code>UseCompressedOops</code>是打开的，则以下对象的指针会被压缩：</p>
<ul>
<li>所有对象的<a href="http://stackoverflow.com/questions/16721021/what-is-klass-klassklass" target="_blank" rel="external">klass</a>属性</li>
<li>所有<a href="http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/sun/jvm/hotspot/oops/Oop.java#Oop" target="_blank" rel="external">对象指针实例</a>的属性</li>
<li>所有对象指针数组的元素（objArray）</li>
</ul>
<p>HotSpot VM中，用于表示Java类的数据结构是不会压缩的，这部分数据都存放在永久代（PermGen）中。</p>
<p>在解释器中，一般对象指针也是不压缩的，包括JVM本地变量和栈内元素、调用参数、返回值等。解释器会在读取堆内对象时解码对象指针，并在存入时进行编码。</p>
<p>同样，方法调用序列（method calling sequence），无论是解释执行还是编译执行，都不会使用对象指针压缩。</p>
<p>在编译后的代码中，对象指针是否压缩取决于不同的优化结果。优化后的代码可能会将压缩后的对象指针直接从一处搬往另一处，而不进行编解码操作。如果芯片（如x86）支持解码，那在使用对象指针时就不需要自行解码了。</p>
<p>所以，以下数据结构在编译后的代码中既可以是压缩后的对象指针，也可能是本地地址：</p>
<ul>
<li>寄存器或溢出槽（spill slot）中的数据</li>
<li>对象指针映射表（GC映射表）</li>
<li>调试信息</li>
<li>嵌套在机器码中的对象指针（在非RISC芯片中支持，如x86）</li>
<li><a href="http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html#nmethod" target="_blank" rel="external">nmethod</a>常量区（包括那些影响到机器码的重定位操作）</li>
</ul>
<p>在HotSpot JVM的C++代码部分，对象指针压缩与否反映在C++的静态类型系统中。通常情况下，对象指针是不压缩的。具体来说，C++的成员函数在操作本地代码传递过来的指针时（如<em>this</em>），其执行过程不会有什么不同。JVM中的部分方法则提供了重载，能够处理压缩和不压缩的对象指针。</p>
<p>重要的C++数据不会被压缩：</p>
<ul>
<li>C++对象指针（<em>this</em>）</li>
<li>受托管指针的句柄（Handle类型等）</li>
<li>JNI句柄（jobject类型）</li>
</ul>
<p>C++在使用对象指针压缩时（加载和存储等），会以<code>narrowOop</code>作为标记。</p>
<h2 id="使用压缩寻址"><a href="#使用压缩寻址" class="headerlink" title="使用压缩寻址"></a>使用压缩寻址</h2><p>以下是使用对象指针压缩的x86指令示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! int R8; oop[] R9;  // R9是64位</div><div class="line">! oop R10 = R9[R8];  // R10是32位</div><div class="line">! 从原始基址指针加载压缩对象指针：</div><div class="line">movl R10, [R9 + R8&lt;&lt;3 + 16]</div><div class="line">! klassOop R11 = R10._klass;  // R11是32位</div><div class="line">! void* const R12 = GetHeapBase();</div><div class="line">! 从压缩基址指针加载klass指针：</div><div class="line">movl R11, [R12 + R10&lt;&lt;3 + 8]</div></pre></td></tr></table></figure>
<p>以下sparc指令用于解压对象指针（可为空）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! java.lang.Thread::getThreadGroup@1 (line 1072)</div><div class="line">! L1 = L7.group</div><div class="line">ld  [ %l7 + 0x44 ], %l1</div><div class="line">! L3 = decode(L1)</div><div class="line">cmp  %l1, 0</div><div class="line">sllx  %l1, 3, %l3</div><div class="line">brnz,a   %l3, .+8</div><div class="line">add  %l3, %g6, %l3  ! %g6是常量堆基址</div></pre></td></tr></table></figure>
<p><em>输出中的注解来自<a href="https://wiki.openjdk.java.net/display/HotSpot/PrintAssembly" target="_blank" rel="external">PrintAssembly插件</a>。</em></p>
<h2 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h2><p>32位零值会被解压为64位空值，这就需要在解码逻辑中加入一段特殊的逻辑。或者说可以默认某些压缩对象指针肯定不会空（如klass的属性），这样就能使用简单一些的编解码逻辑了。</p>
<p>隐式空值检测对JVM的性能至关重要，包括解释执行和编译执行的字节码。对于一个偏移量较小的对象指针，如果基址指针为空，那很有可能造成系统崩溃，因为虚拟地址空间的前几页通常是没有映射的。</p>
<p>对于压缩对象指针，我们可以用一种类似的技巧来欺骗它：将堆内存前几页的映射去除，如果解压出的指针为空（相对于基址指针），仍可以用它来做加载和存储的操作，隐式空值检测也能照常运行。</p>
<h2 id="对象头信息"><a href="#对象头信息" class="headerlink" title="对象头信息"></a>对象头信息</h2><p>对象头信息通常包含几个部分：固定长度的标志位；klass信息；如果对象是数组，则包含一个32位的信息，并可能追加一个32位的空隙进行对齐；零个或多个实例属性，数组元素，元信息等。（有趣的是，Klass的对象头信息包含了一个C++的<a href="https://en.wikipedia.org/wiki/Virtual_method_table" target="_blank" rel="external">虚拟方法表</a>）</p>
<p>上述追加的32位空隙通常也可用于存储属性信息。</p>
<p>如果<code>UseCompressedOops</code>关闭，标志位和klass都是正常长度。对于数组，32位空隙在LP64系统中总是存在；而ILP32系统中，只有当数组元素是64位数据时才存在这个空隙。</p>
<p>如果<code>UseCompressedOops</code>打开，则klass是32位的。非数组对象在klass后会追加一个空隙，而数组对象则直接开始存储元素信息。</p>
<h2 id="零基压缩技术"><a href="#零基压缩技术" class="headerlink" title="零基压缩技术"></a>零基压缩技术</h2><p>压缩对象指针（narrow-oop）是基于某个地址的偏移量，这个基础地址（narrow-oop-base）是由Java堆内存基址减去一个内存页的大小得来的，从而支持隐式空值检测。所以一个属性字段的地址可以这样得到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3) + &lt;field-offset&gt;.</div></pre></td></tr></table></figure>
<p>如果基础地址可以是0（Java堆内存不一定要从0偏移量开始），那么公式就可以简化为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(&lt;narrow-oop &lt;&lt; 3) + &lt;field-offset&gt;</div></pre></td></tr></table></figure>
<p>理论上说，这一步可以省去一次寄存器上的加和操作。而且使用零基压缩技术后，空值检测也就不需要了。</p>
<p>之前的解压代码是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if (&lt;narrow-oop&gt; == NULL)</div><div class="line">    &lt;wide_oop&gt; = NULL</div><div class="line">else</div><div class="line">    &lt;wide_oop&gt; = &lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3)</div></pre></td></tr></table></figure>
<p>使用零基压缩后，只需使用移位操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;wide_oop&gt; = &lt;narrow-oop&gt; &lt;&lt; 3</div></pre></td></tr></table></figure>
<p>零基压缩技术会根据堆内存的大小以及平台特性来选择不同的策略：</p>
<ol>
<li>堆内存小于4Gb，直接使用压缩对象指针进行寻址，无需压缩和解压；</li>
<li>堆内存大于4Gb，则尝试分配小于32Gb的堆内存，并使用零基压缩技术；</li>
<li>如果仍然失败，则使用普通的对象指针压缩技术，即<code>narrow-oop-base</code>。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;什么是一般对象指针？&quot;&gt;&lt;a href=&quot;#什么是一般对象指针？&quot; class=&quot;headerlink&quot; title=&quot;什么是一般对象指针？&quot;&gt;&lt;/a&gt;什么是一般对象指针？&lt;/h2&gt;&lt;p&gt;一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。&lt;/p&gt;
&lt;p&gt;在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Self&lt;/a&gt;：一门基于原型的语言，是Smalltalk的近亲&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Strongtalk&lt;/a&gt;：Smalltalk的一种实现&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hotspot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/v8/source/browse/trunk/src/objects.h&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;V8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。&lt;/p&gt;
&lt;h2 id=&quot;为什么需要压缩？&quot;&gt;&lt;a href=&quot;#为什么需要压缩？&quot; class=&quot;headerlink&quot; title=&quot;为什么需要压缩？&quot;&gt;&lt;/a&gt;为什么需要压缩？&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;LP64&lt;/a&gt;系统中，指针需要使用64位来表示；&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ILP32&lt;/a&gt;系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。&lt;/p&gt;
&lt;p&gt;在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。&lt;/p&gt;
&lt;p&gt;压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。&lt;/p&gt;
&lt;p&gt;我们使用&lt;em&gt;解码&lt;/em&gt;来表示从32位对象指针转换成64位地址的过程，其反过程则称为&lt;em&gt;编码&lt;/em&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/cnblogs/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>Apache HBase的适用场景</title>
    <link href="http://shzhangji.com/cnblogs/2015/03/08/hbase-dos-and-donts/"/>
    <id>http://shzhangji.com/cnblogs/2015/03/08/hbase-dos-and-donts/</id>
    <published>2015-03-08T00:03:00.000Z</published>
    <updated>2017-03-09T22:24:58.558Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/" target="_blank" rel="external">http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</a></p>
<p>最近我在<a href="http://www.meetup.com/LA-HUG/" target="_blank" rel="external">洛杉矶Hadoop用户组</a>做了一次关于<a href="http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/" target="_blank" rel="external">HBase适用场景</a>的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：</p>
<ul>
<li>HBase很棒，但不是关系型数据库或HDFS的替代者；</li>
<li>配置得当才能运行良好；</li>
<li>监控，监控，监控，重要的事情要说三遍。</li>
</ul>
<p>Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多<a href="#use-cases">成功案例</a>，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。</p>
<a id="more"></a>
<h2 id="何时使用HBase"><a href="#何时使用HBase" class="headerlink" title="何时使用HBase"></a>何时使用HBase</h2><p>虽然HBase是一种绝佳的工具，但我们一定要记住，它并非银弹。HBase并不擅长传统的事务处理程序或关联分析，它也不能完全替代MapReduce过程中使用到的HDFS。从文末的<a href="#use-cases">成功案例</a>中你可以大致了解HBase适用于怎样的应用场景。如果你还有疑问，可以到<a href="http://www.cloudera.com/community/" target="_blank" rel="external">社区</a>中提问，我说过这是一个非常棒的社区。</p>
<p>除去上述限制之外，你为何要选择HBase呢？如果你的应用程序中，数据表每一行的结构是有差别的，那就可以考虑使用HBase，比如在标准化建模的过程中使用它；如果你需要经常追加字段，且大部分字段是NULL值的，那可以考虑HBase；如果你的数据（包括元数据、消息、二进制数据等）都有着同一个主键，那就可以使用HBase；如果你需要通过键来访问和修改数据，使用HBase吧。</p>
<h2 id="后台服务"><a href="#后台服务" class="headerlink" title="后台服务"></a>后台服务</h2><p>如果你已决定尝试一下HBase，那以下是一些部署过程中的提示。HBase会用到一些后台服务，这些服务非常关键。如果你之前没有了解过ZooKeeper，那现在是个好时候。HBase使用ZooKeeper作为它的分布式协调服务，用于选举Master等。随着HBase的发展，ZooKeeper发挥的作用越来越重要。另外，你需要搭建合适的网络基础设施，如NTP和DNS。HBase要求集群内的所有服务器时间一致，并且能正确地访问其它服务器。正确配置NTP和DNS可以杜绝一些奇怪的问题，如服务器A认为当前是明天，B认为当前是昨天；再如Master要求服务器C开启新的Region，而C不知道自己的机器名，从而无法响应。NTP和DNS服务器可以让你减少很多麻烦。</p>
<p>我前面提到过，在考虑是否使用HBase时，需要针对你自己的应用场景来进行判别。而在真正使用HBase时，监控则成了第一要务。和大多数分布式服务一样，HBase服务器宕机会有多米诺骨牌效应。如果一台服务器因内存不足开始swap数据，它会失去和Master的联系，这时Master会命令其他服务器接过这部分请求，可能会导致第二台服务器也发生宕机。所以，你需要密切监控服务器的CPU、I/O以及网络延迟，确保每台HBase服务器都在良好地工作。监控对于维护HBase集群的健康至关重要。</p>
<h2 id="HBase架构最佳实践"><a href="#HBase架构最佳实践" class="headerlink" title="HBase架构最佳实践"></a>HBase架构最佳实践</h2><p>当你找到了适用场景，并搭建起一个健康的HBase集群后，我们来看一些使用过程中的最佳实践。键的前缀要有良好的分布性。如果你使用时间戳或其他类似的递增量作为前缀，那就会让单个Region承载所有请求，而不是分布到各个Region上。此外，你需要根据Memstore和内存的大小来控制Region的数量。RegionServer的JVM堆内存应该控制在12G以内，从而避免过长的GC停顿。举个例子，在一台内存为36G的服务器上部署RegionServer，同时还运行着DataNode，那大约可以提供100个48M大小的Region。这样的配置对HDFS、HBase、以及Linux本身的文件缓存都是有利的。</p>
<p>其他一些设置包括禁用自动合并机制（默认的合并操作会在HBase启动后每隔24小时进行），改为手动的方式在低峰期间执行。你还应该配置数据文件压缩（如LZO），并将正确的配置文件加入HBase的CLASSPATH中。</p>
<h2 id="非适用场景"><a href="#非适用场景" class="headerlink" title="非适用场景"></a>非适用场景</h2><p>上文讲述了HBase的适用场景和最佳实践，以下则是一些需要规避的问题。比如，不要期许HBase可以完全替代关系型数据库——虽然它在许多方面都表现优秀。它不支持SQL，也没有优化器，更不能支持跨越多条记录的事务或关联查询。如果你用不到这些特性，那HBase将是你的不二选择。</p>
<p>在复用HBase的服务器时有一些注意事项。如果你需要保证HBase的服务器质量，同时又想在HBase上运行批处理脚本（如使用Pig从HBase中获取数据进行处理），建议还是另搭一套集群。HBase在处理大量顺序I/O操作时（如MapReduce），其CPU和内存资源将会十分紧张。将这两类应用放置在同一集群上会造成不可预估的服务延迟。此外，共享集群时还需要调低任务槽（task slot）的数量，至少要留一半的CPU核数给HBase。密切关注内存，因为一旦发生swap，HBase很可能会停止心跳，从而被集群判为无效，最终产生一系列宕机。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后要提的一点是，在加载数据到HBase时，应该使用MapReduce+HFileOutputFormat来实现。如果仅使用客户端API，不仅速度慢，也没有充分利用HBase的分布式特性。</p>
<p>用一句话概述，HBase可以让你用键来存储和搜索数据，且无需定义表结构。</p>
<h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a><a id="use-cases"></a>使用案例</h2><ul>
<li>Apache HBase: <a href="http://wiki.apache.org/hadoop/Hbase/PoweredBy" target="_blank" rel="external">Powered By HBase Wiki</a></li>
<li>Mozilla: <a href="http://blog.mozilla.com/webdev/2010/07/26/moving-socorro-to-hbase/" target="_blank" rel="external">Moving Socorro to HBase</a></li>
<li>Facebook: <a href="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html" target="_blank" rel="external">Facebook’s New Real-Time Messaging System: HBase</a></li>
<li>StumbleUpon: <a href="http://www.stumbleupon.com/devblog/hbase_at_stumbleupon/" target="_blank" rel="external">HBase at StumbleUpon</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近我在&lt;a href=&quot;http://www.meetup.com/LA-HUG/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;洛杉矶Hadoop用户组&lt;/a&gt;做了一次关于&lt;a href=&quot;http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;HBase适用场景&lt;/a&gt;的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase很棒，但不是关系型数据库或HDFS的替代者；&lt;/li&gt;
&lt;li&gt;配置得当才能运行良好；&lt;/li&gt;
&lt;li&gt;监控，监控，监控，重要的事情要说三遍。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多&lt;a href=&quot;#use-cases&quot;&gt;成功案例&lt;/a&gt;，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="translation" scheme="http://shzhangji.com/cnblogs/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Reduce-side Join</title>
    <link href="http://shzhangji.com/cnblogs/2015/01/13/understand-reduce-side-join/"/>
    <id>http://shzhangji.com/cnblogs/2015/01/13/understand-reduce-side-join/</id>
    <published>2015-01-13T06:20:00.000Z</published>
    <updated>2017-03-09T22:24:58.557Z</updated>
    
    <content type="html"><![CDATA[<p>在《<a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176" target="_blank" rel="external">MapReduce Design Patterns</a>》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：</p>
<p><img src="/cnblogs/images/reduce-side-join/reduce-side-join.png" alt=""></p>
<ol>
<li>使用<a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html" target="_blank" rel="external">MultipleInputs</a>指定不同的来源表和相应的Mapper类；</li>
<li>Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；</li>
<li>Reducer在接收到同一个Key的记录后，执行以下两步：<ol>
<li>遍历Values，根据标签将来源表的记录分别放到两个List中；</li>
<li>遍历两个List，输出Join结果。</li>
</ol>
</li>
</ol>
<p>具体实现可以参考<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java" target="_blank" rel="external">这段代码</a>。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：</p>
<ol>
<li>Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；</li>
<li>如果前面几张表内存中放不下，就写入磁盘。</li>
</ol>
<a id="more"></a>
<p>按照我们的实现，Mapper输出的Key是<code>product_id</code>，Values是打了标签的产品表（Product）和订单表（Order）的记录。从数据量来看，应该缓存产品表，扫描订单表。这就要求两表记录到达Reducer时是有序的，产品表在前，边扫描边放入内存；订单表在后，边扫描边结合产品表的记录进行输出。要让Hadoop在Shuffle&amp;Sort阶段先按<code>product_id</code>排序、再按表的标签排序，就需要用到二次排序。</p>
<p>二次排序的概念很简单，将Mapper输出的Key由单一的<code>product_id</code>修改为<code>product_id+tag</code>的复合Key就可以了，但需通过以下几步实现：</p>
<h3 id="自定义Key类型"><a href="#自定义Key类型" class="headerlink" title="自定义Key类型"></a>自定义Key类型</h3><p>原来<code>product_id</code>是Text类型，我们的复合Key则要包含<code>product_id</code>和<code>tag</code>两个数据，并实现<code>WritableComparable</code>接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedKey</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TaggedKey</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Text joinKey = <span class="keyword">new</span> Text();</div><div class="line">    <span class="keyword">private</span> IntWritable tag = <span class="keyword">new</span> IntWritable();</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TaggedKey taggedKey)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> compareValue = joinKey.compareTo(taggedKey.getJoinKey());</div><div class="line">        <span class="keyword">if</span> (compareValue == <span class="number">0</span>) &#123;</div><div class="line">            compareValue = tag.compareTo(taggedKey.getTag());</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> compareValue;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// 此处省略部分代码</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到，在比较两个TaggedKey时，会先比较joinKey（即<code>product_id</code>），再比较<code>tag</code>。</p>
<h3 id="自定义分区方法"><a href="#自定义分区方法" class="headerlink" title="自定义分区方法"></a>自定义分区方法</h3><p>默认情况下，Hadoop会对Key进行哈希，以保证相同的Key会分配到同一个Reducer中。由于我们改变了Key的结构，因此需要重新编 写分区函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">TaggedKey</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(TaggedKey taggedKey, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> taggedKey.getJoinKey().hashCode() % numPartitions;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="自定义分组方法"><a href="#自定义分组方法" class="headerlink" title="自定义分组方法"></a>自定义分组方法</h3><p>同理，调用reduce函数需要传入同一个Key的所有记录，这就需要重新定义分组函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TaggedJoiningGroupingComparator</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>(TaggedKey.class, <span class="keyword">true</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</div><div class="line">        TaggedKey taggedKey1 = (TaggedKey) a;</div><div class="line">        TaggedKey taggedKey2 = (TaggedKey) b;</div><div class="line">        <span class="keyword">return</span> taggedKey1.getJoinKey().compareTo(taggedKey2.getJoinKey());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="配置Job"><a href="#配置Job" class="headerlink" title="配置Job"></a>配置Job</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">job.setMapOutputKeyClass(TaggedKey.class);</div><div class="line">job.setMapOutputValueClass(Text.class);</div><div class="line"></div><div class="line">job.setPartitionerClass(TaggedJoiningPartitioner.class);</div><div class="line">job.setGroupingComparatorClass(TaggedJoiningGroupingComparator.class);</div></pre></td></tr></table></figure>
<h3 id="MapReduce过程"><a href="#MapReduce过程" class="headerlink" title="MapReduce过程"></a>MapReduce过程</h3><p>最后，我们在Mapper阶段使用TaggedKey，在Reducer阶段按照tag进行不同的操作就可以了：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(TaggedKey key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">    List&lt;String&gt; products = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (Text value : values) &#123;</div><div class="line">        <span class="keyword">switch</span> (key.getTag().get()) &#123;</div><div class="line">        <span class="keyword">case</span> <span class="number">1</span>: <span class="comment">// Product</span></div><div class="line">            products.add(value.toString());</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="number">2</span>: <span class="comment">// Order</span></div><div class="line">            String[] order = value.toString().split(<span class="string">","</span>);</div><div class="line">            <span class="keyword">for</span> (String productString : products) &#123;</div><div class="line">                String[] product = productString.split(<span class="string">","</span>);</div><div class="line">                List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">                output.add(order[<span class="number">0</span>]);</div><div class="line">                <span class="comment">// ...</span></div><div class="line">                context.write(NullWritable.get(), <span class="keyword">new</span> Text(StringUtils.join(output, <span class="string">","</span>)));</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">default</span>:</div><div class="line">            <span class="keyword">assert</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>遍历values时，开始都是tag=1的记录，之后都是tag=2的记录。以上代码可以<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/ReduceSideJoinJob.java" target="_blank" rel="external">在这里</a>查看。</p>
<p>对于第二个问题，超过缓存大小的记录（默认25000条）就会存入临时文件，由Hive的RowContainer类实现，具体可以看<a href="http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hive/hive-exec/0.10.0-cdh4.5.0/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#RowContainer.add%28java.util.List%29" target="_blank" rel="external">这个链接</a>。</p>
<p>需要注意的是，Hive默认是按SQL中表的书写顺序来决定排序的，因此应该将大表放在最后。如果要人工改变顺序，可以使用STREAMTABLE配置：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(a) */</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key = b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key1)</div></pre></td></tr></table></figure>
<p>但不要将这点和Map-side Join混淆，在配置了<code>hive.auto.convert.join=true</code>后，是不需要注意表的顺序的，Hive会自动将小表缓存在Mapper的内存中。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://codingjunkie.net/mapreduce-reduce-joins/" target="_blank" rel="external">http://codingjunkie.net/mapreduce-reduce-joins/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在《&lt;a href=&quot;http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;MapReduce Design Patterns&lt;/a&gt;》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/reduce-side-join/reduce-side-join.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用&lt;a href=&quot;https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;MultipleInputs&lt;/a&gt;指定不同的来源表和相应的Mapper类；&lt;/li&gt;
&lt;li&gt;Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；&lt;/li&gt;
&lt;li&gt;Reducer在接收到同一个Key的记录后，执行以下两步：&lt;ol&gt;
&lt;li&gt;遍历Values，根据标签将来源表的记录分别放到两个List中；&lt;/li&gt;
&lt;li&gt;遍历两个List，输出Join结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体实现可以参考&lt;a href=&quot;https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这段代码&lt;/a&gt;。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；&lt;/li&gt;
&lt;li&gt;如果前面几张表内存中放不下，就写入磁盘。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="http://shzhangji.com/cnblogs/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://shzhangji.com/cnblogs/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>使用git rebase让历史变得清晰</title>
    <link href="http://shzhangji.com/cnblogs/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <id>http://shzhangji.com/cnblogs/2014/12/23/use-git-rebase-to-clarify-history/</id>
    <published>2014-12-23T08:10:00.000Z</published>
    <updated>2017-03-09T22:24:58.555Z</updated>
    
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>
<p><img src="/cnblogs/images/git-rebase/rebase-result.png" alt=""></p>
<h2 id="“Merge-branch”提交的产生"><a href="#“Merge-branch”提交的产生" class="headerlink" title="“Merge branch”提交的产生"></a>“Merge branch”提交的产生</h2><p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>
<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>
<h2 id="使用git-pull-–rebase"><a href="#使用git-pull-–rebase" class="headerlink" title="使用git pull –rebase"></a>使用git pull –rebase</h2><p>修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>
<a id="more"></a>
<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">nothing to commit, working directory clean</div></pre></td></tr></table></figure>
<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">Untracked files:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</div><div class="line">    test.txt</div><div class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</div></pre></td></tr></table></figure>
<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>
<h2 id="修改git-pull的默认行为"><a href="#修改git-pull的默认行为" class="headerlink" title="修改git pull的默认行为"></a>修改git pull的默认行为</h2><p>每次都加–rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config branch.master.rebase true</div></pre></td></tr></table></figure>
<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config --global branch.autosetuprebase always</div></pre></td></tr></table></figure>
<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>
<h2 id="扩展阅读-1-：git-rebase工作原理"><a href="#扩展阅读-1-：git-rebase工作原理" class="headerlink" title="扩展阅读[1]：git rebase工作原理"></a>扩展阅读[1]：git rebase工作原理</h2><p>先看看git merge的示意图：</p>
<p><img src="/cnblogs/images/git-rebase/merge.png" alt=""></p>
<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches" target="_blank" rel="external">图片来源</a></p>
<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>
<p>再来看git rebase的示意图：</p>
<p><img src="/cnblogs/images/git-rebase/rebase-1.png" alt=""></p>
<p><img src="/cnblogs/images/git-rebase/rebase-2.png" alt=""></p>
<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>
<h2 id="扩展阅读-2-：git-merge-–no-ff"><a href="#扩展阅读-2-：git-merge-–no-ff" class="headerlink" title="扩展阅读[2]：git merge –no-ff"></a>扩展阅读[2]：git merge –no-ff</h2><p>在做项目开发时会用到分支，合并时采用以下步骤：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git checkout feature-branch</div><div class="line">$ git rebase master</div><div class="line">$ git checkout master</div><div class="line">$ git merge --no-ff feature-branch</div><div class="line">$ git push origin master</div></pre></td></tr></table></figure>
<p>历史就成了这样：</p>
<p><img src="/cnblogs/images/git-rebase/no-ff.png" alt=""></p>
<p>可以看到，Merge branch ‘feature-branch’那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/cnblogs/images/git-rebase/rebase-result.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;“Merge-branch”提交的产生&quot;&gt;&lt;a href=&quot;#“Merge-branch”提交的产生&quot; class=&quot;headerlink&quot; title=&quot;“Merge branch”提交的产生&quot;&gt;&lt;/a&gt;“Merge branch”提交的产生&lt;/h2&gt;&lt;p&gt;我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。&lt;/p&gt;
&lt;p&gt;要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。&lt;/p&gt;
&lt;h2 id=&quot;使用git-pull-–rebase&quot;&gt;&lt;a href=&quot;#使用git-pull-–rebase&quot; class=&quot;headerlink&quot; title=&quot;使用git pull –rebase&quot;&gt;&lt;/a&gt;使用git pull –rebase&lt;/h2&gt;&lt;p&gt;修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark快速入门</title>
    <link href="http://shzhangji.com/cnblogs/2014/12/16/spark-quick-start/"/>
    <id>http://shzhangji.com/cnblogs/2014/12/16/spark-quick-start/</id>
    <published>2014-12-16T07:59:00.000Z</published>
    <updated>2017-06-04T11:24:34.855Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt=""></p>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>
<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>
<p>Spark是用<a href="http://www.scala-lang.org/" target="_blank" rel="external">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>
<h2 id="安装Spark和SBT"><a href="#安装Spark和SBT" class="headerlink" title="安装Spark和SBT"></a>安装Spark和SBT</h2><ul>
<li>从<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_HOME=/path/to/spark</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</div></pre></td></tr></table></figure>
<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/" target="_blank" rel="external">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar" target="_blank" rel="external">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SBT_OPTS=<span class="string">"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"</span></div><div class="line">java <span class="variable">$SBT_OPTS</span> -jar `dirname <span class="variable">$0</span>`/sbt-launch.jar <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="日志分析示例"><a href="#日志分析示例" class="headerlink" title="日志分析示例"></a>日志分析示例</h2><p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2014-12-11 18:33:52	INFO	Java	some message</div><div class="line">2014-12-11 18:34:33	INFO	MySQL	some message</div><div class="line">2014-12-11 18:34:54	WARN	Java	some message</div><div class="line">2014-12-11 18:35:25	WARN	Nginx	some message</div><div class="line">2014-12-11 18:36:09	INFO	Java	some message</div></pre></td></tr></table></figure>
<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>
<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ spark-shell</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0</div><div class="line">      /_/</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<h3 id="加载并预览数据"><a href="#加载并预览数据" class="headerlink" title="加载并预览数据"></a>加载并预览数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> lines = sc.textFile(<span class="string">"/tmp/logs.txt"</span>)</div><div class="line">lines: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /tmp/logs.txt <span class="type">MappedRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">12</span></div><div class="line"></div><div class="line">scala&gt; lines.first()</div><div class="line">res0: <span class="type">String</span> = <span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>	<span class="type">INFO</span>	<span class="type">Java</span>	some message</div></pre></td></tr></table></figure>
<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>
<h3 id="解析日志"><a href="#解析日志" class="headerlink" title="解析日志"></a>解析日志</h3><p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> logs = lines.map(line =&gt; line.split(<span class="string">"\t"</span>))</div><div class="line">logs: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">MappedRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">14</span></div><div class="line"></div><div class="line">scala&gt; logs.first()</div><div class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>, <span class="type">INFO</span>, <span class="type">Java</span>, some message)</div></pre></td></tr></table></figure>
<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line =&gt; line.split(“\t”)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>
<h3 id="过滤并计数"><a href="#过滤并计数" class="headerlink" title="过滤并计数"></a>过滤并计数</h3><p>我们想要统计错误日志的数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> errors = logs.filter(log =&gt; log(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">errors: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">FilteredRDD</span>[<span class="number">3</span>] at filter at &lt;console&gt;:<span class="number">16</span></div><div class="line"></div><div class="line">scala&gt; errors.first()</div><div class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message)</div><div class="line"></div><div class="line">scala&gt; errors.count()</div><div class="line">res3: <span class="type">Long</span> = <span class="number">158</span></div></pre></td></tr></table></figure>
<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == “ERROR”表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; errors.cache()</div></pre></td></tr></table></figure>
<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>
<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>
<h3 id="显示前10条记录"><a href="#显示前10条记录" class="headerlink" title="显示前10条记录"></a>显示前10条记录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> firstTenErrors = errors.take(<span class="number">10</span>)</div><div class="line">firstTenErrors: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message), <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>, <span class="type">ERROR</span>, <span class="type">Nginx</span>, some message), ...)</div><div class="line"></div><div class="line">scala&gt; firstTenErrors.map(log =&gt; log.mkString(<span class="string">"\t"</span>)).foreach(line =&gt; println(line))</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>	<span class="type">ERROR</span>	<span class="type">Java</span>	some message</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>	<span class="type">ERROR</span>	<span class="type">Nginx</span>	some message</div><div class="line">...</div></pre></td></tr></table></figure>
<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>
<h3 id="按应用进行统计"><a href="#按应用进行统计" class="headerlink" title="按应用进行统计"></a>按应用进行统计</h3><p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> apps = errors.map(log =&gt; (log(<span class="number">2</span>), <span class="number">1</span>))</div><div class="line">apps: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MappedRDD</span>[<span class="number">15</span>] at map at &lt;console&gt;:<span class="number">18</span></div><div class="line"></div><div class="line">scala&gt; apps.first()</div><div class="line">res20: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">Java</span>,<span class="number">1</span>)</div><div class="line"></div><div class="line">scala&gt; <span class="keyword">val</span> counts = apps.reduceByKey((a, b) =&gt; a + b)</div><div class="line">counts: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">17</span>] at reduceByKey at &lt;console&gt;:<span class="number">20</span></div><div class="line"></div><div class="line">scala&gt; counts.foreach(t =&gt; println(t))</div><div class="line">(<span class="type">Java</span>,<span class="number">58</span>)</div><div class="line">(<span class="type">Nginx</span>,<span class="number">53</span>)</div><div class="line">(<span class="type">MySQL</span>,<span class="number">47</span>)</div></pre></td></tr></table></figure>
<p>errors.map(log =&gt; (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>(&quot;Java&quot;, 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>
<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>).reduce((a, b) =&gt; a + b)</div><div class="line">res23: <span class="type">Int</span> = <span class="number">10</span></div></pre></td></tr></table></figure>
<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>
<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>
<h2 id="打包应用程序"><a href="#打包应用程序" class="headerlink" title="打包应用程序"></a>打包应用程序</h2><p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">spark-sandbox</div><div class="line">├── build.sbt</div><div class="line">├── project</div><div class="line">│   ├── build.properties</div><div class="line">│   └── plugins.sbt</div><div class="line">└── src</div><div class="line">    └── main</div><div class="line">        └── scala</div><div class="line">            └── LogMining.scala</div></pre></td></tr></table></figure>
<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox" target="_blank" rel="external">这个项目</a>作为模板。下面说明一些关键部分：</p>
<h3 id="配置依赖"><a href="#配置依赖" class="headerlink" title="配置依赖"></a>配置依赖</h3><p><code>build.sbt</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"1.1.1"</span></div></pre></td></tr></table></figure>
<h3 id="程序内容"><a href="#程序内容" class="headerlink" title="程序内容"></a>程序内容</h3><p><code>src/main/scala/LogMining.scala</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogMining</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogMining"</span>)</div><div class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">  <span class="keyword">val</span> inputFile = args(<span class="number">0</span>)</div><div class="line">  <span class="keyword">val</span> lines = sc.textFile(inputFile)</div><div class="line">  <span class="comment">// 解析日志</span></div><div class="line">  <span class="keyword">val</span> logs = lines.map(_.split(<span class="string">"\t"</span>))</div><div class="line">  <span class="keyword">val</span> errors = logs.filter(_(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">  <span class="comment">// 缓存错误日志</span></div><div class="line">  errors.cache()</div><div class="line">  <span class="comment">// 统计错误日志记录数</span></div><div class="line">  println(errors.count())</div><div class="line">  <span class="comment">// 获取前10条MySQL的错误日志</span></div><div class="line">  <span class="keyword">val</span> mysqlErrors = errors.filter(_(<span class="number">2</span>) == <span class="string">"MySQL"</span>)</div><div class="line">  mysqlErrors.take(<span class="number">10</span>).map(_ mkString <span class="string">"\t"</span>).foreach(println)</div><div class="line">  <span class="comment">// 统计每个应用的错误日志数</span></div><div class="line">  <span class="keyword">val</span> errorApps = errors.map(_(<span class="number">2</span>) -&gt; <span class="number">1</span>)</div><div class="line">  errorApps.countByKey().foreach(println)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="打包运行"><a href="#打包运行" class="headerlink" title="打包运行"></a>打包运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> spark-sandbox</div><div class="line">$ sbt package</div><div class="line">$ spark-submit --class LogMining --master <span class="built_in">local</span> target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final" target="_blank" rel="external">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals" target="_blank" rel="external">Spark Runtime Internals</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://spark.apache.org/images/spark-logo.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apache Spark&lt;/a&gt;是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通用计算引擎&lt;/strong&gt; 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于内存&lt;/strong&gt; 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与Hadoop集成&lt;/strong&gt; 能够直接读写HDFS中的数据，并能运行在YARN之上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark是用&lt;a href=&quot;http://www.scala-lang.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Scala语言&lt;/a&gt;编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。&lt;/p&gt;
&lt;h2 id=&quot;安装Spark和SBT&quot;&gt;&lt;a href=&quot;#安装Spark和SBT&quot; class=&quot;headerlink&quot; title=&quot;安装Spark和SBT&quot;&gt;&lt;/a&gt;安装Spark和SBT&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;从&lt;a href=&quot;http://spark.apache.org/downloads.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。&lt;/li&gt;
&lt;li&gt;为了方便起见，可以将spark/bin添加到$PATH环境变量中：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; SPARK_HOME=/path/to/spark&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;variable&quot;&gt;$SPARK_HOME&lt;/span&gt;/bin&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;在练习例子时，我们还会用到&lt;a href=&quot;http://www.scala-sbt.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SBT&lt;/a&gt;这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：&lt;ul&gt;
&lt;li&gt;下载&lt;a href=&quot;https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;sbt-launch.jar&lt;/a&gt;到$HOME/bin目录；&lt;/li&gt;
&lt;li&gt;新建$HOME/bin/sbt文件，权限设置为755，内容如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;SBT_OPTS=&lt;span class=&quot;string&quot;&gt;&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;java &lt;span class=&quot;variable&quot;&gt;$SBT_OPTS&lt;/span&gt; -jar `dirname &lt;span class=&quot;variable&quot;&gt;$0&lt;/span&gt;`/sbt-launch.jar &lt;span class=&quot;string&quot;&gt;&quot;&lt;span class=&quot;variable&quot;&gt;$@&lt;/span&gt;&quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/cnblogs/tags/spark/"/>
    
      <category term="scala" scheme="http://shzhangji.com/cnblogs/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>离线环境下构建sbt项目</title>
    <link href="http://shzhangji.com/cnblogs/2014/11/07/sbt-offline/"/>
    <id>http://shzhangji.com/cnblogs/2014/11/07/sbt-offline/</id>
    <published>2014-11-07T07:02:00.000Z</published>
    <updated>2017-03-09T22:24:58.553Z</updated>
    
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/" target="_blank" rel="external">sbt</a>、<a href="http://maven.apache.org/" target="_blank" rel="external">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/" target="_blank" rel="external">Nexus</a>镜像服务，原因有以下几个：</p>
<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>
<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/" target="_blank" rel="external">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/" target="_blank" rel="external">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>
<a id="more"></a>
<h2 id="配置Nexus镜像"><a href="#配置Nexus镜像" class="headerlink" title="配置Nexus镜像"></a>配置Nexus镜像</h2><p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html" target="_blank" rel="external">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>
<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt=""></p>
<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>
<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/" target="_blank" rel="external">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/" target="_blank" rel="external">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/" target="_blank" rel="external">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>
<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>
<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/" target="_blank" rel="external">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/" target="_blank" rel="external">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>
<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>
<h2 id="配置sbt"><a href="#配置sbt" class="headerlink" title="配置sbt"></a>配置sbt</h2><p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[repositories]</div><div class="line">  local</div><div class="line">  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</div><div class="line">  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/</div></pre></td></tr></table></figure>
<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sbt -Dsbt.override.build.repos=<span class="literal">true</span></div></pre></td></tr></table></figure>
<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>
<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在公司网络中使用&lt;a href=&quot;http://www.scala-sbt.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;sbt&lt;/a&gt;、&lt;a href=&quot;http://maven.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Maven&lt;/a&gt;等项目构建工具时，我们通常会搭建一个公用的&lt;a href=&quot;http://www.sonatype.org/nexus/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Nexus&lt;/a&gt;镜像服务，原因有以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免重复下载依赖，节省公司带宽；&lt;/li&gt;
&lt;li&gt;国内网络环境不理想，下载速度慢；&lt;/li&gt;
&lt;li&gt;IDC服务器没有外网访问权限；&lt;/li&gt;
&lt;li&gt;用于发布内部模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;sbt的依赖管理基于&lt;a href=&quot;http://ant.apache.org/ivy/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ivy&lt;/a&gt;，虽然它能直接使用&lt;a href=&quot;http://search.maven.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Maven中央仓库&lt;/a&gt;中的Jar包，在配置时还是有一些注意事项的。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL异常UTF-8字符的处理</title>
    <link href="http://shzhangji.com/cnblogs/2014/10/14/mysql-incorrent-utf8-value/"/>
    <id>http://shzhangji.com/cnblogs/2014/10/14/mysql-incorrent-utf8-value/</id>
    <published>2014-10-14T05:16:00.000Z</published>
    <updated>2017-03-09T22:24:58.552Z</updated>
    
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Incorrect string value: &apos;\xF0\x9D\x8C\x86&apos; for column ...</div></pre></td></tr></table></figure>
<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>
<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> REGEXP_REPLACE(<span class="keyword">content</span>, <span class="string">'[^\\u0000-\\uFFFF]'</span>, <span class="string">''</span>) <span class="keyword">FROM</span> ...</div></pre></td></tr></table></figure>
<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>
<p>以下是一些详细说明和参考资料。</p>
<a id="more"></a>
<h2 id="Unicode字符集和UTF编码"><a href="#Unicode字符集和UTF编码" class="headerlink" title="Unicode字符集和UTF编码"></a>Unicode字符集和UTF编码</h2><p><a href="http://en.wikipedia.org/wiki/Unicode" target="_blank" rel="external">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_\(Unicode\" target="_blank" rel="external">基本多语平面（BMP）</a>#Basic_Multilingual_Plane)和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>
<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set" target="_blank" rel="external">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>
<p>比如，磁盘上记录了以下二进制数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">1101000 1100101 1101100 1101100 1101111</div></pre></td></tr></table></figure>
<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">104 101 108 108 111</div></pre></td></tr></table></figure>
<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>
<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>
<h2 id="UTF-8编码"><a href="#UTF-8编码" class="headerlink" title="UTF-8编码"></a>UTF-8编码</h2><p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="external">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>
<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>
<h2 id="MySQL的UTF-8编码"><a href="#MySQL的UTF-8编码" class="headerlink" title="MySQL的UTF-8编码"></a>MySQL的UTF-8编码</h2><p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html" target="_blank" rel="external">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>
<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8" target="_blank" rel="external">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html" target="_blank" rel="external">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode" target="_blank" rel="external">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Incorrect string value: &amp;apos;\xF0\x9D\x8C\x86&amp;apos; for column ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。&lt;/p&gt;
&lt;p&gt;根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; REGEXP_REPLACE(&lt;span class=&quot;keyword&quot;&gt;content&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;[^\\u0000-\\uFFFF]&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。&lt;/p&gt;
&lt;p&gt;以下是一些详细说明和参考资料。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>在CDH 4.5上安装Shark 0.9</title>
    <link href="http://shzhangji.com/cnblogs/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <id>http://shzhangji.com/cnblogs/2014/07/05/deploy-shark-0.9-with-cdh-4.5/</id>
    <published>2014-07-05T09:16:00.000Z</published>
    <updated>2017-03-09T22:24:58.551Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org" target="_blank" rel="external">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org" target="_blank" rel="external">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org" target="_blank" rel="external">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/" target="_blank" rel="external">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/" target="_blank" rel="external">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/" target="_blank" rel="external">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">STANDALONE_SPARK_MASTER_HOST=one-843</div><div class="line">DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh</div><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh</div></pre></td></tr></table></figure>
<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(<span class="string">"hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo"</span>).count</div></pre></td></tr></table></figure>
<h2 id="安装Shark"><a href="#安装Shark" class="headerlink" title="安装Shark"></a>安装Shark</h2><ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz" target="_blank" rel="external">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45</div><div class="line"><span class="built_in">export</span> SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</div><div class="line"></div><div class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/etc/hive/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> MASTER=spark://one-843:7077</div><div class="line"></div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[program:sharkserver2]</div><div class="line">command = /opt/shark/bin/shark --service sharkserver2</div><div class="line">autostart = true</div><div class="line">autorestart = true</div><div class="line">stdout_logfile = /var/log/sharkserver2.log</div><div class="line">redirect_stderr = true</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ supervisorctl start sharkserver2</div></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root</div></pre></td></tr></table></figure>
<h2 id="版本问题"><a href="#版本问题" class="headerlink" title="版本问题"></a>版本问题</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h4><p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html" target="_blank" rel="external">CDH4.5</a>的部分对应关系如下：</p>
<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>
<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html" target="_blank" rel="external">Metastore Server</a>使用的也是0.10.0版本的API。</p>
<h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h4><p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>
<h4 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h4><p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>
<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive" target="_blank" rel="external">AMPLab patched Hive</a>：</p>
<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>
<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>
<h4 id="Shark与Hive的并存"><a href="#Shark与Hive的并存" class="headerlink" title="Shark与Hive的并存"></a>Shark与Hive的并存</h4><p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>
<h3 id="目前发现的不兼容SQL"><a href="#目前发现的不兼容SQL" class="headerlink" title="目前发现的不兼容SQL"></a>目前发现的不兼容SQL</h3><ul>
<li>DROP TABLE …</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: &apos;drop_table_with_environment_context&apos;</div></pre></td></tr></table></figure>
<ul>
<li>INSERT OVERWRITE TABLE … PARTITION (…) SELECT …</li>
<li>LOAD DATA INPATH ‘…’ OVERWRITE INTO TABLE … PARTITION (…)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Failed with exception org.apache.thrift.TApplicationException: Invalid method name: &apos;partition_name_has_valid_characters&apos;</div></pre></td></tr></table></figure>
<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="对存在问题的SQL使用Hive命令去调"><a href="#对存在问题的SQL使用Hive命令去调" class="headerlink" title="对存在问题的SQL使用Hive命令去调"></a>对存在问题的SQL使用Hive命令去调</h4><p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>
<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>
<h4 id="升级到CDH5"><a href="#升级到CDH5" class="headerlink" title="升级到CDH5"></a>升级到CDH5</h4><p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>
<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc" target="_blank" rel="external">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>
<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spark&lt;/a&gt;是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的&lt;a href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hadoop&lt;/a&gt;生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于&lt;a href=&quot;http://hive.apache.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hive&lt;/a&gt;，Spark也有相应的替代项目——&lt;a href=&quot;http://shark.cs.berkeley.edu/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Shark&lt;/a&gt;，能做到 &lt;strong&gt;drop-in replacement&lt;/strong&gt; ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。&lt;/p&gt;
&lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动&lt;/li&gt;
&lt;li&gt;软件版本&lt;ul&gt;
&lt;li&gt;Cloudera Manager 4.8.2&lt;/li&gt;
&lt;li&gt;CDH 4.5&lt;/li&gt;
&lt;li&gt;Spark 0.9.0 Parcel&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cloudera.rst.im/shark/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Shark 0.9.1 Binary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;服务器基础配置&lt;ul&gt;
&lt;li&gt;可用的软件源（如&lt;a href=&quot;http://mirrors.ustc.edu.cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;中科大的源&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;配置主节点至子节点的root账户SSH无密码登录。&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;/etc/hosts&lt;/code&gt;中写死IP和主机名，或者DNS做好正反解析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive小文件问题的处理</title>
    <link href="http://shzhangji.com/cnblogs/2014/04/07/hive-small-files/"/>
    <id>http://shzhangji.com/cnblogs/2014/04/07/hive-small-files/</id>
    <published>2014-04-07T09:09:00.000Z</published>
    <updated>2017-03-09T22:24:58.549Z</updated>
    
    <content type="html"><![CDATA[<p>Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。</p>
<h2 id="小文件带来的问题"><a href="#小文件带来的问题" class="headerlink" title="小文件带来的问题"></a>小文件带来的问题</h2><p>关于这个问题的阐述可以读一读Cloudera的<a href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/" target="_blank" rel="external">这篇文章</a>。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>
<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。</p>
<a id="more"></a>
<h2 id="Hive小文件产生的原因"><a href="#Hive小文件产生的原因" class="headerlink" title="Hive小文件产生的原因"></a>Hive小文件产生的原因</h2><p>前面已经提到，汇总后的数据量通常比源数据要少得多。而为了提升运算速度，我们会增加Reducer的数量，Hive本身也会做类似优化——Reducer数量等于源数据的量除以hive.exec.reducers.bytes.per.reducer所配置的量（默认1G）。Reducer数量的增加也即意味着结果文件的增加，从而产生小文件的问题。</p>
<h2 id="配置Hive结果合并"><a href="#配置Hive结果合并" class="headerlink" title="配置Hive结果合并"></a>配置Hive结果合并</h2><p>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</p>
<ul>
<li><code>hive.merge.mapfiles</code> 在map-only job后合并文件，默认<code>true</code></li>
<li><code>hive.merge.mapredfiles</code> 在map-reduce job后合并文件，默认<code>false</code></li>
<li><code>hive.merge.size.per.task</code> 合并后每个文件的大小，默认<code>256000000</code></li>
<li><code>hive.merge.smallfiles.avgsize</code> 平均文件大小，是决定是否执行合并操作的阈值，默认<code>16000000</code></li>
</ul>
<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>
<ol>
<li>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</li>
<li>结果文件的平均大小需要大于avgsize参数的值。</li>
</ol>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">-- map-red job，5个reducer，产生5个60K的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，一个mapper，产生一个300K的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- map-only job，45个mapper，产生45个25M左右的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，4个mapper，产生4个250M左右的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h3 id="压缩文件的处理"><a href="#压缩文件的处理" class="headerlink" title="压缩文件的处理"></a>压缩文件的处理</h3><p>如果结果表使用了压缩格式，则必须配合SequenceFile来存储，否则无法进行合并，以下是示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span> mapred.output.compression.type=<span class="keyword">BLOCK</span>;</div><div class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</div><div class="line"><span class="keyword">set</span> mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec;</div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"></div><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> dw_stage.zj_small;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small</div><div class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE</div><div class="line"><span class="keyword">as</span> <span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h2 id="使用HAR归档文件"><a href="#使用HAR归档文件" class="headerlink" title="使用HAR归档文件"></a>使用HAR归档文件</h2><p>Hadoop的<a href="http://hadoop.apache.org/docs/stable1/hadoop_archives.html" target="_blank" rel="external">归档文件</a>格式也是解决小文件问题的方式之一。而且Hive提供了<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving" target="_blank" rel="external">原生支持</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">set hive.archive.enabled=true;</div><div class="line">set hive.archive.har.parentdir.settable=true;</div><div class="line">set har.partfile.size=1099511627776;</div><div class="line"></div><div class="line">ALTER TABLE srcpart ARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div><div class="line"></div><div class="line">ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div></pre></td></tr></table></figure>
<p>如果使用的不是分区表，则可创建成外部表，并使用<code>har://</code>协议来指定路径。</p>
<h2 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h2><p>Hadoop V2引入了HDFS Federation的概念：</p>
<p><img src="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/images/federation.gif" alt=""></p>
<p>实则是将NameNode做了拆分，从而增强了它的扩展性，小文件的问题也能够得到缓解。</p>
<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><p>对于通常的应用，使用Hive结果合并就能达到很好的效果。如果不想因此增加运行时间，可以自行编写一些脚本，在系统空闲时对分区内的文件进行合并，也能达到目的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。&lt;/p&gt;
&lt;h2 id=&quot;小文件带来的问题&quot;&gt;&lt;a href=&quot;#小文件带来的问题&quot; class=&quot;headerlink&quot; title=&quot;小文件带来的问题&quot;&gt;&lt;/a&gt;小文件带来的问题&lt;/h2&gt;&lt;p&gt;关于这个问题的阐述可以读一读Cloudera的&lt;a href=&quot;http://blog.cloudera.com/blog/2009/02/the-small-files-problem/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这篇文章&lt;/a&gt;。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。&lt;/p&gt;
&lt;p&gt;此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/cnblogs/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/cnblogs/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
</feed>
