<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark Streaming 中如何实现 Exactly-Once 语义 | 张吉的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实">
<meta name="keywords" content="scala,spark,stream processing,spark streaming,kafka">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Streaming 中如何实现 Exactly-Once 语义">
<meta property="og:url" content="http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/index.html">
<meta property="og:site_name" content="张吉的博客">
<meta property="og:description" content="Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png">
<meta property="og:updated_time" content="2017-08-05T00:47:06.615Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Streaming 中如何实现 Exactly-Once 语义">
<meta name="twitter:description" content="Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实">
<meta name="twitter:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/cnblogs/atom.xml" title="张吉的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link rel="stylesheet" href="/cnblogs/css/source-code-pro.css">
  
  <link rel="stylesheet" href="/cnblogs/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-37223379-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/cnblogs/" id="logo">张吉的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/cnblogs/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/cnblogs/">首页</a>
        
          <a class="main-nav-link" href="/cnblogs/categories/Big-Data">大数据</a>
        
          <a class="main-nav-link" href="/cnblogs/categories/Programming">编程</a>
        
          <a class="main-nav-link" href="/cnblogs/categories/Digest">摘译</a>
        
          <a class="main-nav-link" href="/cnblogs/archives">全部文章</a>
        
          <a class="main-nav-link" href="http://shzhangji.com/">English</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/cnblogs/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shzhangji.com/cnblogs"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-how-to-achieve-exactly-once-semantics-in-spark-streaming" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/" class="article-date">
  <time datetime="2017-08-01T04:54:47.000Z" itemprop="datePublished">2017-08-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/cnblogs/categories/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark Streaming 中如何实现 Exactly-Once 语义
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Exactly-once 语义是实时计算的难点之一。要做到每一条记录只会被处理一次，即使服务器或网络发生故障时也能保证没有遗漏，这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。此外，我们在编写计算流程时也需要遵循一定规范，才能真正实现 Exactly-once。本文将讲述如何结合 Spark Streaming 框架、Kafka 消息系统、以及 MySQL 数据库来实现 Exactly-once 的实时计算流程。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Spark Streaming"></p>
<h2 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h2><p>首先让我们实现一个简单而完整的实时计算流程。我们从 Kafka 接收用户访问日志，解析并提取其中的时间和日志级别，并统计每分钟错误日志的数量，结果保存到 MySQL 中。</p>
<p>示例日志:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017-07-30 14:09:08 ERROR some message</span><br><span class="line">2017-07-30 14:09:20 INFO  some message</span><br><span class="line">2017-07-30 14:10:50 ERROR some message</span><br></pre></td></tr></table></figure>
<p>结果表结构，其中 <code>log_time</code> 字段会截取到分钟级别：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_log (</span><br><span class="line">  log_time datetime primary <span class="keyword">key</span>,</span><br><span class="line">  log_count <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="number">0</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>Scala 项目通常使用 <code>sbt</code> 来管理。我们将下列依赖添加到 <code>build.sbt</code> 文件中。本例使用的是 Spark 2.2 和 Kafka 0.10，数据库操作类库使用了 ScalikeJDBC 3.0。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scalaVersion := <span class="string">"2.11.11"</span></span><br><span class="line"></span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(</span><br><span class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming"</span> % <span class="string">"2.2.0"</span> % <span class="string">"provided"</span>,</span><br><span class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming-kafka-0-10"</span> % <span class="string">"2.2.0"</span>,</span><br><span class="line">  <span class="string">"org.scalikejdbc"</span> %% <span class="string">"scalikejdbc"</span> % <span class="string">"3.0.1"</span>,</span><br><span class="line">  <span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.43"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>完整的示例代码已上传至 GitHub（<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/ExactlyOnce.scala" target="_blank" rel="noopener">链接</a>），下面我仅选取重要的部分加以说明：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化数据库连接</span></span><br><span class="line"><span class="type">ConnectionPool</span>.singleton(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"root"</span>, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 Spark Streaming 上下文</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ExactlyOnce"</span>).setIfMissing(<span class="string">"spark.master"</span>, <span class="string">"local[2]"</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 Kafka Direct API 创建 DStream</span></span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Seq</span>(<span class="string">"alog"</span>), kafkaParams))</span><br><span class="line"></span><br><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="comment">// 日志处理</span></span><br><span class="line">  <span class="keyword">val</span> result = rdd.map(_.value)</span><br><span class="line">    .flatMap(parseLog) <span class="comment">// 日志解析函数</span></span><br><span class="line">    .filter(_.level == <span class="string">"ERROR"</span>)</span><br><span class="line">    .map(log =&gt; log.time.truncatedTo(<span class="type">ChronoUnit</span>.<span class="type">MINUTES</span>) -&gt; <span class="number">1</span>)</span><br><span class="line">    .reduceByKey(_ + _)</span><br><span class="line">    .collect()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 结果保存至数据库</span></span><br><span class="line">  <span class="type">DB</span>.autoCommit &#123; <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</span><br><span class="line">      <span class="string">sql""</span><span class="string">"</span></span><br><span class="line"><span class="string">      insert into error_log (log_time, log_count)</span></span><br><span class="line"><span class="string">      value ($&#123;time&#125;, $&#123;count&#125;)</span></span><br><span class="line"><span class="string">      on duplicate key update log_count = log_count + values(log_count)</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="实时计算语义"><a href="#实时计算语义" class="headerlink" title="实时计算语义"></a>实时计算语义</h2><p>实时计算有三种语义，分别是 At-most-once、At-least-once、以及 Exactly-once。一个典型的 Spark Streaming 应用程序会包含三个处理阶段：接收数据、处理汇总、输出结果。每个阶段都需要做不同的处理才能实现相应的语义。</p>
<p>对于 <strong>接收数据</strong>，主要取决于上游数据源的特性。例如，从 HDFS 这类支持容错的文件系统中读取文件，能够直接支持 Exactly-once 语义。如果上游消息系统支持 ACK（如RabbitMQ），我们就可以结合 Spark 的 Write Ahead Log 特性来实现 At-least-once 语义。对于非可靠的数据接收器（如 <code>socketTextStream</code>），当 Worker 或 Driver 节点发生故障时就会产生数据丢失，提供的语义也是未知的。而 Kafka 消息系统是基于偏移量（Offset）的，它的 Direct API 可以提供 Exactly-once 语义。</p>
<p>在使用 Spark RDD 对数据进行 <strong>转换或汇总</strong> 时，我们可以天然获得 Exactly-once 语义，因为 RDD 本身就是一种具备容错性、不变性、以及计算确定性的数据结构。只要数据来源是可用的，且处理过程中没有副作用（Side effect），我们就能一直得到相同的计算结果。</p>
<p><strong>结果输出</strong> 默认符合 At-least-once 语义，因为 <code>foreachRDD</code> 方法可能会因为 Worker 节点失效而执行多次，从而重复写入外部存储。我们有两种方式解决这一问题，幂等更新和事务更新。下面我们将深入探讨这两种方式。</p>
<h2 id="使用幂等写入实现-Exactly-once"><a href="#使用幂等写入实现-Exactly-once" class="headerlink" title="使用幂等写入实现 Exactly-once"></a>使用幂等写入实现 Exactly-once</h2><p>如果多次写入会产生相同的结果数据，我们可以认为这类写入操作是幂等的。<code>saveAsTextFile</code> 就是一种典型的幂等写入。如果消息中包含唯一主键，那么多次写入相同的数据也不会在数据库中产生重复记录。这种方式也就能等价于 Exactly-once 语义了。但需要注意的是，幂等写入只适用于 Map-only 型的计算流程，即没有 Shuffle、Reduce、Repartition 等操作。此外，我们还需对 Kafka DStream 做一些额外设置：</p>
<ul>
<li>将 <code>enable.auto.commit</code> 设置为 <code>false</code>。默认情况下，Kafka DStream 会在接收到数据后立刻更新自己的偏移量，我们需要将这个动作推迟到计算完成之后。</li>
<li>打开 Spark Streaming 的 Checkpoint 特性，用于存放 Kafka 偏移量。但若应用程序代码发生变化，Checkpoint 数据也将无法使用，这就需要改用下面的操作：</li>
<li>在数据输出之后手动提交 Kafka 偏移量。<code>HasOffsetRanges</code> 类，以及 <code>commitAsync</code> API 可以做到这一点：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    <span class="comment">// output to database</span></span><br><span class="line">  &#125;</span><br><span class="line">  messages.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="使用事务写入实现-Exactly-once"><a href="#使用事务写入实现-Exactly-once" class="headerlink" title="使用事务写入实现 Exactly-once"></a>使用事务写入实现 Exactly-once</h2><p>在使用事务型写入时，我们需要生成一个唯一 ID，这个 ID 可以使用当前批次的时间、分区号、或是 Kafka 偏移量来生成。之后，我们需要在一个事务中将处理结果和这个唯一 ID 一同写入数据库。这一原子性的操作将带给我们 Exactly-once 语义，而且该方法可以同时适用于 Map-only 以及包含汇聚操作的计算流程。</p>
<p>我们通常会在 <code>foreachPartition</code> 方法中来执行数据库写入操作。对于 Map-only 流程来说是适用的，因为这种流程下 Kafka 分区和 RDD 分区是一一对应的，我们可以用以下方式获取各分区的偏移量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">val</span> offsetRange = offsetRanges(<span class="type">TaskContext</span>.get.partitionId)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但对于包含 Shuffle 的计算流程（如上文的错误日志统计），我们需要先将处理结果拉取到 Driver 进程中，然后才能执行事务操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  <span class="keyword">val</span> result = processLogs(rdd).collect() <span class="comment">// parse log and count error</span></span><br><span class="line">  <span class="type">DB</span>.localTx &#123; <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</span><br><span class="line">      <span class="comment">// save to error_log table</span></span><br><span class="line">    &#125;</span><br><span class="line">    offsetRanges.foreach &#123; offsetRange =&gt;</span><br><span class="line">      <span class="keyword">val</span> affectedRows = <span class="string">sql""</span><span class="string">"</span></span><br><span class="line"><span class="string">      update kafka_offset set offset = $&#123;offsetRange.untilOffset&#125;</span></span><br><span class="line"><span class="string">      where topic = $&#123;topic&#125; and `partition` = $&#123;offsetRange.partition&#125;</span></span><br><span class="line"><span class="string">      and offset = $&#123;offsetRange.fromOffset&#125;</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (affectedRows != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"fail to update offset"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果偏移量写入失败，或者重复处理了某一部分数据（<code>offset != $fromOffset</code> 判断条件不通过），该事务就会回滚，从而做到 Exactly-once。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实时计算中的 Exactly-once 是比较强的一种语义，因而会给你的应用程序引入额外的开销。此外，它尚不能很好地支持<a href="https://github.com/koeninger/kafka-exactly-once/blob/master/src/main/scala/example/Windowed.scala" target="_blank" rel="noopener">窗口型</a>操作。因此，是否要在代码中使用这一语义就需要开发者自行判断了。很多情况下，数据丢失或重复处理并不那么重要。不过，了解 Exactly-once 的开发流程还是有必要的，对学习 Spark Streaming 也会有所助益。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/" target="_blank" rel="noopener">http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</a></li>
<li><a href="http://kafka.apache.org/documentation.html#semantics" target="_blank" rel="noopener">http://kafka.apache.org/documentation.html#semantics</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/" data-id="cjmu5v8qd0030n8r4cgiueoj4" class="article-share-link">分享</a>
      
        <a href="http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/#disqus_thread" class="article-comment-link">留言</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/cnblogs/tags/kafka/">kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/cnblogs/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/cnblogs/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/cnblogs/tags/spark-streaming/">spark streaming</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/cnblogs/tags/stream-processing/">stream processing</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/cnblogs/2017/08/06/how-to-extract-event-time-in-apache-flume/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">
        
          Apache Flume 如何解析消息中的事件时间
        
      </div>
    </a>
  
  
    <a href="/cnblogs/2017/07/23/learn-pandas-from-a-sql-perspective/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">通过 SQL 查询学习 Pandas 数据处理</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Python数据平台</h3>
    <div class="widget">
      <img src="/cnblogs/images/pydp-qrcode.jpg" style="width: 100%;"></img>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/cnblogs/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/cnblogs/tags/analytics/" style="font-size: 15.71px;">analytics</a> <a href="/cnblogs/tags/angular/" style="font-size: 10px;">angular</a> <a href="/cnblogs/tags/aop/" style="font-size: 10px;">aop</a> <a href="/cnblogs/tags/aosa/" style="font-size: 11.43px;">aosa</a> <a href="/cnblogs/tags/apache-beam/" style="font-size: 10px;">apache beam</a> <a href="/cnblogs/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/cnblogs/tags/c/" style="font-size: 10px;">c</a> <a href="/cnblogs/tags/canal/" style="font-size: 10px;">canal</a> <a href="/cnblogs/tags/cdh/" style="font-size: 10px;">cdh</a> <a href="/cnblogs/tags/clojure/" style="font-size: 17.14px;">clojure</a> <a href="/cnblogs/tags/crossfilter/" style="font-size: 10px;">crossfilter</a> <a href="/cnblogs/tags/data-science/" style="font-size: 10px;">data science</a> <a href="/cnblogs/tags/dc-js/" style="font-size: 10px;">dc.js</a> <a href="/cnblogs/tags/docker/" style="font-size: 10px;">docker</a> <a href="/cnblogs/tags/druid/" style="font-size: 10px;">druid</a> <a href="/cnblogs/tags/eclipse/" style="font-size: 10px;">eclipse</a> <a href="/cnblogs/tags/es6/" style="font-size: 10px;">es6</a> <a href="/cnblogs/tags/eslint/" style="font-size: 10px;">eslint</a> <a href="/cnblogs/tags/etl/" style="font-size: 12.86px;">etl</a> <a href="/cnblogs/tags/flume/" style="font-size: 12.86px;">flume</a> <a href="/cnblogs/tags/frontend/" style="font-size: 12.86px;">frontend</a> <a href="/cnblogs/tags/functional-programming/" style="font-size: 10px;">functional programming</a> <a href="/cnblogs/tags/git/" style="font-size: 11.43px;">git</a> <a href="/cnblogs/tags/hadoop/" style="font-size: 12.86px;">hadoop</a> <a href="/cnblogs/tags/hbase/" style="font-size: 10px;">hbase</a> <a href="/cnblogs/tags/hdfs/" style="font-size: 10px;">hdfs</a> <a href="/cnblogs/tags/hive/" style="font-size: 14.29px;">hive</a> <a href="/cnblogs/tags/java/" style="font-size: 17.14px;">java</a> <a href="/cnblogs/tags/javascript/" style="font-size: 15.71px;">javascript</a> <a href="/cnblogs/tags/jvm/" style="font-size: 10px;">jvm</a> <a href="/cnblogs/tags/kafka/" style="font-size: 10px;">kafka</a> <a href="/cnblogs/tags/lodash/" style="font-size: 10px;">lodash</a> <a href="/cnblogs/tags/machine-learning/" style="font-size: 11.43px;">machine learning</a> <a href="/cnblogs/tags/mapreduce/" style="font-size: 11.43px;">mapreduce</a> <a href="/cnblogs/tags/mysql/" style="font-size: 11.43px;">mysql</a> <a href="/cnblogs/tags/nginx/" style="font-size: 10px;">nginx</a> <a href="/cnblogs/tags/noir/" style="font-size: 12.86px;">noir</a> <a href="/cnblogs/tags/opensource/" style="font-size: 10px;">opensource</a> <a href="/cnblogs/tags/ops/" style="font-size: 11.43px;">ops</a> <a href="/cnblogs/tags/pandas/" style="font-size: 11.43px;">pandas</a> <a href="/cnblogs/tags/perl/" style="font-size: 11.43px;">perl</a> <a href="/cnblogs/tags/python/" style="font-size: 18.57px;">python</a> <a href="/cnblogs/tags/react/" style="font-size: 10px;">react</a> <a href="/cnblogs/tags/restful/" style="font-size: 10px;">restful</a> <a href="/cnblogs/tags/scala/" style="font-size: 12.86px;">scala</a> <a href="/cnblogs/tags/source-code/" style="font-size: 10px;">source code</a> <a href="/cnblogs/tags/spark/" style="font-size: 15.71px;">spark</a> <a href="/cnblogs/tags/spark-streaming/" style="font-size: 10px;">spark streaming</a> <a href="/cnblogs/tags/spring/" style="font-size: 12.86px;">spring</a> <a href="/cnblogs/tags/sql/" style="font-size: 11.43px;">sql</a> <a href="/cnblogs/tags/storm/" style="font-size: 10px;">storm</a> <a href="/cnblogs/tags/stream-processing/" style="font-size: 14.29px;">stream processing</a> <a href="/cnblogs/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/cnblogs/tags/thrift/" style="font-size: 10px;">thrift</a> <a href="/cnblogs/tags/translation/" style="font-size: 20px;">translation</a> <a href="/cnblogs/tags/tutorial/" style="font-size: 17.14px;">tutorial</a> <a href="/cnblogs/tags/unix/" style="font-size: 10px;">unix</a> <a href="/cnblogs/tags/vue/" style="font-size: 10px;">vue</a> <a href="/cnblogs/tags/vuex/" style="font-size: 10px;">vuex</a> <a href="/cnblogs/tags/websocket/" style="font-size: 10px;">websocket</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2018/09/">九月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2018/05/">五月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2017/10/">十月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2017/09/">九月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2016/03/">三月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/09/">九月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/06/">六月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/03/">三月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/01/">一月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/12/">十二月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/11/">十一月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/10/">十月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/07/">七月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/04/">四月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/01/">一月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/12/">十二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/09/">九月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/06/">六月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/05/">五月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/04/">四月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/03/">三月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/02/">二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/01/">一月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2012/12/">十二月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2012/11/">十一月 2012</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/cnblogs/2018/10/04/flume-source-code-hdfs-sink/">Flume 源码解析：HDFS Sink</a>
          </li>
        
          <li>
            <a href="/cnblogs/2018/09/22/how-to-avoid-null-pointer-exception/">Java 空指针异常的若干解决方案</a>
          </li>
        
          <li>
            <a href="/cnblogs/2018/09/14/is-it-necessary-to-apply-eslint-jsx-no-bind-rule/">是否需要使用 ESLint jsx-no-bind 规则？</a>
          </li>
        
          <li>
            <a href="/cnblogs/2018/05/14/serve-tensorflow-estimator-with-savedmodel/">TensorFlow 模型如何对外提供服务</a>
          </li>
        
          <li>
            <a href="/cnblogs/2018/04/22/connect-hbase-with-python-and-thrift/">使用 Python 和 Thrift 连接 HBase</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><img alt="知识共享许可协议" style="border-width:0" src="https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-nc-sa.svg"></a>
      <br>
      &copy; 2018 张吉<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/cnblogs/" class="mobile-nav-link">首页</a>
  
    <a href="/cnblogs/categories/Big-Data" class="mobile-nav-link">大数据</a>
  
    <a href="/cnblogs/categories/Programming" class="mobile-nav-link">编程</a>
  
    <a href="/cnblogs/categories/Digest" class="mobile-nav-link">摘译</a>
  
    <a href="/cnblogs/archives" class="mobile-nav-link">全部文章</a>
  
    <a href="http://shzhangji.com/" class="mobile-nav-link">English</a>
  
</nav>
    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/cnblogs/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/cnblogs/fancybox/jquery.fancybox.css">
  <script src="/cnblogs/fancybox/jquery.fancybox.pack.js"></script>


<script src="/cnblogs/js/script.js"></script>

  </div>
</body>
</html>