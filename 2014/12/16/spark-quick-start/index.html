<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark快速入门 | 张吉的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Apache Spark是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：

通用计算引擎 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；
基于内存 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；
与Hadoop集成 能够直接读写HDFS中的数据，并能运行在YARN之上。

Spark是用Scala语言编写的，所提供的API也很好地利用了这门语言的">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark快速入门">
<meta property="og:url" content="http://shzhangji.com/2014/12/16/spark-quick-start/index.html">
<meta property="og:site_name" content="张吉的博客">
<meta property="og:description" content="Apache Spark是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：

通用计算引擎 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；
基于内存 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；
与Hadoop集成 能够直接读写HDFS中的数据，并能运行在YARN之上。

Spark是用Scala语言编写的，所提供的API也很好地利用了这门语言的">
<meta property="og:image" content="http://spark.apache.org/images/spark-logo.png">
<meta property="og:updated_time" content="2017-03-09T03:37:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark快速入门">
<meta name="twitter:description" content="Apache Spark是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：

通用计算引擎 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；
基于内存 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；
与Hadoop集成 能够直接读写HDFS中的数据，并能运行在YARN之上。

Spark是用Scala语言编写的，所提供的API也很好地利用了这门语言的">
<meta name="twitter:image" content="http://spark.apache.org/images/spark-logo.png">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/atom.xml" title="张吉的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="https://fonts.proxy.ustclug.org/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/cnblogs/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-37223379-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/cnblogs/" id="logo">张吉的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/cnblogs/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/cnblogs/">首页</a>
        
          <a class="main-nav-link" href="/cnblogs/categories/Big-Data">大数据</a>
        
          <a class="main-nav-link" href="/cnblogs/categories/Programming">编程</a>
        
          <a class="main-nav-link" href="/cnblogs/archives">全部文章</a>
        
          <a class="main-nav-link" href="http://shzhangji.com/">English</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shzhangji.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark-quick-start" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/cnblogs/2014/12/16/spark-quick-start/" class="article-date">
  <time datetime="2014-12-16T07:59:00.000Z" itemprop="datePublished">2014-12-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/cnblogs/categories/Tutorial/">Tutorial</a>►<a class="article-category-link" href="/cnblogs/categories/Tutorial/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark快速入门
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://spark.apache.org/images/spark-logo.png" alt=""></p>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>
<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>
<p>Spark是用<a href="http://www.scala-lang.org/" target="_blank" rel="external">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>
<h2 id="安装Spark和SBT"><a href="#安装Spark和SBT" class="headerlink" title="安装Spark和SBT"></a>安装Spark和SBT</h2><ul>
<li>从<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_HOME=/path/to/spark</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</div></pre></td></tr></table></figure>
<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/" target="_blank" rel="external">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar" target="_blank" rel="external">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SBT_OPTS=<span class="string">"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"</span></div><div class="line">java <span class="variable">$SBT_OPTS</span> -jar `dirname <span class="variable">$0</span>`/sbt-launch.jar <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="日志分析示例"><a href="#日志分析示例" class="headerlink" title="日志分析示例"></a>日志分析示例</h2><p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2014-12-11 18:33:52	INFO	Java	some message</div><div class="line">2014-12-11 18:34:33	INFO	MySQL	some message</div><div class="line">2014-12-11 18:34:54	WARN	Java	some message</div><div class="line">2014-12-11 18:35:25	WARN	Nginx	some message</div><div class="line">2014-12-11 18:36:09	INFO	Java	some message</div></pre></td></tr></table></figure>
<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>
<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ spark-shell</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0</div><div class="line">      /_/</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<h3 id="加载并预览数据"><a href="#加载并预览数据" class="headerlink" title="加载并预览数据"></a>加载并预览数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> lines = sc.textFile(<span class="string">"/tmp/logs.txt"</span>)</div><div class="line">lines: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /tmp/logs.txt <span class="type">MappedRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">12</span></div><div class="line"></div><div class="line">scala&gt; lines.first()</div><div class="line">res0: <span class="type">String</span> = <span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>	<span class="type">INFO</span>	<span class="type">Java</span>	some message</div></pre></td></tr></table></figure>
<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>
<h3 id="解析日志"><a href="#解析日志" class="headerlink" title="解析日志"></a>解析日志</h3><p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> logs = lines.map(line =&gt; line.split(<span class="string">"\t"</span>))</div><div class="line">logs: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">MappedRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">14</span></div><div class="line"></div><div class="line">scala&gt; logs.first()</div><div class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>, <span class="type">INFO</span>, <span class="type">Java</span>, some message)</div></pre></td></tr></table></figure>
<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line =&gt; line.split(“\t”)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>
<h3 id="过滤并计数"><a href="#过滤并计数" class="headerlink" title="过滤并计数"></a>过滤并计数</h3><p>我们想要统计错误日志的数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> errors = logs.filter(log =&gt; log(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">errors: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">FilteredRDD</span>[<span class="number">3</span>] at filter at &lt;console&gt;:<span class="number">16</span></div><div class="line"></div><div class="line">scala&gt; errors.first()</div><div class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message)</div><div class="line"></div><div class="line">scala&gt; errors.count()</div><div class="line">res3: <span class="type">Long</span> = <span class="number">158</span></div></pre></td></tr></table></figure>
<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == “ERROR”表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; errors.cache()</div></pre></td></tr></table></figure>
<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>
<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>
<h3 id="显示前10条记录"><a href="#显示前10条记录" class="headerlink" title="显示前10条记录"></a>显示前10条记录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> firstTenErrors = errors.take(<span class="number">10</span>)</div><div class="line">firstTenErrors: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message), <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>, <span class="type">ERROR</span>, <span class="type">Nginx</span>, some message), ...)</div><div class="line"></div><div class="line">scala&gt; firstTenErrors.map(log =&gt; log.mkString(<span class="string">"\t"</span>)).foreach(line =&gt; println(line))</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>	<span class="type">ERROR</span>	<span class="type">Java</span>	some message</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>	<span class="type">ERROR</span>	<span class="type">Nginx</span>	some message</div><div class="line">...</div></pre></td></tr></table></figure>
<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>
<h3 id="按应用进行统计"><a href="#按应用进行统计" class="headerlink" title="按应用进行统计"></a>按应用进行统计</h3><p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> apps = errors.map(log =&gt; (log(<span class="number">2</span>), <span class="number">1</span>))</div><div class="line">apps: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MappedRDD</span>[<span class="number">15</span>] at map at &lt;console&gt;:<span class="number">18</span></div><div class="line"></div><div class="line">scala&gt; apps.first()</div><div class="line">res20: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">Java</span>,<span class="number">1</span>)</div><div class="line"></div><div class="line">scala&gt; <span class="keyword">val</span> counts = apps.reduceByKey((a, b) =&gt; a + b)</div><div class="line">counts: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">17</span>] at reduceByKey at &lt;console&gt;:<span class="number">20</span></div><div class="line"></div><div class="line">scala&gt; counts.foreach(t =&gt; println(t))</div><div class="line">(<span class="type">Java</span>,<span class="number">58</span>)</div><div class="line">(<span class="type">Nginx</span>,<span class="number">53</span>)</div><div class="line">(<span class="type">MySQL</span>,<span class="number">47</span>)</div></pre></td></tr></table></figure>
<p>errors.map(log =&gt; (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>(&quot;Java&quot;, 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>
<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>).reduce((a, b) =&gt; a + b)</div><div class="line">res23: <span class="type">Int</span> = <span class="number">10</span></div></pre></td></tr></table></figure>
<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>
<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>
<h2 id="打包应用程序"><a href="#打包应用程序" class="headerlink" title="打包应用程序"></a>打包应用程序</h2><p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">spark-sandbox</div><div class="line">├── build.sbt</div><div class="line">├── project</div><div class="line">│   ├── build.properties</div><div class="line">│   └── plugins.sbt</div><div class="line">└── src</div><div class="line">    └── main</div><div class="line">        └── scala</div><div class="line">            └── LogMining.scala</div></pre></td></tr></table></figure>
<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox" target="_blank" rel="external">这个项目</a>作为模板。下面说明一些关键部分：</p>
<h3 id="配置依赖"><a href="#配置依赖" class="headerlink" title="配置依赖"></a>配置依赖</h3><p><code>build.sbt</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"1.1.1"</span></div></pre></td></tr></table></figure>
<h3 id="程序内容"><a href="#程序内容" class="headerlink" title="程序内容"></a>程序内容</h3><p><code>src/main/scala/LogMining.scala</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogMining</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogMining"</span>)</div><div class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">  <span class="keyword">val</span> inputFile = args(<span class="number">0</span>)</div><div class="line">  <span class="keyword">val</span> lines = sc.textFile(inputFile)</div><div class="line">  <span class="comment">// 解析日志</span></div><div class="line">  <span class="keyword">val</span> logs = lines.map(_.split(<span class="string">"\t"</span>))</div><div class="line">  <span class="keyword">val</span> errors = logs.filter(_(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">  <span class="comment">// 缓存错误日志</span></div><div class="line">  errors.cache()</div><div class="line">  <span class="comment">// 统计错误日志记录数</span></div><div class="line">  println(errors.count())</div><div class="line">  <span class="comment">// 获取前10条MySQL的错误日志</span></div><div class="line">  <span class="keyword">val</span> mysqlErrors = errors.filter(_(<span class="number">2</span>) == <span class="string">"MySQL"</span>)</div><div class="line">  mysqlErrors.take(<span class="number">10</span>).map(_ mkString <span class="string">"\t"</span>).foreach(println)</div><div class="line">  <span class="comment">// 统计每个应用的错误日志数</span></div><div class="line">  <span class="keyword">val</span> errorApps = errors.map(_(<span class="number">2</span>) -&gt; <span class="number">1</span>)</div><div class="line">  errorApps.countByKey().foreach(println)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="打包运行"><a href="#打包运行" class="headerlink" title="打包运行"></a>打包运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> spark-sandbox</div><div class="line">$ sbt package</div><div class="line">$ spark-submit --class LogMining --master <span class="built_in">local</span> target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final" target="_blank" rel="external">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals" target="_blank" rel="external">Spark Runtime Internals</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shzhangji.com/cnblogs/2014/12/16/spark-quick-start/" data-id="cj01vpkvh001xenzjwq7q2fvm" class="article-share-link">分享</a>
      
        <a href="http://shzhangji.com/cnblogs/2014/12/16/spark-quick-start/#disqus_thread" class="article-comment-link">留言</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/cnblogs/2014/12/23/use-git-rebase-to-clarify-history/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">
        
          使用git rebase让历史变得清晰
        
      </div>
    </a>
  
  
    <a href="/cnblogs/2014/11/07/sbt-offline/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">离线环境下构建sbt项目</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/cnblogs/tags/aosa/" style="font-size: 13.33px;">aosa</a> <a href="/cnblogs/tags/clojure/" style="font-size: 20px;">clojure</a> <a href="/cnblogs/tags/fp/" style="font-size: 10px;">fp</a> <a href="/cnblogs/tags/git/" style="font-size: 10px;">git</a> <a href="/cnblogs/tags/hadoop/" style="font-size: 16.67px;">hadoop</a> <a href="/cnblogs/tags/hive/" style="font-size: 10px;">hive</a> <a href="/cnblogs/tags/javascript/" style="font-size: 10px;">javascript</a> <a href="/cnblogs/tags/mapreduce/" style="font-size: 10px;">mapreduce</a> <a href="/cnblogs/tags/nginx/" style="font-size: 10px;">nginx</a> <a href="/cnblogs/tags/noir/" style="font-size: 16.67px;">noir</a> <a href="/cnblogs/tags/ops/" style="font-size: 10px;">ops</a> <a href="/cnblogs/tags/perl/" style="font-size: 13.33px;">perl</a> <a href="/cnblogs/tags/programming/" style="font-size: 10px;">programming</a> <a href="/cnblogs/tags/python/" style="font-size: 10px;">python</a> <a href="/cnblogs/tags/storm/" style="font-size: 10px;">storm</a> <a href="/cnblogs/tags/translation/" style="font-size: 13.33px;">translation</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2016/03/">三月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/09/">九月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/06/">六月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/03/">三月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2015/01/">一月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/12/">十二月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/11/">十一月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/10/">十月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/07/">七月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/04/">四月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2014/01/">一月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/12/">十二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/09/">九月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/06/">六月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/05/">五月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/04/">四月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/03/">三月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/02/">二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2013/01/">一月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2012/12/">十二月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/cnblogs/archives/2012/11/">十一月 2012</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/cnblogs/2016/03/13/top-5-frameworks/">开发人员必知的5种开源框架</a>
          </li>
        
          <li>
            <a href="/cnblogs/2015/09/12/model-dependency-injection-with-spring-aop/">使用Spring AOP向领域模型注入依赖</a>
          </li>
        
          <li>
            <a href="/cnblogs/2015/09/05/anemic-domain-model/">贫血领域模型</a>
          </li>
        
          <li>
            <a href="/cnblogs/2015/06/25/compressed-oops-in-the-hotspot-jvm/">HotSpot JVM中的对象指针压缩</a>
          </li>
        
          <li>
            <a href="/cnblogs/2015/03/08/hbase-dos-and-donts/">Apache HBase的适用场景</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 张吉<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/cnblogs/" class="mobile-nav-link">首页</a>
  
    <a href="/cnblogs/categories/Big-Data" class="mobile-nav-link">大数据</a>
  
    <a href="/cnblogs/categories/Programming" class="mobile-nav-link">编程</a>
  
    <a href="/cnblogs/archives" class="mobile-nav-link">全部文章</a>
  
    <a href="http://shzhangji.com/" class="mobile-nav-link">English</a>
  
</nav>
    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'http://shzhangji.com/cnblogs/2014/12/16/spark-quick-start/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="https://ajax.proxy.ustclug.org/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/cnblogs/fancybox/jquery.fancybox.css">
  <script src="/cnblogs/fancybox/jquery.fancybox.pack.js"></script>


<script src="/cnblogs/js/script.js"></script>

  </div>
</body>
</html>